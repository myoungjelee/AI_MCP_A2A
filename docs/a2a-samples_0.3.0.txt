This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.
The content has been processed where security check has been disabled.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: samples/python/agents/langgraph/**/*.*, samples/python/agents/headless_agent_auth/**/*.*, samples/python/agents/crewai/**/*.*, samples/python/agents/a2a_mcp/**/*.*, samples/python/agents/a2a-mcp-without-framework/**/*.*
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Security check has been disabled - content may contain sensitive information
- Files are sorted by Git change count (files with more changes are at the bottom)


================================================================
Directory Structure
================================================================
samples/
  python/
    agents/
      a2a_mcp/
        agent_cards/
          air_ticketing_agent.json
          car_rental_agent.json
          hotel_booking_agent.json
          orchestrator_agent.json
          planner_agent.json
        src/
          a2a_mcp/
            agents/
              __main__.py
              adk_travel_agent.py
              langgraph_planner_agent.py
              orchestrator_agent.py
            common/
              agent_executor.py
              agent_runner.py
              base_agent.py
              prompts.py
              types.py
              utils.py
              workflow.py
            mcp/
              client.py
              server.py
            __init__.py
        pyproject.toml
        README.md
        run.sh
      a2a-mcp-without-framework/
        src/
          no_llm_framework/
            client/
              __main__.py
              agent_answer.jinja
              agent.py
              agents.jinja
              called_agent_history.jinja
              constant.py
              decide.jinja
            server/
              __main__.py
              agent_executor.py
              agent.py
              called_tools_history.jinja
              constant.py
              decide.jinja
              mcp.py
              tool.jinja
        pyproject.toml
        README.md
        response.xml
      crewai/
        __main__.py
        agent_executor.py
        agent.py
        in_memory_cache.py
        pyproject.toml
        README.md
      headless_agent_auth/
        __main__.py
        .env.example
        agent_executor.py
        agent.py
        api.py
        oauth2_middleware.py
        pyproject.toml
        README.md
        test_client.py
      langgraph/
        .vscode/
          launch.json
        app/
          __main__.py
          agent_executor.py
          agent.py
          test_client.py
        pyproject.toml
        README.md

================================================================
Files
================================================================

================
File: samples/python/agents/a2a_mcp/agent_cards/air_ticketing_agent.json
================
{
    "name": "Air Ticketing Agent",
    "description": "Helps book air tickets given a criteria",
    "url": "http://localhost:10103/",
    "version": "1.0.0",
    "capabilities": {
        "streaming": true,
        "pushNotifications": true,
        "stateTransitionHistory": false
    },
    "defaultInputModes": [
        "text",
        "text/plain"
    ],
    "defaultOutputModes": [
        "text",
        "text/plain"
    ],
    "skills": [
        {
            "id": "book_air_tickets",
            "name": "Book Air Tickets",
            "description": "Helps with booking air tickets given a criteria",
            "tags": [
                "Book air tickets"
            ],
            "examples": [
                "Book return tickets from SFO to LHR, starting June 24 2025 and returning 30th June 2025"
            ]
        }
    ]
}

================
File: samples/python/agents/a2a_mcp/agent_cards/car_rental_agent.json
================
{
    "name": "Car Rental Agent",
    "description": "Helps book car rental",
    "url": "http://localhost:10105/",
    "version": "1.0.0",
    "capabilities": {
        "streaming": true,
        "pushNotifications": true,
        "stateTransitionHistory": false
    },
    "defaultInputModes": [
        "text",
        "text/plain"
    ],
    "defaultOutputModes": [
        "text",
        "text/plain"
    ],
    "skills": [
        {
            "id": "book_cars",
            "name": "Book Car Rental",
            "description": "Helps with booking car rental given a criteria",
            "tags": [
                "Book cars"
            ],
            "examples": [
                "Book a rental car in London, starting on June 20, 2025, and ending on July 10, 2025"
            ]
        }
    ]
}

================
File: samples/python/agents/a2a_mcp/agent_cards/hotel_booking_agent.json
================
{
    "name": "Hotel Booking Agent",
    "description": "Helps book hotels given a criteria",
    "url": "http://localhost:10104/",
    "version": "1.0.0",
    "capabilities": {
        "streaming": true,
        "pushNotifications": true,
        "stateTransitionHistory": false
    },
    "defaultInputModes": [
        "text",
        "text/plain"
    ],
    "defaultOutputModes": [
        "text",
        "text/plain"
    ],
    "skills": [
        {
            "id": "book_accommodation",
            "name": "Book Hotels",
            "description": "Helps with booking hotels given a criteria",
            "tags": [
                "Book accommodation"
            ],
            "examples": [
                "Book a hotel for 1 adult in Central London, starting June 24 2025 for 2 nights"
            ]
        }
    ]
}

================
File: samples/python/agents/a2a_mcp/agent_cards/orchestrator_agent.json
================
{
    "name": "Orchestrator Agent",
    "description": "Orchestrates the task generation and execution",
    "url": "http://localhost:10101/",
    "version": "1.0.0",
    "capabilities": {
        "streaming": true,
        "pushNotifications": true,
        "stateTransitionHistory": false
    },
    "defaultInputModes": [
        "text",
        "text/plain"
    ],
    "defaultOutputModes": [
        "text",
        "text/plain"
    ],
    "skills": [
        {
            "id": "executor",
            "name": "Task Executor",
            "description": "Orchestrates the task generation and execution, takes help from the planner to generate tasks",
            "tags": [
                "execute plan"
            ],
            "examples": [
                "Plan my trip to London, submit an expense report"
            ]
        }
    ]
}

================
File: samples/python/agents/a2a_mcp/agent_cards/planner_agent.json
================
{
    "name": "Langraph Planner Agent",
    "description": "Helps breakdown a request in to actionable tasks",
    "url": "http://localhost:10102/",
    "version": "1.0.0",
    "capabilities": {
        "streaming": true,
        "pushNotifications": true,
        "stateTransitionHistory": false
    },
    "defaultInputModes": [
        "text",
        "text/plain"
    ],
    "defaultOutputModes": [
        "text",
        "text/plain"
    ],
    "skills": [
        {
            "id": "planner",
            "name": "Task Planner",
            "description": "Helps breakdown a request in to actionable tasks",
            "tags": [
                "planner"
            ],
            "examples": [
                "Plan my business trip from San Francisco to London, submit an expense report"
            ]
        }
    ]
}

================
File: samples/python/agents/a2a_mcp/src/a2a_mcp/agents/__main__.py
================
# type: ignore

import json
import logging
import sys

from pathlib import Path

import click
import httpx
import uvicorn

from a2a.server.apps import A2AStarletteApplication
from a2a.server.request_handlers import DefaultRequestHandler
from a2a.server.tasks import (
    BasePushNotificationSender,
    InMemoryPushNotificationConfigStore,
    InMemoryTaskStore,
)
from a2a.types import AgentCard
from a2a_mcp.common import prompts
from a2a_mcp.common.agent_executor import GenericAgentExecutor
from adk_travel_agent import TravelAgent
from langgraph_planner_agent import LangGraphPlannerAgent
from orchestrator_agent import OrchestratorAgent


logger = logging.getLogger(__name__)


def get_agent(agent_card: AgentCard):
    """Get the agent, given an agent card."""
    try:
        if agent_card.name == 'Orchestrator Agent':
            return OrchestratorAgent()
        if agent_card.name == 'Langraph Planner Agent':
            return LangGraphPlannerAgent()
        if agent_card.name == 'Air Ticketing Agent':
            return TravelAgent(
                agent_name='AirTicketingAgent',
                description='Book air tickets given a criteria',
                instructions=prompts.AIRFARE_COT_INSTRUCTIONS,
            )
        if agent_card.name == 'Hotel Booking Agent':
            return TravelAgent(
                agent_name='HotelBookingAgent',
                description='Book hotels given a criteria',
                instructions=prompts.HOTELS_COT_INSTRUCTIONS,
            )
        if agent_card.name == 'Car Rental Agent':
            return TravelAgent(
                agent_name='CarRentalBookingAgent',
                description='Book rental cars given a criteria',
                instructions=prompts.CARS_COT_INSTRUCTIONS,
            )
            # return LangraphCarRentalAgent()
    except Exception as e:
        raise e


@click.command()
@click.option('--host', 'host', default='localhost')
@click.option('--port', 'port', default=10101)
@click.option('--agent-card', 'agent_card')
def main(host, port, agent_card):
    """Starts an Agent server."""
    try:
        if not agent_card:
            raise ValueError('Agent card is required')
        with Path.open(agent_card) as file:
            data = json.load(file)
        agent_card = AgentCard(**data)

        client = httpx.AsyncClient()
        push_notification_config_store = InMemoryPushNotificationConfigStore()
        push_notification_sender = BasePushNotificationSender(
            client, config_store=push_notification_config_store
        )

        request_handler = DefaultRequestHandler(
            agent_executor=GenericAgentExecutor(agent=get_agent(agent_card)),
            task_store=InMemoryTaskStore(),
            push_config_store=push_notification_config_store,
            push_sender=push_notification_sender,
        )

        server = A2AStarletteApplication(
            agent_card=agent_card, http_handler=request_handler
        )

        logger.info(f'Starting server on {host}:{port}')

        uvicorn.run(server.build(), host=host, port=port)
    except FileNotFoundError:
        logger.error(f"Error: File '{agent_card}' not found.")
        sys.exit(1)
    except json.JSONDecodeError:
        logger.error(f"Error: File '{agent_card}' contains invalid JSON.")
        sys.exit(1)
    except Exception as e:
        logger.error(f'An error occurred during server startup: {e}')
        sys.exit(1)


if __name__ == '__main__':
    main()

================
File: samples/python/agents/a2a_mcp/src/a2a_mcp/agents/adk_travel_agent.py
================
# type: ignore

import json
import logging
import re

from collections.abc import AsyncIterable
from typing import Any

from a2a_mcp.common.agent_runner import AgentRunner
from a2a_mcp.common.base_agent import BaseAgent
from a2a_mcp.common.utils import get_mcp_server_config, init_api_key
from google.adk.agents import Agent
from google.adk.models.lite_llm import LiteLlm
from google.adk.tools.mcp_tool.mcp_toolset import MCPToolset
from google.adk.tools.mcp_tool.mcp_session_manager import SseServerParams
from google.genai import types as genai_types


logger = logging.getLogger(__name__)


class TravelAgent(BaseAgent):
    """Travel Agent backed by ADK."""

    def __init__(self, agent_name: str, description: str, instructions: str):
        init_api_key()

        super().__init__(
            agent_name=agent_name,
            description=description,
            content_types=['text', 'text/plain'],
        )

        logger.info(f'Init {self.agent_name}')

        self.instructions = instructions
        self.agent = None

    async def init_agent(self):
        logger.info(f'Initializing {self.agent_name} metadata')
        config = get_mcp_server_config()
        logger.info(f'MCP Server url={config.url}')
        tools = await MCPToolset(
            connection_params=SseServerParams(url=config.url)
        ).get_tools()

        for tool in tools:
            logger.info(f'Loaded tools {tool.name}')
        generate_content_config = genai_types.GenerateContentConfig(
            temperature=0.0
        )
        LITELLM_MODEL = os.getenv('LITELLM_MODEL', 'gemini/gemini-2.0-flash')
        self.agent = Agent(
            name=self.agent_name,
            instruction=self.instructions,
            model=LiteLlm(model=LITELLM_MODEL),
            disallow_transfer_to_parent=True,
            disallow_transfer_to_peers=True,
            generate_content_config=generate_content_config,
            tools=tools,
        )
        self.runner = AgentRunner()

    async def invoke(self, query, session_id) -> dict:
        logger.info(f'Running {self.agent_name} for session {session_id}')

        raise NotImplementedError('Please use the streraming function')

    async def stream(
        self, query, context_id, task_id
    ) -> AsyncIterable[dict[str, Any]]:
        logger.info(
            f'Running {self.agent_name} stream for session {context_id} {task_id} - {query}'
        )

        if not query:
            raise ValueError('Query cannot be empty')

        if not self.agent:
            await self.init_agent()
        async for chunk in self.runner.run_stream(
            self.agent, query, context_id
        ):
            logger.info(f'Received chunk {chunk}')
            if isinstance(chunk, dict) and chunk.get('type') == 'final_result':
                response = chunk['response']
                yield self.get_agent_response(response)
            else:
                yield {
                    'is_task_complete': False,
                    'require_user_input': False,
                    'content': f'{self.agent_name}: Processing Request...',
                }

    def format_response(self, chunk):
        patterns = [
            r'```\n(.*?)\n```',
            r'```json\s*(.*?)\s*```',
            r'```tool_outputs\s*(.*?)\s*```',
        ]

        for pattern in patterns:
            match = re.search(pattern, chunk, re.DOTALL)
            if match:
                content = match.group(1)
                try:
                    return json.loads(content)
                except json.JSONDecodeError:
                    return content
        return chunk

    def get_agent_response(self, chunk):
        logger.info(f'Response Type {type(chunk)}')
        data = self.format_response(chunk)
        logger.info(f'Formatted Response {data}')
        try:
            if isinstance(data, dict):
                if 'status' in data and data['status'] == 'input_required':
                    return {
                        'response_type': 'text',
                        'is_task_complete': False,
                        'require_user_input': True,
                        'content': data['question'],
                    }
                return {
                    'response_type': 'data',
                    'is_task_complete': True,
                    'require_user_input': False,
                    'content': data,
                }
            return_type = 'data'
            try:
                data = json.loads(data)
                return_type = 'data'
            except Exception as json_e:
                logger.error(f'Json conversion error {json_e}')
                return_type = 'text'
            return {
                'response_type': return_type,
                'is_task_complete': True,
                'require_user_input': False,
                'content': data,
            }
        except Exception as e:
            logger.error(f'Error in get_agent_response: {e}')
            return {
                'response_type': 'text',
                'is_task_complete': True,
                'require_user_input': False,
                'content': 'Could not complete booking / task. Please try again.',
            }

================
File: samples/python/agents/a2a_mcp/src/a2a_mcp/agents/langgraph_planner_agent.py
================
# type: ignore

import logging

from collections.abc import AsyncIterable
from typing import Any, Literal

from a2a_mcp.common import prompts
from a2a_mcp.common.base_agent import BaseAgent
from a2a_mcp.common.types import TaskList
from a2a_mcp.common.utils import init_api_key
from langchain_core.messages import AIMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent
from pydantic import BaseModel, Field


memory = MemorySaver()
logger = logging.getLogger(__name__)


class ResponseFormat(BaseModel):
    """Respond to the user in this format."""

    status: Literal['input_required', 'completed', 'error'] = 'input_required'
    question: str = Field(
        description='Input needed from the user to generate the plan'
    )
    content: TaskList = Field(
        description='List of tasks when the plan is generated'
    )


class LangGraphPlannerAgent(BaseAgent):
    """Planner Agent backed by LangGraph."""

    def __init__(self):
        init_api_key()

        logger.info('Initializing LanggraphPlannerAgent')

        super().__init__(
            agent_name='PlannerAgent',
            description='Breakdown the user request into executable tasks',
            content_types=['text', 'text/plain'],
        )

        self.model = ChatGoogleGenerativeAI(
            model='gemini-2.0-flash', temperature=0.0
        )

        self.graph = create_react_agent(
            self.model,
            checkpointer=memory,
            prompt=prompts.PLANNER_COT_INSTRUCTIONS,
            # prompt=prompts.TRIP_PLANNER_INSTRUCTIONS_1,
            response_format=ResponseFormat,
            tools=[],
        )

    def invoke(self, query, sessionId) -> str:
        config = {'configurable': {'thread_id': sessionId}}
        self.graph.invoke({'messages': [('user', query)]}, config)
        return self.get_agent_response(config)

    async def stream(
        self, query, sessionId, task_id
    ) -> AsyncIterable[dict[str, Any]]:
        inputs = {'messages': [('user', query)]}
        config = {'configurable': {'thread_id': sessionId}}

        logger.info(
            f'Running LanggraphPlannerAgent stream for session {sessionId} {task_id} with input {query}'
        )

        for item in self.graph.stream(inputs, config, stream_mode='values'):
            message = item['messages'][-1]
            if isinstance(message, AIMessage):
                yield {
                    'response_type': 'text',
                    'is_task_complete': False,
                    'require_user_input': False,
                    'content': message.content,
                }
        yield self.get_agent_response(config)

    def get_agent_response(self, config):
        current_state = self.graph.get_state(config)
        structured_response = current_state.values.get('structured_response')
        if structured_response and isinstance(
            structured_response, ResponseFormat
        ):
            if (
                structured_response.status == 'input_required'
                # and structured_response.content.tasks
            ):
                return {
                    'response_type': 'text',
                    'is_task_complete': False,
                    'require_user_input': True,
                    'content': structured_response.question,
                }
            if structured_response.status == 'error':
                return {
                    'response_type': 'text',
                    'is_task_complete': False,
                    'require_user_input': True,
                    'content': structured_response.question,
                }
            if structured_response.status == 'completed':
                return {
                    'response_type': 'data',
                    'is_task_complete': True,
                    'require_user_input': False,
                    'content': structured_response.content.model_dump(),
                }
        return {
            'is_task_complete': False,
            'require_user_input': True,
            'content': 'We are unable to process your request at the moment. Please try again.',
        }

================
File: samples/python/agents/a2a_mcp/src/a2a_mcp/agents/orchestrator_agent.py
================
import json
import logging

from collections.abc import AsyncIterable

from a2a.types import (
    SendStreamingMessageSuccessResponse,
    TaskArtifactUpdateEvent,
    TaskState,
    TaskStatusUpdateEvent,
)
from a2a_mcp.common import prompts
from a2a_mcp.common.base_agent import BaseAgent
from a2a_mcp.common.utils import init_api_key
from a2a_mcp.common.workflow import Status, WorkflowGraph, WorkflowNode
from google import genai


logger = logging.getLogger(__name__)


class OrchestratorAgent(BaseAgent):
    """Orchestrator Agent."""

    def __init__(self):
        init_api_key()
        super().__init__(
            agent_name='Orchestrator Agent',
            description='Facilitate inter agent communication',
            content_types=['text', 'text/plain'],
        )
        self.graph = None
        self.results = []
        self.travel_context = {}
        self.query_history = []
        self.context_id = None

    async def generate_summary(self) -> str:
        client = genai.Client()
        response = client.models.generate_content(
            model='gemini-2.0-flash',
            contents=prompts.SUMMARY_COT_INSTRUCTIONS.replace(
                '{travel_data}', str(self.results)
            ),
            config={'temperature': 0.0},
        )
        return response.text

    def answer_user_question(self, question) -> str:
        try:
            client = genai.Client()
            response = client.models.generate_content(
                model='gemini-2.0-flash',
                contents=prompts.QA_COT_PROMPT.replace(
                    '{TRIP_CONTEXT}', str(self.travel_context)
                )
                .replace('{CONVERSATION_HISTORY}', str(self.query_history))
                .replace('{TRIP_QUESTION}', question),
                config={
                    'temperature': 0.0,
                    'response_mime_type': 'application/json',
                },
            )
            return response.text
        except Exception as e:
            logger.info(f'Error answering user question: {e}')
        return '{"can_answer": "no", "answer": "Cannot answer based on provided context"}'

    def set_node_attributes(
        self, node_id, task_id=None, context_id=None, query=None
    ):
        attr_val = {}
        if task_id:
            attr_val['task_id'] = task_id
        if context_id:
            attr_val['context_id'] = context_id
        if query:
            attr_val['query'] = query

        self.graph.set_node_attributes(node_id, attr_val)

    def add_graph_node(
        self,
        task_id,
        context_id,
        query: str,
        node_id: str = None,
        node_key: str = None,
        node_label: str = None,
    ) -> WorkflowNode:
        """Add a node to the graph."""
        node = WorkflowNode(
            task=query, node_key=node_key, node_label=node_label
        )
        self.graph.add_node(node)
        if node_id:
            self.graph.add_edge(node_id, node.id)
        self.set_node_attributes(node.id, task_id, context_id, query)
        return node

    def clear_state(self):
        self.graph = None
        self.results.clear()
        self.travel_context.clear()
        self.query_history.clear()

    async def stream(
        self, query, context_id, task_id
    ) -> AsyncIterable[dict[str, any]]:
        """Execute and stream response."""
        logger.info(
            f'Running {self.agent_name} stream for session {context_id}, task {task_id} - {query}'
        )
        if not query:
            raise ValueError('Query cannot be empty')
        if self.context_id != context_id:
            # Clear state when the context changes
            self.clear_state()
            self.context_id = context_id

        self.query_history.append(query)
        start_node_id = None
        # Graph does not exist, start a new graph with planner node.
        if not self.graph:
            self.graph = WorkflowGraph()
            planner_node = self.add_graph_node(
                task_id=task_id,
                context_id=context_id,
                query=query,
                node_key='planner',
                node_label='Planner',
            )
            start_node_id = planner_node.id
        # Paused state is when the agent might need more information.
        elif self.graph.state == Status.PAUSED:
            start_node_id = self.graph.paused_node_id
            self.set_node_attributes(node_id=start_node_id, query=query)

        # This loop can be avoided if the workflow graph is dynamic or
        # is built from the results of the planner when the planner
        # iself is not a part of the graph.
        # TODO: Make the graph dynamically iterable over edges
        while True:
            # Set attributes on the node so we propagate task and context
            self.set_node_attributes(
                node_id=start_node_id,
                task_id=task_id,
                context_id=context_id,
            )
            # Resume workflow, used when the workflow nodes are updated.
            should_resume_workflow = False
            async for chunk in self.graph.run_workflow(
                start_node_id=start_node_id
            ):
                if isinstance(chunk.root, SendStreamingMessageSuccessResponse):
                    # The graph node retured TaskStatusUpdateEvent
                    # Check if the node is complete and continue to the next node
                    if isinstance(chunk.root.result, TaskStatusUpdateEvent):
                        task_status_event = chunk.root.result
                        context_id = task_status_event.context_id
                        if (
                            task_status_event.status.state
                            == TaskState.completed
                            and context_id
                        ):
                            ## yeild??
                            continue
                        if (
                            task_status_event.status.state
                            == TaskState.input_required
                        ):
                            question = task_status_event.status.message.parts[
                                0
                            ].root.text

                            try:
                                answer = json.loads(
                                    self.answer_user_question(question)
                                )
                                logger.info(f'Agent Answer {answer}')
                                if answer['can_answer'] == 'yes':
                                    # Orchestrator can answer on behalf of the user set the query
                                    # Resume workflow from paused state.
                                    query = answer['answer']
                                    start_node_id = self.graph.paused_node_id
                                    self.set_node_attributes(
                                        node_id=start_node_id, query=query
                                    )
                                    should_resume_workflow = True
                            except Exception:
                                logger.info('Cannot convert answer data')

                    # The graph node retured TaskArtifactUpdateEvent
                    # Store the node and continue.
                    if isinstance(chunk.root.result, TaskArtifactUpdateEvent):
                        artifact = chunk.root.result.artifact
                        self.results.append(artifact)
                        if artifact.name == 'PlannerAgent-result':
                            # Planning agent returned data, update graph.
                            artifact_data = artifact.parts[0].root.data
                            if 'trip_info' in artifact_data:
                                self.travel_context = artifact_data['trip_info']
                            logger.info(
                                f'Updating workflow with {len(artifact_data["tasks"])} task nodes'
                            )
                            # Define the edges
                            current_node_id = start_node_id
                            for idx, task_data in enumerate(
                                artifact_data['tasks']
                            ):
                                node = self.add_graph_node(
                                    task_id=task_id,
                                    context_id=context_id,
                                    query=task_data['description'],
                                    node_id=current_node_id,
                                )
                                current_node_id = node.id
                                # Restart graph from the newly inserted subgraph state
                                # Start from the new node just created.
                                if idx == 0:
                                    should_resume_workflow = True
                                    start_node_id = node.id
                        else:
                            # Not planner but artifacts from other tasks,
                            # continue to the next node in the workflow.
                            # client does not get the artifact,
                            # a summary is shown at the end of the workflow.
                            continue
                # When the workflow needs to be resumed, do not yield partial.
                if not should_resume_workflow:
                    logger.info('No workflow resume detected, yielding chunk')
                    # Yield partial execution
                    yield chunk
            # The graph is complete and no updates, so okay to break from the loop.
            if not should_resume_workflow:
                logger.info(
                    'Workflow iteration complete and no restart requested. Exiting main loop.'
                )
                break
            else:
                # Readable logs
                logger.info('Restarting workflow loop.')
        if self.graph.state == Status.COMPLETED:
            # All individual actions complete, now generate the summary
            logger.info(f'Generating summary for {len(self.results)} results')
            summary = await self.generate_summary()
            self.clear_state()
            logger.info(f'Summary: {summary}')
            yield {
                'response_type': 'text',
                'is_task_complete': True,
                'require_user_input': False,
                'content': summary,
            }

================
File: samples/python/agents/a2a_mcp/src/a2a_mcp/common/agent_executor.py
================
import logging

from a2a.server.agent_execution import AgentExecutor, RequestContext
from a2a.server.events import EventQueue
from a2a.server.tasks import TaskUpdater
from a2a.types import (
    DataPart,
    InvalidParamsError,
    SendStreamingMessageSuccessResponse,
    Task,
    TaskArtifactUpdateEvent,
    TaskState,
    TaskStatusUpdateEvent,
    TextPart,
    UnsupportedOperationError,
)
from a2a.utils import new_agent_text_message, new_task
from a2a.utils.errors import ServerError
from a2a_mcp.common.base_agent import BaseAgent


logger = logging.getLogger(__name__)


class GenericAgentExecutor(AgentExecutor):
    """AgentExecutor used by the tragel agents."""

    def __init__(self, agent: BaseAgent):
        self.agent = agent

    async def execute(
        self,
        context: RequestContext,
        event_queue: EventQueue,
    ) -> None:
        logger.info(f'Executing agent {self.agent.agent_name}')
        error = self._validate_request(context)
        if error:
            raise ServerError(error=InvalidParamsError())

        query = context.get_user_input()

        task = context.current_task

        if not task:
            task = new_task(context.message)
            await event_queue.enqueue_event(task)

        updater = TaskUpdater(event_queue, task.id, task.context_id)

        async for item in self.agent.stream(query, task.context_id, task.id):
            # Agent to Agent call will return events,
            # Update the relevant ids to proxy back.
            if hasattr(item, 'root') and isinstance(
                item.root, SendStreamingMessageSuccessResponse
            ):
                event = item.root.result
                if isinstance(
                    event,
                    (TaskStatusUpdateEvent | TaskArtifactUpdateEvent),
                ):
                    await event_queue.enqueue_event(event)
                continue

            is_task_complete = item['is_task_complete']
            require_user_input = item['require_user_input']

            if is_task_complete:
                if item['response_type'] == 'data':
                    part = DataPart(data=item['content'])
                else:
                    part = TextPart(text=item['content'])

                await updater.add_artifact(
                    [part],
                    name=f'{self.agent.agent_name}-result',
                )
                await updater.complete()
                break
            if require_user_input:
                await updater.update_status(
                    TaskState.input_required,
                    new_agent_text_message(
                        item['content'],
                        task.context_id,
                        task.id,
                    ),
                    final=True,
                )
                break
            await updater.update_status(
                TaskState.working,
                new_agent_text_message(
                    item['content'],
                    task.context_id,
                    task.id,
                ),
            )

    def _validate_request(self, context: RequestContext) -> bool:
        return False

    async def cancel(
        self, request: RequestContext, event_queue: EventQueue
    ) -> Task | None:
        raise ServerError(error=UnsupportedOperationError())

================
File: samples/python/agents/a2a_mcp/src/a2a_mcp/common/agent_runner.py
================
# type: ignore

import uuid

from collections.abc import AsyncGenerator

from google.adk.agents import Agent
from google.adk.events import Event
from google.adk.runners import Runner
from google.adk.sessions import InMemorySessionService
from google.genai import types


class AgentRunner:
    """Manages the execution of an ADK (Agent Development Kit) Agent.

    This class encapsulates the logic for running an agent, handling session
    management (creation and retrieval), and streaming responses back to the
    caller. It uses an in-memory session service.
    """

    def __init__(
        self,
        user_id: str = 'user_1',
        app_name: str = 'A2A-MCP',
    ):
        self.session_service = InMemorySessionService()
        self.session = None
        self.app_name = app_name
        self.user_id = user_id

    async def run_stream(
        self, agent: Agent, query: str, session_id: str
    ) -> AsyncGenerator[Event, None]:
        runner = Runner(
            agent=agent,
            app_name=self.app_name,
            session_service=self.session_service,
        )
        if not session_id:
            session_id = uuid.uuid4().hex
        else:
            self.session = await self.session_service.get_session(
                app_name=self.app_name,
                user_id=self.user_id,
                session_id=session_id,
            )
        if not self.session:
            self.session = await self.session_service.create_session(
                app_name=self.app_name,
                user_id=self.user_id,
                session_id=session_id,
            )
        content = types.Content(role='user', parts=[types.Part(text=query)])

        async for event in runner.run_async(
            user_id=self.user_id,
            session_id=self.session.id,
            new_message=content,
        ):
            if event.is_final_response():
                response = ''
                if (
                    event.content
                    and event.content.parts
                    and event.content.parts[0].text
                ):
                    response = '\n'.join(
                        [p.text for p in event.content.parts if p.text]
                    )
                elif (
                    event.content
                    and event.content.parts
                    and any(
                        True for p in event.content.parts if p.function_response
                    )
                ):
                    response = next(
                        p.function_response.model_dump()
                        for p in event.content.parts
                    )
                else:
                    response = f'Error in running agent: {agent.name}'
                yield {
                    'type': 'final_result',
                    'response': response,
                }
            else:
                yield {
                    'is_task_complete': False,
                    'require_user_input': False,
                    'content': f'{agent.name}: Processing request...',
                }

================
File: samples/python/agents/a2a_mcp/src/a2a_mcp/common/base_agent.py
================
# type: ignore
from abc import ABC

from pydantic import BaseModel, Field


class BaseAgent(BaseModel, ABC):
    """Base class for agents."""

    model_config = {
        'arbitrary_types_allowed': True,
        'extra': 'allow',
    }

    agent_name: str = Field(
        description='The name of the agent.',
    )

    description: str = Field(
        description="A brief description of the agent's purpose.",
    )

    content_types: list[str] = Field(description='Supported content types.')

================
File: samples/python/agents/a2a_mcp/src/a2a_mcp/common/prompts.py
================
# System Instructions to the Airfare Agent
AIRFARE_COT_INSTRUCTIONS = """
You are an Airline ticket booking / reservation assistant.
Your task is to help the users with flight bookings.

Always use chain-of-thought reasoning before responding to track where you are 
in the decision tree and determine the next appropriate question.

Your question should follow the example format below
{
    "status": "input_required",
    "question": "What cabin class do you wish to fly?"
}

DECISION TREE:
1. Origin
    - If unknown, ask for origin.
    - If known, proceed to step 2.
2. Destination
    - If unknown, ask for destination.
    - If known, proceed to step 3.
3. Dates
    - If unknown, ask for start and return dates.
    - If known, proceed to step 4.
4. Class
    - If unknown, ask for cabin class.
    - If known, proceed to step 5.

CHAIN-OF-THOUGHT PROCESS:
Before each response, reason through:
1. What information do I already have? [List all known information]
2. What is the next unknown information in the decision tree? [Identify gap]
3. How should I naturally ask for this information? [Formulate question]
4. What context from previous information should I include? [Add context]
5. If I have all the information I need, I should now proceed to search

You will use the tools provided to you to search for the ariline tickets, after you have all the information.
For return bookings, you will use the tools again.


If the search does not return any results for the user criteria.
    - Search again for a different ticket class.
    - Respond to the user in the following format.
    {
        "status": "input_required",
        "question": "I could not find any flights that match your criteria, but I found tickets in First Class, would you like to book that instead?"
    }

Schema for the datamodel is in the DATAMODEL section.
Respond in the format shown in the RESPONSE section.


DATAMODEL:
CREATE TABLE flights (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        carrier TEXT NOT NULL,
        flight_number INTEGER NOT NULL,
        from_airport TEXT NOT NULL,
        to_airport TEXT NOT NULL,
        ticket_class TEXT NOT NULL,
        price REAL NOT NULL
    )

    ticket_class is an enum with values 'ECONOMY', 'BUSINESS' and 'FIRST'

    Example:

    Onward Journey:

    SELECT carrier, flight_number, from_airport, to_airport, ticket_class, price FROM flights
    WHERE from_airport = 'SFO' AND to_airport = 'LHR' AND ticket_class = 'BUSINESS'

    Return Journey:
    SELECT carrier, flight_number, from_airport, to_airport, ticket_class, price FROM flights
    WHERE from_airport = 'LHR' AND to_airport = 'SFO' AND ticket_class = 'BUSINESS'

RESPONSE:
    {
        "onward": {
            "airport" : "[DEPARTURE_LOCATION (AIRPORT_CODE)]",
            "date" : "[DEPARTURE_DATE]",
            "airline" : "[AIRLINE]",
            "flight_number" : "[FLIGHT_NUMBER]",
            "travel_class" : "[TRAVEL_CLASS]",
            "cost" : "[PRICE]"
        },
        "return": {
            "airport" : "[DESTINATION_LOCATION (AIRPORT_CODE)]",
            "date" : "[RETURN_DATE]",
            "airline" : "[AIRLINE]",
            "flight_number" : "[FLIGHT_NUMBER]",
            "travel_class" : "[TRAVEL_CLASS]",
            "cost" : "[PRICE]"
        },
        "total_price": "[TOTAL_PRICE]",
        "status": "completed",
        "description": "Booking Complete"
    }
"""

# System Instructions to the Hotels Agent
HOTELS_COT_INSTRUCTIONS = """
You are an Hotel reservation assistant.
Your task is to help the users with hotel bookings.

Always use chain-of-thought reasoning before responding to track where you are 
in the decision tree and determine the next appropriate question.

If you have a question, you should should strictly follow the example format below
{
    "status": "input_required",
    "question": "What is your checkout date?"
}


DECISION TREE:
1. City
    - If unknown, ask for the city.
    - If known, proceed to step 2.
2. Dates
    - If unknown, ask for checkin and checkout dates.
    - If known, proceed to step 3.
3. Property Type
    - If unknown, ask for the type of property. Hotel, AirBnB or a private property.
    - If known, proceed to step 4.
4. Room Type
    - If unknown, ask for the room type. Suite, Standard, Single, Double.
    - If known, proceed to step 5.

CHAIN-OF-THOUGHT PROCESS:
Before each response, reason through:
1. What information do I already have? [List all known information]
2. What is the next unknown information in the decision tree? [Identify gap]
3. How should I naturally ask for this information? [Formulate question]
4. What context from previous information should I include? [Add context]
5. If I have all the information I need, I should now proceed to search.


You will use the tools provided to you to search for the hotels, after you have all the information.

If the search does not return any results for the user criteria.
    - Search again for a different hotel or property type.
    - Respond to the user in the following format.
    {
        "status": "input_required",
        "question": "I could not find any properties that match your criteria, however, I was able to find an AirBnB, would you like to book that instead?"
    }

Schema for the datamodel is in the DATAMODEL section.
Respond in the format shown in the RESPONSE section.

DATAMODEL:
CREATE TABLE hotels (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        name TEXT NOT NULL,
        city TEXT NOT NULL,
        hotel_type TEXT NOT NULL,
        room_type TEXT NOT NULL, 
        price_per_night REAL NOT NULL
    )
    hotel_type is an enum with values 'HOTEL', 'AIRBNB' and 'PRIVATE_PROPERTY'
    room_type is an enum with values 'STANDARD', 'SINGLE', 'DOUBLE', 'SUITE'

    Example:
    SELECT name, city, hotel_type, room_type, price_per_night FROM hotels WHERE city ='London' AND hotel_type = 'HOTEL' AND room_type = 'SUITE'

RESPONSE:
    {
        "name": "[HOTEL_NAME]",
        "city": "[CITY]",
        "hotel_type": "[ACCOMMODATION_TYPE]",
        "room_type": "[ROOM_TYPE]",
        "price_per_night": "[PRICE_PER_NIGHT]",
        "check_in_time": "3:00 pm",
        "check_out_time": "11:00 am",
        "total_rate_usd": "[TOTAL_RATE], --Number of nights * price_per_night"
        "status": "[BOOKING_STATUS]",
        "description": "Booking Complete"
    }
"""

# System Instructions to the Car Rental Agent
CARS_COT_INSTRUCTIONS = """
You are an car rental reservation assistant.
Your task is to help the users with car rental reservations.

Always use chain-of-thought reasoning before responding to track where you are 
in the decision tree and determine the next appropriate question.

Your question should follow the example format below
{
    "status": "input_required",
    "question": "What class of car do you prefer, Sedan, SUV or a Truck?"
}


DECISION TREE:
1. City
    - If unknown, ask for the city.
    - If known, proceed to step 2.
2. Dates
    - If unknown, ask for pickup and return dates.
    - If known, proceed to step 3.
3. Class of car
    - If unknown, ask for the class of car. Sedan, SUV or a Truck.
    - If known, proceed to step 4.

CHAIN-OF-THOUGHT PROCESS:
Before each response, reason through:
1. What information do I already have? [List all known information]
2. What is the next unknown information in the decision tree? [Identify gap]
3. How should I naturally ask for this information? [Formulate question]
4. What context from previous information should I include? [Add context]
5. If I have all the information I need, I should now proceed to search

You will use the tools provided to you to search for the hotels, after you have all the information.

If the search does not return any results for the user criteria.
    - Search again for a different type of car.
    - Respond to the user in the following format.
    {
        "status": "input_required",
        "question": "I could not find any cars that match your criteria, however, I was able to find an SUV, would you like to book that instead?"
    }

Schema for the datamodel is in the DATAMODEL section.
Respond in the format shown in the RESPONSE section.

DATAMODEL:
    CREATE TABLE rental_cars (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        provider TEXT NOT NULL,
        city TEXT NOT NULL,
        type_of_car TEXT NOT NULL,
        daily_rate REAL NOT NULL
    )

    type_of_car is an enum with values 'SEDAN', 'SUV' and 'TRUCK'

    Example:
    SELECT provider, city, type_of_car, daily_rate FROM rental_cars WHERE city = 'London' AND type_of_car = 'SEDAN'

RESPONSE:
    {
        "pickup_date": "[PICKUP_DATE]",
        "return_date": "[RETURN_DATE]",
        "provider": "[PROVIDER]",
        "city": "[CITY]",
        "car_type": "[CAR_TYPE]",
        "status": "booking_complete",
        "price": "[TOTAL_PRICE]",
        "description": "Booking Complete"
    }
"""

# System Instructions to the Planner Agent
PLANNER_COT_INSTRUCTIONS = """
You are an ace trip planner.
You take the user input and create a trip plan, break the trip in to actionable task.
You will include 3 tasks in your plan, based on the user request.
1. Airfare Booking.
2. Hotel Booking.
3. Car Rental Booking.

Always use chain-of-thought reasoning before responding to track where you are 
in the decision tree and determine the next appropriate question.

Your question should follow the example format below
{
    "status": "input_required",
    "question": "What class of car do you prefer, Sedan, SUV or a Truck?"
}


DECISION TREE:
1. Origin
    - If unknown, ask for origin.
    - If there are multiple airports at origin, ask for preferred airport.
    - If known, proceed to step 2.
2. Destination
    - If unknown, ask for destination.
    - If there are multiple airports at origin, ask for preferred airport.
    - If known, proceed to step 3.
3. Dates
    - If unknown, ask for start and return dates.
    - If known, proceed to step 4.
4. Budget
    - If unknown, ask for budget.
    - If known, proceed to step 5.
5. Type of travel
    - If unknown, ask for type of travel. Business or Leisure.
    - If known, proceed to step 6.
6. No of travelers
    - If unknown, ask for the number of travelers.
    - If known, proceed to step 7.
7. Class
    - If unknown, ask for cabin class.
    - If known, proceed to step 8.
8. Checkin and Checkout dates
    - Use start and return dates for checkin and checkout dates.
    - Confirm with the user if they wish a different checkin and checkout dates.
    - Validate if the checkin and checkout dates are within the start and return dates.
    - If known and data is valid, proceed to step 9.
9. Property Type
    - If unknown, ask for the type of property. Hotel, AirBnB or a private property.
    - If known, proceed to step 10.
10. Room Type
    - If unknown, ask for the room type. Suite, Standard, Single, Double.
    - If known, proceed to step 11.
11. Car Rental Requirement
    - If unknown, ask if the user needs a rental car.
    - If known, proceed to step 12.
12. Type of car
    - If unknown, ask for the type of car. Sedan, SUV or a Truck.
    - If known, proceed to step 13.
13. Car Rental Pickup and return dates
    - Use start and return dates for pickup and return dates.
    - Confirm with the user if they wish a different pickup and return dates.
    - Validate if the pickup and return dates are within the start and return dates.
    - If known and data is valid, proceed to step 14.



CHAIN-OF-THOUGHT PROCESS:
Before each response, reason through:
1. What information do I already have? [List all known information]
2. What is the next unknown information in the decision tree? [Identify gap]
3. How should I naturally ask for this information? [Formulate question]
4. What context from previous information should I include? [Add context]
5. If I have all the information I need, I should now proceed to generating the tasks.

Your output should follow this example format. DO NOT add any thing else apart from the JSON format below.

{
    'original_query': 'Plan my trip to London',
    'trip_info':
    {
        'total_budget': '5000',
        'origin': 'San Francisco',
        'origin_airport': 'SFO',
        'destination': 'London',
        'destination_airport': 'LHR',
        'type': 'business',
        'start_date': '2025-05-12',
        'end_date': '2025-05-20',
        'travel_class': 'economy',
        'accommodation_type': 'Hotel',
        'room_type': 'Suite',
        'checkin_date': '2025-05-12',
        'checkout_date': '2025-05-20',
        'is_car_rental_required': 'Yes',
        'type_of_car': 'SUV',
        'no_of_travellers': '1'
    },
    'tasks': [
        {
            'id': 1,
            'description': 'Book round-trip economy class air tickets from San Francisco (SFO) to London (LHR) for the dates May 12, 2025 to May 20, 2025.',
            'status': 'pending'
        }, 
        {
            'id': 2,
            'description': 'Book a suite room at a hotel in London for checkin date May 12, 2025 and checkout date May 20th 2025',
            'status': 'pending'
        },
        {
            'id': 3,
            'description': 'Book an SUV rental car in London with a pickup on May 12, 2025 and return on May 20, 2025', 
            'status': 'pending'
        }
    ]
}

"""

# System Instructions to the Summary Generator
SUMMARY_COT_INSTRUCTIONS = """
    You are a travel booking assistant that creates comprehensive summaries of travel arrangements. 
    Use the following chain of thought process to systematically analyze the travel data provided in triple backticks generate a detailed summary.

    ## Chain of Thought Process

    ### Step 1: Data Parsing and Validation
    First, carefully parse the provided travel data:

    **Think through this systematically:**
    - Parse the data structure and identify all travel components

    ### Step 2: Flight Information Analysis
    **For each flight in the data, extract:**

    *Reasoning: I need to capture all flight details for complete air travel summary*

    - Route information (departure/arrival cities and airports)
    - Schedule details (dates, times, duration)
    - Airline information and flight numbers
    - Cabin class
    - Cost breakdown per passenger
    - Total cost

    ### Step 3: Hotel Information Analysis
    **For accommodation details, identify:**

    *Reasoning: Hotel information is essential for complete trip coordination*

    - Property name, and location
    - Check-in and check-out dates/times
    - Room type
    - Total nights and nightly rates
    - Total cost

    ### Step 4: Car Rental Analysis
    **For vehicle rental information, extract:**

    *Reasoning: Ground transportation affects the entire travel experience*

    - Rental company and vehicle details
    - Pickup and return locations/times
    - Rental duration and daily rates
    - Total cost

    ### Step 5: Budget Analysis
    **Calculate comprehensive cost breakdown:**

    *Reasoning: Financial summary helps with expense tracking and budget management*

    - Individual cost categories (flights, hotels, car rental)
    - Total trip cost and per-person costs
    - Budget comparison if original budget provided

    ## Input Travel Data:
    ```{travel_data}```

    ## Instructions:

    Based on the travel data provided above, use your chain of thought process to analyze the travel information and generate a comprehensive summary in the following format:

    ## Travel Booking Summary

    ### Trip Overview
    - **Travelers:** [Number from the travel data]
    - **Destination(s):** [Primary destinations]
    - **Travel Dates:** [Overall trip duration]

    **Outbound Journey:**
    - Route: [Departure]  [Arrival]
    - Date & Time: [Departure date/time] | Arrival: [Arrival date/time, if available]
    - Airline: [Airline] Flight [Number]
    - Class: [Cabin class]
    - Passengers: [Number]
    - Cost: [Outbound journey cost]

    **Return Journey:**
    - Route: [Departure]  [Arrival]
    - Date & Time: [Departure date/time] | Arrival: [Arrival date/time, if available]
    - Airline: [Airline] Flight [Number]
    - Class: [Cabin class]
    - Passengers: [Number]
    - Cost: [Return journey cost]

    ### Accommodation Details
    **Hotel:** [Hotel name]
    - **Location:** [City]
    - **Check-in:** [Date] at [Time]
    - **Check-out:** [Date] at [Time]
    - **Duration:** [Number] nights
    - **Room:** [Room type] for [Number] guests
    - **Rate:** [Nightly rate]  [Nights] = [Total cost]

    ### Ground Transportation
    **Car Rental:** [Company]
    - **Vehicle:** [Vehicle type/category]
    - **Pickup:** [Date/Time] from [Location]
    - **Return:** [Date/Time] to [Location]
    - **Duration:** [Number] days
    - **Rate:** [Daily rate]  [Days] = [Total cost]

    ### Financial Summary
    **Total Trip Cost:** [Currency] [Grand total]
    - Flights: [Currency] [Amount]
    - Accommodation: [Currency] [Amount]
    - Car Rental: [Currency] [Amount]

    **Per Person Cost:** [Currency] [Amount] *(if multiple travelers)*
    **Budget Status:** [Over/Under budget by amount, if original budget provided]
"""

QA_COT_PROMPT = """
You are an AI assistant that answers questions about trip details based on provided JSON context and the conversation history. Follow this step-by-step reasoning process:


Instructions:

Step 1: Context Analysis
    -- Carefully read and understand the provided Conversation History and the JSON context containing trip details
    -- Identify all available information fields (dates, locations, preferences, bookings, etc.)
    -- Note what information is present and what might be missing

Step 2: Question Understanding

    -- Parse the question to understand exactly what information is being requested
    -- Identify the specific data points needed to answer the question
    -- Determine if the question is asking for factual information, preferences, or derived conclusions

Step 3: Information Matching
    -- Search through the JSON context for relevant information
    -- Check if all required data points to answer the question are available
    -- Consider if partial information exists that could lead to an incomplete answer

Step 4: Answer Determination
    -- If all necessary information is present in the context: formulate a complete answer
    -- If some information is missing but a partial answer is possible: determine if it's sufficient
    -- If critical information is missing: conclude that the question cannot be answered

Step 5: Response Formatting
    -- Provide your response in this exact JSON format:

json

{"can_answer": "yes" or "no","answer": "Your answer here" or "Cannot answer based on provided context"}

Guidelines:

Strictly adhere to the context: Only use information explicitly provided in the JSON

No assumptions: Do not infer or assume information not present in the context

Be precise: Answer exactly what is asked, not more or less

Handle edge cases: If context is malformed or question is unclear, set can_answer to "no"

Example Process:

Context: {'total_budget': '9000', 'origin': 'San Francisco', 'destination': 'London', 'type': 'business', 'start_date': '2025-06-12', 'end_date': '2025-06-18', 'travel_class': 'business', 'accommodation_type': 'Hotel', 'room_type': 'Suite', 'is_car_rental_required': 'Yes', 'type_of_car': 'Sedan', 'no_of_travellers': '1', 'checkin_date': '2025-06-12', 'checkout_date': '2025-06-18', 'car_rental_start_date': '2025-06-12', 'car_rental_end_date': '2025-06-18'}

History: {"contextId":"b5a4f803-80f3-4524-b93d-b009219796ac","history":[{"contextId":"b5a4f803-80f3-4524-b93d-b009219796ac","kind":"message","messageId":"f4ced6dd-a7fd-4a4e-8f4a-30a37e62e81b","parts":[{"kind":"text","text":"Plan my trip to London"}],"role":"user","taskId":"a53e8d32-8119-4864-aba7-4ea1db39437d"}]}}


Question: "Do I need a rental car for this trip?"

Reasoning:

Context contains trip details with transportation preferences

Question asks about rental car requirement

Context shows "is_car_rental_required": yes

Information is directly available and complete

Response:

json

{"can_answer": "yes","answer": "Yes, the user needs a rental car for this trip"}

Now apply this reasoning process to answer questions based on the provided trip context.


Context: ```{TRIP_CONTEXT}```
History: ```{CONVERSATION_HISTORY}```
Question: ```{TRIP_QUESTION}```
"""

================
File: samples/python/agents/a2a_mcp/src/a2a_mcp/common/types.py
================
# type: ignore

from typing import Any, Literal

from pydantic import BaseModel, Field, model_validator


class ServerConfig(BaseModel):
    """Server Confgiguration."""

    host: str
    port: int
    transport: str
    url: str


class PlannerTask(BaseModel):
    """Represents a single task generated by the Planner."""

    id: int = Field(description='Sequential ID for the task.')
    description: str = Field(
        description='Clear description of the task to be executed.'
    )
    status: (
        Any
        | Literal[
            'input_required',
            'completed',
            'error',
            'pending',
            'incomplete',
            'todo',
            'not_started',
        ]
        | None
    ) = Field(description='Status of the task', default='input_required')


class TripInfo(BaseModel):
    """Trip Info."""

    total_budget: str | None = Field(description='Total Budget for the trip')
    origin: str | None = Field(description='Trip Origin')
    destination: str | None = Field(description='Trip destination')
    type: str | None = Field(description='Trip type, business or leisure')
    start_date: str | None = Field(description='Trip Start Date')
    end_date: str | None = Field(description='Trip End Date')
    travel_class: str | None = Field(
        description='Travel class, first, business or economy'
    )
    accommodation_type: str | None = Field(
        description='Luxury Hotel, Budget Hotel, AirBnB, etc'
    )
    room_type: str | None = Field(description='Suite, Single, Double etc.')
    is_car_rental_required: str | None = Field(
        description='Whether a rental car is required in the trip.'
    )
    type_of_car: str | None = Field(
        description='Type of the car, SUV, Sedan, Truck etc.'
    )
    no_of_travellers: str | None = Field(
        description='Total number of travellers in the trip'
    )

    checkin_date: str | None = Field(description='Hotel Checkin Date')
    checkout_date: str | None = Field(description='Hotel Checkout Date')
    car_rental_start_date: str | None = Field(
        description='Car Rental Start Date'
    )
    car_rental_end_date: str | None = Field(description='Car Rental End Date')

    @model_validator(mode='before')
    @classmethod
    def set_dependent_var(cls, values):
        """Pydantic dependent setters."""
        if isinstance(values, dict) and 'start_date' in values:
            values['checkin_date'] = values['start_date']

        if isinstance(values, dict) and 'end_date' in values:
            values['checkout_date'] = values['end_date']

        if isinstance(values, dict) and 'start_date' in values:
            values['car_rental_start_date'] = values['start_date']

        if isinstance(values, dict) and 'end_date' in values:
            values['car_rental_end_date'] = values['end_date']
        return values


class TaskList(BaseModel):
    """Output schema for the Planner Agent."""

    original_query: str | None = Field(
        description='The original user query for context.'
    )

    trip_info: TripInfo | None = Field(description='Trip information')

    tasks: list[PlannerTask] = Field(
        description='A list of tasks to be executed sequentially.'
    )


class AgentResponse(BaseModel):
    """Output schema for the Agent."""

    content: str | dict = Field(description='The content of the response.')
    is_task_complete: bool = Field(description='Whether the task is complete.')
    require_user_input: bool = Field(
        description='Whether the agent requires user input.'
    )

================
File: samples/python/agents/a2a_mcp/src/a2a_mcp/common/utils.py
================
# type: ignore
import logging
import os

import google.generativeai as genai

from a2a_mcp.common.types import ServerConfig


logger = logging.getLogger(__name__)


def init_api_key():
    """Initialize the API key for Google Generative AI."""
    if not os.getenv('GOOGLE_API_KEY'):
        logger.error('GOOGLE_API_KEY is not set')
        raise ValueError('GOOGLE_API_KEY is not set')

    genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))


def config_logging():
    """Configure basic logging."""
    log_level = (
        os.getenv('A2A_LOG_LEVEL') or os.getenv('FASTMCP_LOG_LEVEL') or 'INFO'
    ).upper()
    logging.basicConfig(level=getattr(logging, log_level, logging.INFO))


def config_logger(logger):
    """Logger specific config, avoiding clutter in enabling all loggging."""
    # TODO: replace with env
    logger.setLevel(logging.INFO)
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)

    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)


def get_mcp_server_config() -> ServerConfig:
    """Get the MCP server configuration."""
    return ServerConfig(
        host='localhost',
        port=10100,
        transport='sse',
        url='http://localhost:10100/sse',
    )

================
File: samples/python/agents/a2a_mcp/src/a2a_mcp/common/workflow.py
================
import json
import logging
import uuid

from collections.abc import AsyncIterable
from enum import Enum
from uuid import uuid4

import httpx
import networkx as nx

from a2a.client import A2AClient
from a2a.types import (
    AgentCard,
    MessageSendParams,
    SendStreamingMessageRequest,
    SendStreamingMessageSuccessResponse,
    TaskArtifactUpdateEvent,
    TaskState,
    TaskStatusUpdateEvent,
)
from a2a_mcp.common.utils import get_mcp_server_config
from a2a_mcp.mcp import client


logger = logging.getLogger(__name__)


class Status(Enum):
    """Represents the status of a workflow and its associated node."""

    READY = 'READY'
    RUNNING = 'RUNNING'
    COMPLETED = 'COMPLETED'
    PAUSED = 'PAUSED'
    INITIALIZED = 'INITIALIZED'


class WorkflowNode:
    """Represents a single node in a workflow graph.

    Each node encapsulates a specific task to be executed, such as finding an
    agent or invoking an agent's capabilities. It manages its own state
    (e.g., READY, RUNNING, COMPLETED, PAUSED) and can execute its assigned task.

    """

    def __init__(
        self,
        task: str,
        node_key: str | None = None,
        node_label: str | None = None,
    ):
        self.id = str(uuid.uuid4())
        self.node_key = node_key
        self.node_label = node_label
        self.task = task
        self.results = None
        self.state = Status.READY

    async def get_planner_resource(self) -> AgentCard | None:
        logger.info(f'Getting resource for node {self.id}')
        config = get_mcp_server_config()
        async with client.init_session(
            config.host, config.port, config.transport
        ) as session:
            response = await client.find_resource(
                session, 'resource://agent_cards/planner_agent'
            )
            data = json.loads(response.contents[0].text)
            return AgentCard(**data['agent_card'][0])

    async def find_agent_for_task(self) -> AgentCard | None:
        logger.info(f'Find agent for task - {self.task}')
        config = get_mcp_server_config()
        async with client.init_session(
            config.host, config.port, config.transport
        ) as session:
            result = await client.find_agent(session, self.task)
            agent_card_json = json.loads(result.content[0].text)
            logger.debug(f'Found agent {agent_card_json} for task {self.task}')
            return AgentCard(**agent_card_json)

    async def run_node(
        self,
        query: str,
        task_id: str,
        context_id: str,
    ) -> AsyncIterable[dict[str, any]]:
        logger.info(f'Executing node {self.id}')
        agent_card = None
        if self.node_key == 'planner':
            agent_card = await self.get_planner_resource()
        else:
            agent_card = await self.find_agent_for_task()
        async with httpx.AsyncClient() as httpx_client:
            client = A2AClient(httpx_client, agent_card)

            payload: dict[str, any] = {
                'message': {
                    'role': 'user',
                    'parts': [{'kind': 'text', 'text': query}],
                    'messageId': uuid4().hex,
                    'taskId': task_id,
                    'contextId': context_id,
                },
            }
            request = SendStreamingMessageRequest(
                id=str(uuid4()), params=MessageSendParams(**payload)
            )
            response_stream = client.send_message_streaming(request)
            async for chunk in response_stream:
                # Save the artifact as a result of the node
                if isinstance(
                    chunk.root, SendStreamingMessageSuccessResponse
                ) and (isinstance(chunk.root.result, TaskArtifactUpdateEvent)):
                    artifact = chunk.root.result.artifact
                    self.results = artifact
                yield chunk


class WorkflowGraph:
    """Represents a graph of workflow nodes."""

    def __init__(self) -> None:
        self.graph = nx.DiGraph()
        self.nodes = {}
        self.latest_node = None
        self.node_type = None
        self.state = Status.INITIALIZED
        self.paused_node_id = None

    def add_node(self, node) -> None:
        logger.info(f'Adding node {node.id}')
        self.graph.add_node(node.id, query=node.task)
        self.nodes[node.id] = node
        self.latest_node = node.id

    def add_edge(self, from_node_id: str, to_node_id: str) -> None:
        if from_node_id not in self.nodes or to_node_id not in self.nodes:
            raise ValueError('Invalid node IDs')

        self.graph.add_edge(from_node_id, to_node_id)

    async def run_workflow(
        self, start_node_id: str | None = None
    ) -> AsyncIterable[dict[str, any]]:
        logger.info('Executing workflow graph')
        if not start_node_id or start_node_id not in self.nodes:
            start_nodes = [n for n, d in self.graph.in_degree() if d == 0]
        else:
            start_nodes = [self.nodes[start_node_id].id]

        applicable_graph = set()

        for node_id in start_nodes:
            applicable_graph.add(node_id)
            applicable_graph.update(nx.descendants(self.graph, node_id))

        complete_graph = list(nx.topological_sort(self.graph))
        sub_graph = [n for n in complete_graph if n in applicable_graph]
        logger.info(f'Sub graph {sub_graph} size {len(sub_graph)}')
        self.state = Status.RUNNING
        # Alternative is to loop over all nodes, but we only need the connected nodes.
        for node_id in sub_graph:
            node = self.nodes[node_id]
            node.state = Status.RUNNING
            query = self.graph.nodes[node_id].get('query')
            task_id = self.graph.nodes[node_id].get('task_id')
            context_id = self.graph.nodes[node_id].get('context_id')
            async for chunk in node.run_node(query, task_id, context_id):
                # When the workflow node is paused, do not yield any chunks
                # but, let the loop complete.
                if node.state != Status.PAUSED:
                    if isinstance(
                        chunk.root, SendStreamingMessageSuccessResponse
                    ) and (
                        isinstance(chunk.root.result, TaskStatusUpdateEvent)
                    ):
                        task_status_event = chunk.root.result
                        context_id = task_status_event.context_id
                        if (
                            task_status_event.status.state
                            == TaskState.input_required
                            and context_id
                        ):
                            node.state = Status.PAUSED
                            self.state = Status.PAUSED
                            self.paused_node_id = node.id
                    yield chunk
            if self.state == Status.PAUSED:
                break
            if node.state == Status.RUNNING:
                node.state = Status.COMPLETED
        if self.state == Status.RUNNING:
            self.state = Status.COMPLETED

    def set_node_attribute(self, node_id, attribute, value) -> None:
        nx.set_node_attributes(self.graph, {node_id: value}, attribute)

    def set_node_attributes(self, node_id, attr_val) -> None:
        nx.set_node_attributes(self.graph, {node_id: attr_val})

    def is_empty(self) -> bool:
        return self.graph.number_of_nodes() == 0

================
File: samples/python/agents/a2a_mcp/src/a2a_mcp/mcp/client.py
================
# type:ignore
import asyncio
import json
import os

from contextlib import asynccontextmanager

import click

from fastmcp.utilities.logging import get_logger
from mcp import ClientSession, StdioServerParameters
from mcp.client.sse import sse_client
from mcp.client.stdio import stdio_client
from mcp.types import CallToolResult, ReadResourceResult


logger = get_logger(__name__)

env = {
    'GOOGLE_API_KEY': os.getenv('GOOGLE_API_KEY'),
}


@asynccontextmanager
async def init_session(host, port, transport):
    """Initializes and manages an MCP ClientSession based on the specified transport.

    This asynchronous context manager establishes a connection to an MCP server
    using either Server-Sent Events (SSE) or Standard I/O (STDIO) transport.
    It handles the setup and teardown of the connection and yields an active
    `ClientSession` object ready for communication.

    Args:
        host: The hostname or IP address of the MCP server (used for SSE).
        port: The port number of the MCP server (used for SSE).
        transport: The communication transport to use ('sse' or 'stdio').

    Yields:
        ClientSession: An initialized and ready-to-use MCP client session.

    Raises:
        ValueError: If an unsupported transport type is provided (implicitly,
                    as it won't match 'sse' or 'stdio').
        Exception: Other potential exceptions during client initialization or
                   session setup.
    """
    if transport == 'sse':
        url = f'http://{host}:{port}/sse'
        async with sse_client(url) as (read_stream, write_stream):
            async with ClientSession(
                read_stream=read_stream, write_stream=write_stream
            ) as session:
                logger.debug('SSE ClientSession created, initializing...')
                await session.initialize()
                logger.info('SSE ClientSession initialized successfully.')
                yield session
    elif transport == 'stdio':
        if not os.getenv('GOOGLE_API_KEY'):
            logger.error('GOOGLE_API_KEY is not set')
            raise ValueError('GOOGLE_API_KEY is not set')
        stdio_params = StdioServerParameters(
            command='uv',
            args=['run', 'a2a-mcp'],
            env=env,
        )
        async with stdio_client(stdio_params) as (read_stream, write_stream):
            async with ClientSession(
                read_stream=read_stream,
                write_stream=write_stream,
            ) as session:
                logger.debug('STDIO ClientSession created, initializing...')
                await session.initialize()
                logger.info('STDIO ClientSession initialized successfully.')
                yield session
    else:
        logger.error(f'Unsupported transport type: {transport}')
        raise ValueError(
            f"Unsupported transport type: {transport}. Must be 'sse' or 'stdio'."
        )


async def find_agent(session: ClientSession, query) -> CallToolResult:
    """Calls the 'find_agent' tool on the connected MCP server.

    Args:
        session: The active ClientSession.
        query: The natural language query to send to the 'find_agent' tool.

    Returns:
        The result of the tool call.
    """
    logger.info(f"Calling 'find_agent' tool with query: '{query[:50]}...'")
    return await session.call_tool(
        name='find_agent',
        arguments={
            'query': query,
        },
    )


async def find_resource(session: ClientSession, resource) -> ReadResourceResult:
    """Reads a resource from the connected MCP server.

    Args:
        session: The active ClientSession.
        resource: The URI of the resource to read (e.g., 'resource://agent_cards/list').

    Returns:
        The result of the resource read operation.
    """
    logger.info(f'Reading resource: {resource}')
    return await session.read_resource(resource)


async def search_flights(session: ClientSession) -> CallToolResult:
    """Calls the 'search_flights' tool on the connected MCP server.

    Args:
        session: The active ClientSession.
        query: The natural language query to send to the 'search_flights' tool.

    Returns:
        The result of the tool call.
    """
    # TODO: Implementation pending
    logger.info("Calling 'search_flights' tool'")
    return await session.call_tool(
        name='search_flights',
        arguments={
            'departure_airport': 'SFO',
            'arrival_airport': 'LHR',
            'start_date': '2025-06-03',
            'end_date': '2025-06-09',
        },
    )


async def search_hotels(session: ClientSession) -> CallToolResult:
    """Calls the 'search_hotels' tool on the connected MCP server.

    Args:
        session: The active ClientSession.
        query: The natural language query to send to the 'search_hotels' tool.

    Returns:
        The result of the tool call.
    """
    # TODO: Implementation pending
    logger.info("Calling 'search_hotels' tool'")
    return await session.call_tool(
        name='search_hotels',
        arguments={
            'location': 'A Suite room in St Pancras Square in London',
            'check_in_date': '2025-06-03',
            'check_out_date': '2025-06-09',
        },
    )


async def query_db(session: ClientSession) -> CallToolResult:
    """Calls the 'query' tool on the connected MCP server.

    Args:
        session: The active ClientSession.
        query: The natural language query to send to the 'query_db' tool.

    Returns:
        The result of the tool call.
    """
    logger.info("Calling 'query_db' tool'")
    return await session.call_tool(
        name='query_travel_data',
        arguments={
            'query': "SELECT id, name, city, hotel_type, room_type, price_per_night FROM hotels WHERE city='London'",
        },
    )


# Test util
async def main(host, port, transport, query, resource, tool):
    """Main asynchronous function to connect to the MCP server and execute commands.

    Used for local testing.

    Args:
        host: Server hostname.
        port: Server port.
        transport: Connection transport ('sse' or 'stdio').
        query: Optional query string for the 'find_agent' tool.
        resource: Optional resource URI to read.
        tool: Optional tool name to execute. Valid options are:
            'search_flights', 'search_hotels', or 'query_db'.
    """
    logger.info('Starting Client to connect to MCP')
    async with init_session(host, port, transport) as session:
        if query:
            result = await find_agent(session, query)
            data = json.loads(result.content[0].text)
            logger.info(json.dumps(data, indent=2))
        if resource:
            result = await find_resource(session, resource)
            logger.info(result)
            data = json.loads(result.contents[0].text)
            logger.info(json.dumps(data, indent=2))
        if tool:
            if tool == 'search_flights':
                results = await search_flights(session)
                logger.info(results.model_dump())
            if tool == 'search_hotels':
                result = await search_hotels(session)
                data = json.loads(result.content[0].text)
                logger.info(json.dumps(data, indent=2))
            if tool == 'query_db':
                result = await query_db(session)
                logger.info(result)
                data = json.loads(result.content[0].text)
                logger.info(json.dumps(data, indent=2))


# Command line tester
@click.command()
@click.option('--host', default='localhost', help='SSE Host')
@click.option('--port', default='10100', help='SSE Port')
@click.option('--transport', default='stdio', help='MCP Transport')
@click.option('--find_agent', help='Query to find an agent')
@click.option('--resource', help='URI of the resource to locate')
@click.option('--tool_name', type=click.Choice(['search_flights', 'search_hotels', 'query_db']),
              help='Tool to execute: search_flights, search_hotels, or query_db')
def cli(host, port, transport, find_agent, resource, tool_name):
    """A command-line client to interact with the Agent Cards MCP server."""
    asyncio.run(main(host, port, transport, find_agent, resource, tool_name))


if __name__ == '__main__':
    cli()

================
File: samples/python/agents/a2a_mcp/src/a2a_mcp/mcp/server.py
================
# type: ignore
import json
import os
import sqlite3
import traceback

from pathlib import Path

import google.generativeai as genai
import numpy as np
import pandas as pd
import requests

from a2a_mcp.common.utils import init_api_key
from mcp.server.fastmcp import FastMCP
from mcp.server.fastmcp.utilities.logging import get_logger


logger = get_logger(__name__)
AGENT_CARDS_DIR = 'agent_cards'
MODEL = 'models/embedding-001'
SQLLITE_DB = 'travel_agency.db'
PLACES_API_URL = 'https://places.googleapis.com/v1/places:searchText'


def generate_embeddings(text):
    """Generates embeddings for the given text using Google Generative AI.

    Args:
        text: The input string for which to generate embeddings.

    Returns:
        A list of embeddings representing the input text.
    """
    return genai.embed_content(
        model=MODEL,
        content=text,
        task_type='retrieval_document',
    )['embedding']


def load_agent_cards():
    """Loads agent card data from JSON files within a specified directory.

    Returns:
        A list containing JSON data from an agent card file found in the specified directory.
        Returns an empty list if the directory is empty, contains no '.json' files,
        or if all '.json' files encounter errors during processing.
    """
    card_uris = []
    agent_cards = []
    dir_path = Path(AGENT_CARDS_DIR)
    if not dir_path.is_dir():
        logger.error(
            f'Agent cards directory not found or is not a directory: {AGENT_CARDS_DIR}'
        )
        return agent_cards

    logger.info(f'Loading agent cards from card repo: {AGENT_CARDS_DIR}')

    for filename in os.listdir(AGENT_CARDS_DIR):
        if filename.lower().endswith('.json'):
            file_path = dir_path / filename

            if file_path.is_file():
                logger.info(f'Reading file: {filename}')
                try:
                    with file_path.open('r', encoding='utf-8') as f:
                        data = json.load(f)
                        card_uris.append(
                            f'resource://agent_cards/{Path(filename).stem}'
                        )
                        agent_cards.append(data)
                except json.JSONDecodeError as jde:
                    logger.error(f'JSON Decoder Error {jde}')
                except OSError as e:
                    logger.error(f'Error reading file {filename}: {e}.')
                except Exception as e:
                    logger.error(
                        f'An unexpected error occurred processing {filename}: {e}',
                        exc_info=True,
                    )
    logger.info(
        f'Finished loading agent cards. Found {len(agent_cards)} cards.'
    )
    return card_uris, agent_cards


def build_agent_card_embeddings() -> pd.DataFrame:
    """Loads agent cards, generates embeddings for them, and returns a DataFrame.

    Returns:
        Optional[pd.DataFrame]: A Pandas DataFrame containing the original
        'agent_card' data and their corresponding 'Embeddings'. Returns None
        if no agent cards were loaded initially or if an exception occurred
        during the embedding generation process.
    """
    card_uris, agent_cards = load_agent_cards()
    logger.info('Generating Embeddings for agent cards')
    try:
        if agent_cards:
            df = pd.DataFrame(
                {'card_uri': card_uris, 'agent_card': agent_cards}
            )
            df['card_embeddings'] = df.apply(
                lambda row: generate_embeddings(json.dumps(row['agent_card'])),
                axis=1,
            )
            return df
        logger.info('Done generating embeddings for agent cards')
    except Exception as e:
        logger.error(f'An unexpected error occurred : {e}.', exc_info=True)
        return None


def serve(host, port, transport):  # noqa: PLR0915
    """Initializes and runs the Agent Cards MCP server.

    Args:
        host: The hostname or IP address to bind the server to.
        port: The port number to bind the server to.
        transport: The transport mechanism for the MCP server (e.g., 'stdio', 'sse').

    Raises:
        ValueError: If the 'GOOGLE_API_KEY' environment variable is not set.
    """
    init_api_key()
    logger.info('Starting Agent Cards MCP Server')
    mcp = FastMCP('agent-cards', host=host, port=port)

    df = build_agent_card_embeddings()

    @mcp.tool(
        name='find_agent',
        description='Finds the most relevant agent card based on a natural language query string.',
    )
    def find_agent(query: str) -> str:
        """Finds the most relevant agent card based on a query string.

        This function takes a user query, typically a natural language question or a task generated by an agent,
        generates its embedding, and compares it against the
        pre-computed embeddings of the loaded agent cards. It uses the dot
        product to measure similarity and identifies the agent card with the
        highest similarity score.

        Args:
            query: The natural language query string used to search for a
                   relevant agent.

        Returns:
            The json representing the agent card deemed most relevant
            to the input query based on embedding similarity.
        """
        query_embedding = genai.embed_content(
            model=MODEL, content=query, task_type='retrieval_query'
        )
        dot_products = np.dot(
            np.stack(df['card_embeddings']), query_embedding['embedding']
        )
        best_match_index = np.argmax(dot_products)
        logger.debug(
            f'Found best match at index {best_match_index} with score {dot_products[best_match_index]}'
        )
        return df.iloc[best_match_index]['agent_card']

    @mcp.tool()
    def query_places_data(query: str):
        """Query Google Places."""
        logger.info(f'Search for places : {query}')
        api_key = os.getenv('GOOGLE_PLACES_API_KEY')
        if not api_key:
            logger.info('GOOGLE_PLACES_API_KEY is not set')
            return {'places': []}

        headers = {
            'X-Goog-Api-Key': api_key,
            'X-Goog-FieldMask': 'places.id,places.displayName,places.formattedAddress',
            'Content-Type': 'application/json',
        }
        payload = {
            'textQuery': query,
            'languageCode': 'en',
            'maxResultCount': 10,
        }

        try:
            response = requests.post(
                PLACES_API_URL, headers=headers, json=payload
            )
            response.raise_for_status()
            return response.json()
        except requests.exceptions.HTTPError as http_err:
            logger.info(f'HTTP error occurred: {http_err}')
            logger.info(f'Response content: {response.text}')
        except requests.exceptions.ConnectionError as conn_err:
            logger.info(f'Connection error occurred: {conn_err}')
        except requests.exceptions.Timeout as timeout_err:
            logger.info(f'Timeout error occurred: {timeout_err}')
        except requests.exceptions.RequestException as req_err:
            logger.info(
                f'An unexpected error occurred with the request: {req_err}'
            )
        except json.JSONDecodeError:
            logger.info(
                f'Failed to decode JSON response. Raw response: {response.text}'
            )

        return {'places': []}

    @mcp.tool()
    def query_travel_data(query: str) -> dict:
        """ "name": "query_travel_data",
        "description": "Retrieves the most up-to-date, ariline, hotel and car rental availability. Helps with the booking.
        This tool should be used when a user asks for the airline ticket booking, hotel or accommodation booking, or car rental reservations.",
        "parameters": {
            "type": "object",
            "properties": {
            "query": {
                "type": "string",
                "description": "A SQL to run against the travel database."
            }
            },
            "required": ["query"]
        }
        """
        # The above is to influence gemini to pickup the tool.
        logger.info(f'Query sqllite : {query}')

        if not query or not query.strip().upper().startswith('SELECT'):
            raise ValueError(f'In correct query {query}')

        try:
            with sqlite3.connect(SQLLITE_DB) as conn:
                conn.row_factory = sqlite3.Row
                cursor = conn.cursor()
                cursor.execute(query)
                rows = cursor.fetchall()
                result = {'results': [dict(row) for row in rows]}
                return json.dumps(result)
        except Exception as e:
            logger.error(f'Exception running query {e}')
            logger.error(traceback.format_exc())
            if 'no such column' in e:
                return {
                    'error': f'Please check your query, {e}. Use the table schema to regenerate the query'
                }
            return {'error': {e}}

    @mcp.resource('resource://agent_cards/list', mime_type='application/json')
    def get_agent_cards() -> dict:
        """Retrieves all loaded agent cards as a json / dictionary for the MCP resource endpoint.

        This function serves as the handler for the MCP resource identified by
        the URI 'resource://agent_cards/list'.

        Returns:
            A json / dictionary structured as {'agent_cards': [...]}, where the value is a
            list containing all the loaded agent card dictionaries. Returns
            {'agent_cards': []} if the data cannot be retrieved.
        """
        resources = {}
        logger.info('Starting read resources')
        resources['agent_cards'] = df['card_uri'].to_list()
        return resources

    @mcp.resource(
        'resource://agent_cards/{card_name}', mime_type='application/json'
    )
    def get_agent_card(card_name: str) -> dict:
        """Retrieves an agent card as a json / dictionary for the MCP resource endpoint.

        This function serves as the handler for the MCP resource identified by
        the URI 'resource://agent_cards/{card_name}'.

        Returns:
            A json / dictionary
        """
        resources = {}
        logger.info(
            f'Starting read resource resource://agent_cards/{card_name}'
        )
        resources['agent_card'] = (
            df.loc[
                df['card_uri'] == f'resource://agent_cards/{card_name}',
                'agent_card',
            ]
        ).to_list()

        return resources

    logger.info(
        f'Agent cards MCP Server at {host}:{port} and transport {transport}'
    )
    mcp.run(transport=transport)

================
File: samples/python/agents/a2a_mcp/src/a2a_mcp/__init__.py
================
"""Convenience methods to start servers."""

import click

from a2a_mcp.mcp import server


@click.command()
@click.option('--run', 'command', default='mcp-server', help='Command to run')
@click.option(
    '--host',
    'host',
    default='localhost',
    help='Host on which the server is started or the client connects to',
)
@click.option(
    '--port',
    'port',
    default=10100,
    help='Port on which the server is started or the client connects to',
)
@click.option(
    '--transport',
    'transport',
    default='stdio',
    help='MCP Transport',
)
def main(command, host, port, transport) -> None:
    # TODO: Add other servers, perhaps dynamic port allocation
    if command == 'mcp-server':
        server.serve(host, port, transport)
    else:
        raise ValueError(f'Unknown run option: {command}')

================
File: samples/python/agents/a2a_mcp/pyproject.toml
================
[project]
name = "a2a-mcp"
version = "0.1.0"
description = "A2A - MCP Sample"
readme = "README.md"
requires-python = ">=3.13"
dependencies = [
    "a2a-sdk[sql]>=0.3.0",
    "click>=8.1.8",
    "fastmcp>=1.0",
    "google-adk>=1.0.0",
    "google-cloud-aiplatform>=1.91.0",
    "google-generativeai>=0.8.5",
    "httpx>=0.28.1",
    "langchain-google-genai>=2.0.10",
    "langchain-mcp-adapters>=0.0.9",
    "langgraph>=0.4.1",
    "mcp[cli]>=1.5.0",
    "nest-asyncio>=1.6.0",
    "networkx>=3.4.2",
    "numpy>=2.2.5",
    "pandas>=2.2.3",
    "pydantic>=2.11.4",
    "litellm",
]

[project.scripts]
a2a-mcp = "a2a_mcp:main"

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

================
File: samples/python/agents/a2a_mcp/README.md
================
# A2A with MCP as Registry

**Leveraging Model Context Protocol (MCP) as a standardized mechanism for discovering and retrieving Google A2A Agent Cards, enabling dynamic agent interaction using A2A.**

## Table of Contents

- [A2A with MCP as Registry](#a2a-with-mcp-as-registry)
  - [Table of Contents](#table-of-contents)
  - [Objective](#objective)
  - [Background](#background)
    - [A2A Protocol](#a2a-protocol)
    - [Model Context Protocol (MCP)](#model-context-protocol-mcp)
  - [Core Proposal](#core-proposal)
    - [Storing Agent Cards](#storing-agent-cards)
    - [Discovering Agents via MCP](#discovering-agents-via-mcp)
    - [Retrieving Agent Cards](#retrieving-agent-cards)
    - [Finding an Agent for a Task](#finding-an-agent-for-a-task)
    - [Initiating A2A Communication](#initiating-a2a-communication)
  - [Use Case: Orchestrated Task Execution](#use-case-orchestrated-task-execution)
    - [Core Concepts](#core-concepts)
    - [Architectural Components](#architectural-components)
  - [Example Flow: Travel Agent](#example-flow-travel-agent)
  - [Steps to execute the example](#steps-to-execute-the-example)
    - [File/Directory Descriptions](#filedirectory-descriptions)
  - [Disclaimer](#disclaimer)

## Objective

To leverage Model Context Protocol (MCP) as a standardized mechanism for discovering and retrieving Google A2A Agent Cards. This enables dynamic agent interaction, particularly for planning and orchestration agents that utilize the A2A protocol.

## Background

### A2A Protocol

The Agent-to-Agent (A2A) protocol standardizes runtime communication between agents. It defines:

- **Agent Card:** A JSON schema describing an agent's identity, capabilities (actions/functions), and interaction endpoints.
- **Message Formats & Interaction Flows:** Such as `ExecuteTask` for direct agent-to-agent interaction.

### Model Context Protocol (MCP)

MCP defines a standard way for applications (including AI models) to discover, access, and utilize contextual information, referred to as "tools", "resources", etc.

## Core Proposal

The central idea is to use an MCP server as a centralized, queryable repository for A2A Agent Cards.

### Storing Agent Cards

- Each A2A Agent Card (JSON) is stored (e.g., as a JSON file).
- The MCP server exposes these Agent Cards as resources.
- The underlying storage system could be a file system, database or even a vector store. This example uses agent cards stored in a file system, generates embeddings and uses them to find matches.

### Discovering Agents via MCP

- Clients query the MCP server's resource API (`list_resources`) to discover available agent cards.
- Filtering can be applied using additional metadata (e.g., `streaming` support, tags like `currency conversion`), though not explicitly covered in this example.

### Retrieving Agent Cards

- The requesting agent uses resource URIs (obtained from discovery) to fetch the full JSON Agent Card(s) via the MCP server API.

### Finding an Agent for a Task

- Requesting agents can use tools exposed on the MCP server to find the most relevant agent for a specific query.

### Initiating A2A Communication

- Once Agent Card(s) are retrieved, the requesting agent uses them in an A2AClient.
- Agents (like a Planning Agent) needing collaborators then use the standard A2A protocol to communicate directly with target agents.
- MCP is not involved in this direct runtime interaction after discovery.

## Use Case: Orchestrated Task Execution

This system enables a workflow where specialized agents collaborate dynamically.

### Core Concepts

1. **Orchestration:** Planner and Executor agents manage the overall flow of a user query.
2. **Specialization:** Task Agents are experts in specific types of tasks.
3. **Dynamic Discovery:** The MCP Server allows for flexible addition, removal, or updates of Task Agents without modifying the Executor.
4. **Standardized Communication:** The A2A protocol ensures reliable inter-agent communication.

### Architectural Components

1. **User Interface (UI) / Application Gateway:** Entry point for user queries.
2. **Orchestrator Agent:**
   - Receives a structured plan from the Planner Agent.
   - Iterates through tasks.
   - For each task:
     - Queries the MCP Server for a suitable Task Agent based task (additionally capabilities).
     - Connects and sends task details to the Task Agent via A2A.
     - Receives results from the Task Agent via A2A.
     - Manages task state and errors.
   - Validates the results and potentially triggers replanning as needed.
   - Synthesizes, summarizes, and formats results into a coherent user response.
3. **Planner Agent:**
   - Receives the raw user query.
   - Decomposes the query into a structured plan of tasks (potentially a DAG), specifying required capabilities for each.
4. **Model Context Protocol (MCP) Server:**
   - Acts as a registry for Task Agents, hosting their Agent Cards.
   - Provides an interface for the Executor Agent to query for agents.
   - Provides an interface for the Executor Agent to query for tools.
5. **Task Agents (Pool/Fleet):**
   - Independent, specialized agents (e.g., Search Agent, Calculation Agent).
   - Expose A2A-compatible endpoints.
   - Execute tasks and return results to the Executor via A2A.
6. **A2A Communication Layer:** The underlying protocol for inter-agent communication.

```mermaid
flowchart LR
 subgraph s1["User Facing Layer"]
        UI["User Interface / Gateway"]
  end
 subgraph s3["Agent / Tool Discovery"]
        MCP["MCP Server (Agent / Tool Registry)"]
  end
 subgraph s4["Task Execution Layer"]
        T1["Task Agent 1 (e.g., Air Tickets)"]
        T2["Task Agent 2 (e.g., Hotel Reservation)"]
        T3["Task Agent N (e.g., Car Rental Reservation)"]
  end
    UI --> E["Orchestrator Agent"]
    E --> P["Planner Agent"] & MCP & A["Summary Agent"] & T1 & T2 & T3 & UI
    T2 --> E & MCP
    T3 --> E & MCP
    T1 --> MCP
     T1:::taskAgent
     T1:::Sky
     T2:::taskAgent
     T2:::Sky
     T3:::taskAgent
     T3:::Sky
    classDef taskAgent fill:#f9f,stroke:#333,stroke-width:2px
    classDef a2aLink stroke-width:2px,stroke:#0077cc,stroke-dasharray: 5 5
    classDef Sky stroke-width:1px, stroke-dasharray:none, stroke:#374D7C, fill:#E2EBFF, color:#374D7C
```

## Example Flow: Travel Agent

1. User requests a trip plan.
2. **Orchestrator Agent** receives the request.
   1. Looks up the **Planner Agent**'s card via MCP and connects.
   2. Invokes the Planner Agent to get a detailed plan.
   3. For each step in the plan:
      1. Invokes an MCP tool (e.g., `find_agent`) to fetch the Agent Card of the best Task Agent.
      2. Invokes the selected Task Agent via A2A to execute the task:
         - _Air Tickets:_ Task Agent will use a helper tool from the MCP server. The tool queries a SQLLite database to find the flights.
         - _Hotels:_ Task Agent will use a helper tool from the MCP server.
           The tool queries a SQLLite database to find the hotels.
         - _Car Rental:_ Task Agent will use a helper tool from the MCP server.
           The tool queries a SQLLite database to find the rental cars.
      3. Stores the results in memory
   4. Aggregates results and summarizes them for the client.
   5. If the agent discovers budget mismatch or failures in booking, a re-planning task is kicked off.

## Steps to execute the example

This sample is built using 3 ADK agents to execute the tasks and a LangGraph agent that works as a planner.
All the 3 ADK agents use the same python code but are instantiated with different agent cards.

You can execute the following command to run all of the steps in one terminal:

```sh
bash samples/python/agents/a2a_mcp/run.sh
```

1. Start the MCP Server:

   ```sh
   cd samples/python/agents/a2a_mcp
   uv venv # (if not already done)
   source .venv/bin/activate
   # Runs on port 10100 by default, change as needed by setting the --host and --port parameters.
   uv run  --env-file .env a2a-mcp --run mcp-server --transport sse
   ```

2. Start the Orchestrator Agent:

   In a new terminal window

   ```bash
   cd samples/python/agents/a2a_mcp
   uv venv # (if not already done)
   source .venv/bin/activate
   # Note: Change the host and port as needed.
   uv run --env-file .env src/a2a_mcp/agents/ --agent-card agent_cards/orchestrator_agent.json --port 10101
   ```

3. Start the Planner Agent:

   In a new terminal window

   ```bash
   cd samples/python/agents/a2a_mcp
   uv venv # (if not already done)
   source .venv/bin/activate
   # Note: Change the host and port as needed.
   uv run  --env-file .env src/a2a_mcp/agents/ --agent-card agent_cards/planner_agent.json --port 10102
   ```

4. Start the Airline Ticketing Agent:

   In a new terminal window

   ```bash
   cd samples/python/agents/a2a_mcp
   uv venv # (if not already done)
   source .venv/bin/activate
   # Note: Change the host and port as needed.
   uv run --env-file .env src/a2a_mcp/agents/ --agent-card agent_cards/air_ticketing_agent.json --port 10103
   ```

5. Start the Hotel Reservations Agent:

   In a new terminal window

   ```bash
   cd samples/python/agents/a2a_mcp
   uv venv # (if not already done)
   source .venv/bin/activate
   # Note: Change the host and port as needed.
   uv run  --env-file .env src/a2a_mcp/agents/ --agent-card agent_cards/hotel_booking_agent.json --port 10104
   ```

6. Start the Car Rental Reservations Agent:

   In a new terminal window

   ```bash
   cd samples/python/agents/a2a_mcp
   uv venv  # (if not already done)
   source .venv/bin/activate
   # Note: Change the host and port as needed.
   uv run --env-file .env src/a2a_mcp/agents/ --agent-card agent_cards/car_rental_agent.json --port 10105
   ```

7. Start the cli:

   In a new terminal window

   ```bash
   cd samples/python/agents/a2a_mcp
   uv venv  # (if not already done)
   source .venv/bin/activate

   uv run --env-file .env src/a2a_mcp/mcp/client.py --resource "resource://agent_cards/list" --find_agent "I would like to plan a trip to France."
   ```


### File/Directory Descriptions

- **`agent_cards/`**: This directory stores the JSON schemas for each A2A Agent Card. These cards define the identity, capabilities, and endpoints of the different agents in the system. The MCP server serves these cards.

  - `*_agent.json`: Each JSON file represents a specific agent's card (e.g., `air_ticketing_agent.json` for the agent that handles flight bookings).

- **`src/a2a_mcp/`**: The primary Python source code for this A2A with MCP sample.

  - **`agents/`**: Contains the Python implementations of the different types of agents described in the design document.
    - `__main__.py`: Main script to start up the agent services.
    - `adk_travel_agent.py`: This is the core travel agent built using ADK, different agent cards are used to instantiate the agent.
    - `langgraph_planner_agent.py`: The implementation of the "Planner Agent" using LangGraph, responsible for breaking down user requests into structured plans.
    - `orchestrator_agent.py`: The implementation of the "Orchestrator Agent," which takes the plan from the Planner, discovers appropriate Task Agents via MCP, and invokes them using A2A.
  - **`common/`**: Holds shared code used by multiple agents or parts of the system.
    - `agent_executor.py`: An A2A module to manage the state, dependencies, and execution of tasks within the orchestration flow.
    - `agent_runner.py`: A utility or framework component for running ADK agent instances, managing their lifecycle, and exposing their services.
    - `base_agent.py`: An abstract base class or interface defining common methods and properties for all agents in this project.
    - `prompts.py`: Contains predefined prompt templates used for interacting with Large Language Models (LLMs) within the agents (e.g., for planning or summarization).
    - `types.py`: Defines custom Python data types, Pydantic models, or Enums used across the project (e.g., for representing tasks, agent card structures, or API requests/responses).
    - `utils.py`: A collection of general-purpose utility functions.
    - `workflow.py`: Workflow to manage the process flow.
  - **`mcp/`**: Contains the implementation related to the Model Context Protocol.
    - `client.py`: A helper MCP client library that used to query the MCP server for agent cards or tools. This is a test utility and not used by the agents.
    - `server.py`: The implementation of the MCP server itself. This server hosts the agent cards as resources.

- **`travel_agency.db`**: A light weight SQLLite DB that hosts the demo data.

## Disclaimer
Important: The sample code provided is for demonstration purposes and illustrates the mechanics of the Agent-to-Agent (A2A) protocol. When building production applications, it is critical to treat any agent operating outside of your direct control as a potentially untrusted entity.

All data received from an external agentincluding but not limited to its AgentCard, messages, artifacts, and task statusesshould be handled as untrusted input. For example, a malicious agent could provide an AgentCard containing crafted data in its fields (e.g., description, name, skills.description). If this data is used without sanitization to construct prompts for a Large Language Model (LLM), it could expose your application to prompt injection attacks.  Failure to properly validate and sanitize this data before use can introduce security vulnerabilities into your application.

Developers are responsible for implementing appropriate security measures, such as input validation and secure handling of credentials to protect their systems and users.

================
File: samples/python/agents/a2a_mcp/run.sh
================
#!/bin/bash

# A script to automate the execution of the a2a_mcp example.
# It starts all necessary servers and agents in the background,
# runs the client, and then cleans up all background processes.

# Exit immediately if a command exits with a non-zero status.
set -e

# --- Configuration ---
# The main working directory for the example
WORK_DIR="samples/python/agents/a2a_mcp"
# Directory to store log files for background processes
LOG_DIR="logs"


# --- Cleanup Function ---
# This function is called automatically when the script exits (for any reason)
# to ensure all background processes are terminated.
cleanup() {
    echo ""
    echo "Shutting down background processes..."
    # Check if the pids array is not empty
    if [ ${#pids[@]} -ne 0 ]; then
        # Kill all processes using their PIDs stored in the array.
        # The 2>/dev/null suppresses "Terminated" messages or errors if a process is already gone.
        kill "${pids[@]}" 2>/dev/null
        wait "${pids[@]}" 2>/dev/null
    fi
    echo "Cleanup complete."
}

# Trap the EXIT signal to call the cleanup function. This ensures cleanup
# runs whether the script finishes successfully, fails, or is interrupted.
trap cleanup EXIT


# --- Main Script Logic ---

# Check if the working directory exists.
if [ ! -d "$WORK_DIR" ]; then
    echo "Error: Directory '$WORK_DIR' not found."
    echo "Please run this script from the root of the repository."
    exit 1
fi

# Navigate into the working directory.
cd "$WORK_DIR"
echo "Changed directory to $(pwd)"

# Create a directory for log files if it doesn't exist.
mkdir -p "$LOG_DIR"

echo "Setting up Python virtual environment with 'uv'..."
uv venv

# Activate the virtual environment for the script and all its child processes.
source .venv/bin/activate
echo "Virtual environment activated."

# Array to store Process IDs (PIDs) of background jobs.
pids=()

# --- Start Background Services ---
echo ""
echo "Starting servers and agents in the background..."

# 1. Start MCP Server
echo "-> Starting MCP Server (Port: 10100)... Log: $LOG_DIR/mcp_server.log"
uv run --env-file .env a2a-mcp --run mcp-server --transport sse --port 10100 > "$LOG_DIR/mcp_server.log" 2>&1 &
pids+=($!)

# 2. Start Orchestrator Agent
echo "-> Starting Orchestrator Agent (Port: 10101)... Log: $LOG_DIR/orchestrator_agent.log"
uv run --env-file .env src/a2a_mcp/agents/ --agent-card agent_cards/orchestrator_agent.json --port 10101 > "$LOG_DIR/orchestrator_agent.log" 2>&1 &
pids+=($!)

# 3. Start Planner Agent
echo "-> Starting Planner Agent (Port: 10102)... Log: $LOG_DIR/planner_agent.log"
uv run --env-file .env src/a2a_mcp/agents/ --agent-card agent_cards/planner_agent.json --port 10102 > "$LOG_DIR/planner_agent.log" 2>&1 &
pids+=($!)

# 4. Start Airline Ticketing Agent
echo "-> Starting Airline Agent (Port: 10103)... Log: $LOG_DIR/airline_agent.log"
uv run --env-file .env src/a2a_mcp/agents/ --agent-card agent_cards/air_ticketing_agent.json --port 10103 > "$LOG_DIR/airline_agent.log" 2>&1 &
pids+=($!)

# 5. Start Hotel Reservations Agent
echo "-> Starting Hotel Agent (Port: 10104)... Log: $LOG_DIR/hotel_agent.log"
uv run --env-file .env src/a2a_mcp/agents/ --agent-card agent_cards/hotel_booking_agent.json --port 10104 > "$LOG_DIR/hotel_agent.log" 2>&1 &
pids+=($!)

# 6. Start Car Rental Reservations Agent
echo "-> Starting Car Rental Agent (Port: 10105)... Log: $LOG_DIR/car_rental_agent.log"
uv run --env-file .env src/a2a_mcp/agents/ --agent-card agent_cards/car_rental_agent.json --port 10105 > "$LOG_DIR/car_rental_agent.log" 2>&1 &
pids+=($!)

echo ""
echo "All services are starting. Waiting 10 seconds for them to initialize..."
sleep 10

# --- Run the Foreground Client ---
echo ""
echo "---------------------------------------------------------"
echo "Starting CLI Client..."
echo "The script will exit after the client finishes."
echo "---------------------------------------------------------"
echo ""

# 7. Start the CLI client in the foreground.
# The script will pause here until this command completes.
uv run --env-file .env src/a2a_mcp/mcp/client.py --resource "resource://agent_cards/list" --find_agent "I would like to plan a trip to France."

echo ""
echo "---------------------------------------------------------"
echo "CLI client finished."
echo "---------------------------------------------------------"

# The 'trap' will now trigger the 'cleanup' function automatically upon exiting.

================
File: samples/python/agents/a2a-mcp-without-framework/src/no_llm_framework/client/__main__.py
================
import asyncio
from typing import Literal


import asyncclick as click
import colorama
from no_llm_framework.client.agent import Agent


@click.command()
@click.option('--host', 'host', default='localhost')
@click.option('--port', 'port', default=9999)
@click.option('--mode', 'mode', default='streaming')
@click.option('--question', 'question', required=True)
async def a_main(
    host: str,
    port: int,
    mode: Literal['completion', 'streaming'],
    question: str,
):
    """Main function to run the A2A Repo Agent client.

    Args:
        host (str): The host address to run the server on.
        port (int): The port number to run the server on.
        mode (Literal['completion', 'streaming']): The mode to run the server on.
        question (str): The question to ask the Agent.
    """  # noqa: E501
    agent = Agent(
        mode='stream',
        token_stream_callback=None,
        agent_urls=[f'http://{host}:{port}/'],
    )
    async for chunk in agent.stream(question):
        if chunk.startswith('<Agent name="'):
            print(colorama.Fore.CYAN + chunk, end='', flush=True)
        elif chunk.startswith('</Agent>'):
            print(colorama.Fore.RESET + chunk, end='', flush=True)
        else:
            print(chunk, end='', flush=True)


def main() -> None:
    """Main function to run the A2A Repo Agent client."""
    asyncio.run(a_main())


if __name__ == '__main__':
    main()

================
File: samples/python/agents/a2a-mcp-without-framework/src/no_llm_framework/client/agent_answer.jinja
================
Previous agents have been called. {% for agent in called_agents %}
- Agent: {{ agent.name }}
- Prompt: {{ agent.prompt }}
- Answer: {{ agent.answer }}
{% endfor %}

================
File: samples/python/agents/a2a-mcp-without-framework/src/no_llm_framework/client/agent.py
================
import asyncio
import json
import re

from collections.abc import Callable, Generator
from pathlib import Path
from typing import Literal
from uuid import uuid4

import httpx

from a2a.client import A2ACardResolver, A2AClient
from a2a.types import (
    AgentCard,
    Message,
    MessageSendParams,
    Part,
    Role,
    SendStreamingMessageRequest,
    SendStreamingMessageSuccessResponse,
    TaskStatusUpdateEvent,
    TextPart,
)
from google import genai
from jinja2 import Template

from no_llm_framework.client.constant import GOOGLE_API_KEY


dir_path = Path(__file__).parent

with Path(dir_path / 'decide.jinja').open('r') as f:
    decide_template = Template(f.read())

with Path(dir_path / 'agents.jinja').open('r') as f:
    agents_template = Template(f.read())

with Path(dir_path / 'agent_answer.jinja').open('r') as f:
    agent_answer_template = Template(f.read())


def stream_llm(prompt: str) -> Generator[str]:
    """Stream LLM response.

    Args:
        prompt (str): The prompt to send to the LLM.

    Returns:
        Generator[str, None, None]: A generator of the LLM response.
    """
    client = genai.Client(vertexai=False, api_key=GOOGLE_API_KEY)
    for chunk in client.models.generate_content_stream(
        model='gemini-2.5-flash-lite',
        contents=prompt,
    ):
        yield chunk.text


class Agent:
    """Agent for interacting with the Google Gemini LLM in different modes."""

    def __init__(
        self,
        mode: Literal['complete', 'stream'] = 'stream',
        token_stream_callback: Callable[[str], None] | None = None,
        agent_urls: list[str] | None = None,
        agent_prompt: str | None = None,
    ):
        self.mode = mode
        self.token_stream_callback = token_stream_callback
        self.agent_urls = agent_urls
        self.agents_registry: dict[str, AgentCard] = {}

    async def get_agents(self) -> tuple[dict[str, AgentCard], str]:
        """Retrieve agent cards from all agent URLs and render the agent prompt.

        Returns:
            tuple[dict[str, AgentCard], str]: A dictionary mapping agent names to AgentCard objects, and the rendered agent prompt string.
        """
        async with httpx.AsyncClient() as httpx_client:
            card_resolvers = [
                A2ACardResolver(httpx_client, url) for url in self.agent_urls
            ]
            agent_cards = await asyncio.gather(
                *[
                    card_resolver.get_agent_card()
                    for card_resolver in card_resolvers
                ]
            )
            agents_registry = {
                agent_card.name: agent_card for agent_card in agent_cards
            }
            agent_prompt = agents_template.render(agent_cards=agent_cards)
            return agents_registry, agent_prompt

    def call_llm(self, prompt: str) -> str:
        """Call the LLM with the given prompt and return the response as a string or generator.

        Args:
            prompt (str): The prompt to send to the LLM.

        Returns:
            str or Generator[str]: The LLM response as a string or generator, depending on mode.
        """
        if self.mode == 'complete':
            return stream_llm(prompt)

        result = ''
        for chunk in stream_llm(prompt):
            result += chunk
        return result

    async def decide(
        self,
        question: str,
        agents_prompt: str,
        called_agents: list[dict] | None = None,
    ) -> Generator[str, None]:
        """Decide which agent(s) to use to answer the question.

        Args:
            question (str): The question to answer.
            agents_prompt (str): The prompt describing available agents.
            called_agents (list[dict] | None): Previously called agents and their answers.

        Returns:
            Generator[str, None]: The LLM's response as a generator of strings.
        """
        if called_agents:
            call_agent_prompt = agent_answer_template.render(
                called_agents=called_agents
            )
        else:
            call_agent_prompt = ''
        prompt = decide_template.render(
            question=question,
            agent_prompt=agents_prompt,
            call_agent_prompt=call_agent_prompt,
        )
        return self.call_llm(prompt)

    def extract_agents(self, response: str) -> list[dict]:
        """Extract the agents from the response.

        Args:
            response (str): The response from the LLM.
        """
        pattern = r'```json\n(.*?)\n```'
        match = re.search(pattern, response, re.DOTALL)
        if match:
            return json.loads(match.group(1))
        return []

    async def send_message_to_an_agent(
        self, agent_card: AgentCard, message: str
    ):
        """Send a message to a specific agent and yield the streaming response.

        Args:
            agent_card (AgentCard): The agent to send the message to.
            message (str): The message to send.

        Yields:
            str: The streaming response from the agent.
        """
        async with httpx.AsyncClient() as httpx_client:
            client = A2AClient(httpx_client, agent_card=agent_card)
            message = MessageSendParams(
                message=Message(
                    role=Role.user,
                    parts=[Part(TextPart(text=message))],
                    message_id=uuid4().hex,
                    task_id=uuid4().hex,
                )
            )

            streaming_request = SendStreamingMessageRequest(
                id=str(uuid4().hex), params=message
            )
            async for chunk in client.send_message_streaming(streaming_request):
                if isinstance(
                    chunk.root, SendStreamingMessageSuccessResponse
                ) and isinstance(chunk.root.result, TaskStatusUpdateEvent):
                    message = chunk.root.result.status.message
                    if message:
                        yield message.parts[0].root.text

    async def stream(self, question: str):
        """Stream the process of answering a question, possibly involving multiple agents.

        Args:
            question (str): The question to answer.

        Yields:
            str: Streaming output, including agent responses and intermediate steps.
        """
        agent_answers: list[dict] = []
        for _ in range(3):
            agents_registry, agent_prompt = await self.get_agents()
            response = ''
            for chunk in await self.decide(
                question, agent_prompt, agent_answers
            ):
                response += chunk
                if self.token_stream_callback:
                    self.token_stream_callback(chunk)
                yield chunk

            agents = self.extract_agents(response)
            if agents:
                for agent in agents:
                    agent_response = ''
                    agent_card = agents_registry[agent['name']]
                    yield f'<Agent name="{agent["name"]}">\n'
                    async for chunk in self.send_message_to_an_agent(
                        agent_card, agent['prompt']
                    ):
                        agent_response += chunk
                        if self.token_stream_callback:
                            self.token_stream_callback(chunk)
                        yield chunk
                    yield '</Agent>\n'
                    match = re.search(
                        r'<Answer>(.*?)</Answer>', agent_response, re.DOTALL
                    )
                    answer = match.group(1).strip() if match else agent_response
                    agent_answers.append(
                        {
                            'name': agent['name'],
                            'prompt': agent['prompt'],
                            'answer': answer,
                        }
                    )
            else:
                return


if __name__ == '__main__':
    import asyncio

    import colorama

    async def main():
        """Main function to run the A2A Repo Agent client."""
        agent = Agent(
            mode='stream',
            token_stream_callback=None,
            agent_urls=['http://localhost:9999/'],
        )

        async for chunk in agent.stream('What is A2A protocol?'):
            if chunk.startswith('<Agent name="'):
                print(colorama.Fore.CYAN + chunk, end='', flush=True)
            elif chunk.startswith('</Agent>'):
                print(colorama.Fore.RESET + chunk, end='', flush=True)
            else:
                print(chunk, end='', flush=True)

    asyncio.run(main())

================
File: samples/python/agents/a2a-mcp-without-framework/src/no_llm_framework/client/agents.jinja
================
# Agent contexts

These are some consults from agent(s) that may be useful to answer the question
{% for agent in agent_cards %}
Agent name: {{agent.name}}
Agent description: {{agent.description}}
Agent skills: {% for skill in agent.skills%}
    - name: {{skill.name}}
        - description: {{skill.description}}
        - example: {{skill.examples}}{% endfor %}{% endfor %}
-------------------

================
File: samples/python/agents/a2a-mcp-without-framework/src/no_llm_framework/client/called_agent_history.jinja
================
Previous agents have been called. {% for agent in called_agents %}
- Agent: {{ agent.name }}
- Prompt: {{ agent.prompt }}
- Answer: {{ agent.answer }}
--------------------------------
{% endfor %}

================
File: samples/python/agents/a2a-mcp-without-framework/src/no_llm_framework/client/constant.py
================
import os

GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')

================
File: samples/python/agents/a2a-mcp-without-framework/src/no_llm_framework/client/decide.jinja
================
You duty is to decide which agent to consult or ask for help to answer the question.
The question is:

{{ question }}

{{ call_agent_prompt }}

{{ agent_prompt }}

You must answer in the following format:


<Thoughts>
Thoughts:
- ...
- ...
- ...
</Thoughts>

<Selected agents>
```json
[
        {
            "name": "agent_name",
            "prompt": "prompt_to_agent"
        },
        ...
]
```
</Selected agents>

Note:
- You can leave the selected agents empty if you think none of the agents are relevant to the question or given contexts are enough to answer the question.
- You can select multiple agents if you think multiple agents are relevant to the question.
- You `agent_name` must be one of the agent names in the agents list, and you must spell it correctly.
- If there is no need to call any agent, Please give your answer by continuing the following format:

<Answer>
<Your answer here>
</Answer>

================
File: samples/python/agents/a2a-mcp-without-framework/src/no_llm_framework/server/__main__.py
================
import click
import uvicorn

from a2a.server.agent_execution import AgentExecutor
from a2a.server.apps import A2AStarletteApplication
from a2a.server.request_handlers.default_request_handler import (
    DefaultRequestHandler,
)
from a2a.server.tasks.inmemory_task_store import InMemoryTaskStore
from a2a.types import (
    AgentCapabilities,
    AgentCard,
    AgentSkill,
    GetTaskRequest,
    GetTaskResponse,
    SendMessageRequest,
    SendMessageResponse,
)

from no_llm_framework.server.agent_executor import HelloWorldAgentExecutor


class A2ARequestHandler(DefaultRequestHandler):
    """A2A Request Handler for the A2A Repo Agent."""

    def __init__(
        self, agent_executor: AgentExecutor, task_store: InMemoryTaskStore
    ):
        super().__init__(agent_executor, task_store)

    async def on_get_task(self, request: GetTaskRequest) -> GetTaskResponse:
        return await super().on_get_task(request)

    async def on_message_send(
        self, request: SendMessageRequest
    ) -> SendMessageResponse:
        return await super().on_message_send(request)


@click.command()
@click.option('--host', 'host', default='localhost')
@click.option('--port', 'port', default=9999)
def main(host: str, port: int):
    """Start the A2A Repo Agent server.

    This function initializes the A2A Repo Agent server with the specified host and port.
    It creates an agent card with the agent's name, description, version, and capabilities.

    Args:
        host (str): The host address to run the server on.
        port (int): The port number to run the server on.
    """
    skill = AgentSkill(
        id='answer_detail_about_A2A_repo',
        name='Answer any information about A2A repo',
        description='The agent will look up the information about A2A repo and answer the question.',
        tags=['A2A repo'],
        examples=['What is A2A repo?', 'What is Google A2A repo?'],
    )

    agent_card = AgentCard(
        name='A2A Protocol Agent',
        description='A2A Protocol knowledge agent who has information about A2A Protocol and can answer questions about it',
        url=f'http://{host}:{port}/',
        version='1.0.0',
        default_input_modes=['text'],
        default_output_modes=['text'],
        capabilities=AgentCapabilities(
            input_modes=['text'],
            output_modes=['text'],
            streaming=True,
        ),
        skills=[skill],
        # authentication=AgentAuthentication(schemes=['public']),
        examples=['What is A2A protocol?', 'What is Google A2A?'],
    )

    task_store = InMemoryTaskStore()
    request_handler = A2ARequestHandler(
        agent_executor=HelloWorldAgentExecutor(),
        task_store=task_store,
    )

    server = A2AStarletteApplication(
        agent_card=agent_card, http_handler=request_handler
    )
    uvicorn.run(server.build(), host=host, port=port)


if __name__ == '__main__':
    main()

================
File: samples/python/agents/a2a-mcp-without-framework/src/no_llm_framework/server/agent_executor.py
================
from typing import override

from a2a.server.agent_execution import AgentExecutor, RequestContext
from a2a.server.events import EventQueue
from a2a.types import (
    TaskArtifactUpdateEvent,
    TaskState,
    TaskStatus,
    TaskStatusUpdateEvent,
)
from a2a.utils import new_agent_text_message, new_task, new_text_artifact
from src.no_llm_framework.server.agent import Agent


class HelloWorldAgentExecutor(AgentExecutor):
    """Test AgentProxy Implementation."""

    def __init__(self):
        self.agent = Agent(
            mode='stream',
            token_stream_callback=print,
            mcp_url='https://gitmcp.io/google/A2A',
        )

    @override
    async def execute(
        self,
        context: RequestContext,
        event_queue: EventQueue,
    ) -> None:
        query = context.get_user_input()
        task = context.current_task

        if not context.message:
            raise Exception('No message provided')

        if not task:
            task = new_task(context.message)
            await event_queue.enqueue_event(task)

        async for event in self.agent.stream(query):
            if event['is_task_complete']:
                await event_queue.enqueue_event(
                    TaskArtifactUpdateEvent(
                        append=False,
                        context_id=task.context_id,
                        task_id=task.id,
                        last_chunk=True,
                        artifact=new_text_artifact(
                            name='current_result',
                            description='Result of request to agent.',
                            text=event['content'],
                        ),
                    )
                )
                await event_queue.enqueue_event(
                    TaskStatusUpdateEvent(
                        status=TaskStatus(state=TaskState.completed),
                        final=True,
                        context_id=task.context_id,
                        task_id=task.id,
                    )
                )
            elif event['require_user_input']:
                await event_queue.enqueue_event(
                    TaskStatusUpdateEvent(
                        status=TaskStatus(
                            state=TaskState.input_required,
                            message=new_agent_text_message(
                                event['content'],
                                task.context_id,
                                task.id,
                            ),
                        ),
                        final=True,
                        context_id=task.context_id,
                        task_id=task.id,
                    )
                )
            else:
                await event_queue.enqueue_event(
                    TaskStatusUpdateEvent(
                        append=True,
                        status=TaskStatus(
                            state=TaskState.working,
                            message=new_agent_text_message(
                                event['content'],
                                task.context_id,
                                task.id,
                            ),
                        ),
                        final=False,
                        context_id=task.context_id,
                        task_id=task.id,
                    )
                )

    @override
    async def cancel(
        self, context: RequestContext, event_queue: EventQueue
    ) -> None:
        raise Exception('cancel not supported')

================
File: samples/python/agents/a2a-mcp-without-framework/src/no_llm_framework/server/agent.py
================
import asyncio
import json
import re

from collections.abc import AsyncGenerator, Callable, Generator
from pathlib import Path
from typing import Literal

from google import genai
from jinja2 import Template
from mcp.types import CallToolResult

from no_llm_framework.server.constant import GOOGLE_API_KEY
from no_llm_framework.server.mcp import call_mcp_tool, get_mcp_tool_prompt


dir_path = Path(__file__).parent

with Path(dir_path / 'decide.jinja').open('r') as f:
    decide_template = Template(f.read())

with Path(dir_path / 'tool.jinja').open('r') as f:
    tool_template = Template(f.read())

with Path(dir_path / 'called_tools_history.jinja').open('r') as f:
    called_tools_history_template = Template(f.read())


def stream_llm(prompt: str) -> Generator[str, None]:
    """Stream LLM response.

    Args:
        prompt (str): The prompt to send to the LLM.

    Returns:
        Generator[str, None, None]: A generator of the LLM response.
    """
    client = genai.Client(vertexai=False, api_key=GOOGLE_API_KEY)
    for chunk in client.models.generate_content_stream(
        model='gemini-2.5-flash-lite',
        contents=prompt,
    ):
        yield chunk.text


class Agent:
    """Agent for interacting with the Google Gemini LLM in different modes."""

    def __init__(
        self,
        mode: Literal['complete', 'stream'] = 'stream',
        token_stream_callback: Callable[[str], None] | None = None,
        mcp_url: str | None = None,
    ):
        self.mode = mode
        self.token_stream_callback = token_stream_callback
        self.mcp_url = mcp_url

    def call_llm(self, prompt: str) -> Generator[str, None]:
        """Call the LLM with the given prompt and return a generator of responses.

        Args:
            prompt (str): The prompt to send to the LLM.

        Returns:
            Generator[str, None]: A generator yielding the LLM's response.
        """
        return stream_llm(prompt)

    async def decide(
        self, question: str, called_tools: list[dict] | None = None
    ) -> Generator[str, None]:
        """Decide which tool to use to answer the question.

        Args:
            question (str): The question to answer.
            called_tools (list[dict]): The tools that have been called.
        """
        if self.mcp_url is None:
            return self.call_llm(question)
        tool_prompt = await get_mcp_tool_prompt(self.mcp_url)
        if called_tools:
            called_tools_prompt = called_tools_history_template.render(
                called_tools=called_tools
            )
        else:
            called_tools_prompt = ''

        prompt = decide_template.render(
            question=question,
            tool_prompt=tool_prompt,
            called_tools=called_tools_prompt,
        )
        return self.call_llm(prompt)

    def extract_tools(self, response: str) -> list[dict]:
        """Extract the tools from the response.

        Args:
            response (str): The response from the LLM.
        """
        pattern = r'```json\n(.*?)\n```'
        match = re.search(pattern, response, re.DOTALL)
        if match:
            return json.loads(match.group(1))
        return []

    async def call_tool(self, tools: list[dict]) -> list[CallToolResult]:
        """Call the tool.

        Args:
            tools (list[dict]): The tools to call.
        """
        return await asyncio.gather(
            *[
                call_mcp_tool(self.mcp_url, tool['name'], tool['arguments'])
                for tool in tools
            ]
        )

    async def stream(self, question: str) -> AsyncGenerator[str]:
        """Stream the process of answering a question, possibly involving tool calls.

        Args:
            question (str): The question to answer.

        Yields:
            dict: Streaming output, including intermediate steps and final result.
        """
        called_tools = []
        for i in range(10):
            yield {
                'is_task_complete': False,
                'require_user_input': False,
                'content': f'Step {i}',
            }

            response = ''
            for chunk in await self.decide(question, called_tools):
                response += chunk
                yield {
                    'is_task_complete': False,
                    'require_user_input': False,
                    'content': chunk,
                }
            tools = self.extract_tools(response)
            if not tools:
                break
            results = await self.call_tool(tools)

            called_tools += [
                {
                    'tool': tool['name'],
                    'arguments': tool['arguments'],
                    'isError': result.isError,
                    'result': result.content[0].text,
                }
                for tool, result in zip(tools, results, strict=True)
            ]
            called_tools_history = called_tools_history_template.render(
                called_tools=called_tools, question=question
            )
            yield {
                'is_task_complete': False,
                'require_user_input': False,
                'content': called_tools_history,
            }

        yield {
            'is_task_complete': True,
            'require_user_input': False,
            'content': 'Task completed',
        }


if __name__ == '__main__':
    agent = Agent(
        token_stream_callback=lambda token: print(token, end='', flush=True),
        mcp_url='https://gitmcp.io/google/A2A',
    )

    async def main():
        """Main function."""
        async for chunk in agent.stream('What is A2A Protocol?'):
            print(chunk)

    asyncio.run(main())

================
File: samples/python/agents/a2a-mcp-without-framework/src/no_llm_framework/server/called_tools_history.jinja
================
Previous tools have been called. {% for tool in called_tools %}
- Tool: {{ tool.name }}
- Arguments: {{ tool.arguments }}
- Result: {{ tool.result }}
{% endfor %}

================
File: samples/python/agents/a2a-mcp-without-framework/src/no_llm_framework/server/constant.py
================
import os

GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')

================
File: samples/python/agents/a2a-mcp-without-framework/src/no_llm_framework/server/decide.jinja
================
You duty is to decide which tool to use to answer the question.
The question is:

{{ question }}

{{ called_tools }}

{{ tool_prompt }}

You must answer in the following format:


<Thoughts>
Thoughts:
- ...
- ...
- ...
</Thoughts>

<Selected tools>
```json
[
        {
            "name": "tool_name",
            "arguments": {
                "argument_name": "argument_value"
            }
        },
        ...
]
```
</Selected tools>

Note:
- You can leave the selected tools empty if you think none of the tools are relevant to the question or given contexts are enough to answer the question.
- You can select multiple tools if you think multiple tools are relevant to the question.
- You `tool_name` must be one of the tool names in the tools list, and you must spell it correctly.
- If there is no need to call any tool, Please give your answer by continuing the following format:

<Answer>
<Your answer here>
</Answer>

================
File: samples/python/agents/a2a-mcp-without-framework/src/no_llm_framework/server/mcp.py
================
import asyncio
from pathlib import Path

from jinja2 import Template
from mcp.client.session import ClientSession
from mcp.client.sse import sse_client
from mcp.types import CallToolResult, TextContent

dir_path = Path(__file__).parent

with Path(dir_path / 'tool.jinja').open('r') as f:
    template = Template(f.read())


async def get_mcp_tool_prompt(url: str) -> str:
    """Get the MCP tool prompt for a given URL.

    Args:
        url (str): The URL of the MCP tool.

    Returns:
        str: The MCP tool prompt.
    """
    async with (
        sse_client(url) as (read, write),
        ClientSession(read, write) as session,
    ):
        await session.initialize()

        resources = await session.list_tools()
        return template.render(tools=resources.tools)


async def call_mcp_tool(
    url: str, tool_name: str, arguments: dict | None = None
) -> CallToolResult:
    """Call an MCP tool with the given URL and tool name.

    Args:
        url (str): The URL of the MCP tool.
        tool_name (str): The name of the tool to call.
        arguments (dict | None, optional): The arguments to pass to the tool. Defaults to None.

    Returns:
        CallToolResult: The result of the tool call.
    """  # noqa: E501
    async with (
        sse_client(
            url=url,
        ) as (read, write),
        ClientSession(read, write) as session,
    ):
        await session.initialize()

        return await session.call_tool(tool_name, arguments=arguments)


if __name__ == '__main__':
    print(asyncio.run(get_mcp_tool_prompt('https://gitmcp.io/google/A2A')))
    result = asyncio.run(
        call_mcp_tool('https://gitmcp.io/google/A2A', 'fetch_A2A_documentation')
    )
    for content in result.content:
        if isinstance(content, TextContent):
            print(content.text)

================
File: samples/python/agents/a2a-mcp-without-framework/src/no_llm_framework/server/tool.jinja
================
Tools{% for tool in tools %}
- {{loop.index}}: {{ tool.name }}
  - Description: {{ tool.description }}
  - Input Schema: {{tool.inputSchema.properties}}{% endfor %}

================
File: samples/python/agents/a2a-mcp-without-framework/pyproject.toml
================
[project]
name = "no-llm-framework"
version = "0.1.0"
description = "Use A2A without any agent framework"
readme = "README.md"
authors = [{ name = "prem", email = "prem.ch@ku.th" }]
requires-python = ">=3.13"
dependencies = [
    "a2a-sdk>=0.3.0",
    "asyncclick>=8.1.8",
    "colorama>=0.4.6",
    "fastmcp>=2.3.4",
    "google-genai",
    "jinja2>=3.1.6",
    "rich>=14.0.0",
]
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project.scripts]
a2a-server = "no_llm_framework.server.__main__:main"
a2a-client = "no_llm_framework.client.__main__:main"

================
File: samples/python/agents/a2a-mcp-without-framework/README.md
================
# Example: Using a2a-python SDK Without an LLM Framework

This repository demonstrates how to set up and use the [a2a-python SDK](https://github.com/google/a2a-python) to create a simple server and client, without relying on any agent framework.

## Overview

- **A2A (Agent-to-Agent):** A protocol and SDK for communication with AI Agents.
- **This Example:** Shows how to run a basic A2A server and client, exchange messages, and view the response.

## Prerequisites

- Python 3.13+
- [uv](https://github.com/astral-sh/uv) (for fast dependency management and running)
- An API key for Gemini (set as `GEMINI_API_KEY`)

## Installation

1. **Clone the repository:**

   ```bash
   git clone <this-repo-url>
   cd <repo-directory>
   ```

2. **Install dependencies:**

   ```bash
   uv pip install -e .
   ```

3. **Set environment variables:**

   ```bash
   export GEMINI_API_KEY=your-gemini-api-key
   ```

   Or create a `.env` file with:

   ```sh
   GEMINI_API_KEY=your-gemini-api-key
   ```

## Running the Example

### 1. Start the Server

```bash
uv run --env-file .env python -m src.no_llm_framework.server.__main__
```

- The server will start on port `9999`.

### 2. Run the Client

In a new terminal:

```bash
uv run --env-file .env python -m src.no_llm_framework.client --question "What is A2A protocol?"
```

- The client will connect to the server and send a request.

### 3. View the Response

- The response from the client will be saved to [`response.xml`](./response.xml).

## File Structure

- `src/no_llm_framework/server/`: Server implementation.
- `src/no_llm_framework/client/`: Client implementation.
- `response.xml`: Example response from the client.

## Troubleshooting

- **Missing dependencies:** Make sure you have `uv` installed.
- **API key errors:** Ensure `GEMINI_API_KEY` is set correctly.
- **Port conflicts:** Make sure port 9999 is free.

## Disclaimer

Important: The sample code provided is for demonstration purposes and illustrates the mechanics of the Agent-to-Agent (A2A) protocol. When building production applications, it is critical to treat any agent operating outside of your direct control as a potentially untrusted entity.

All data received from an external agentincluding but not limited to its AgentCard, messages, artifacts, and task statusesshould be handled as untrusted input. For example, a malicious agent could provide an AgentCard containing crafted data in its fields (e.g., description, name, skills.description). If this data is used without sanitization to construct prompts for a Large Language Model (LLM), it could expose your application to prompt injection attacks.  Failure to properly validate and sanitize this data before use can introduce security vulnerabilities into your application.

Developers are responsible for implementing appropriate security measures, such as input validation and secure handling of credentials to protect their systems and users.

================
File: samples/python/agents/a2a-mcp-without-framework/response.xml
================
<Thoughts>
Thoughts:
- The question directly asks about the A2A protocol.
- The A2A Protocol Agent is specifically designed to answer questions about the A2A protocol.
-  No other agent is relevant.
</Thoughts>

<Selected agents>
```json
[
  {
    "name": "A2A Protocol Agent",
    "prompt": "What is A2A protocol?"
  }
]
```
<Agent name="A2A Protocol Agent">
Step 0<Thoughts>
Thoughts:
- The question asks for a definition of the A2A protocol.  This is a general question about the Google A2A project.
- The `fetch_A2A_documentation` tool is designed to retrieve the entire documentation, which is likely to contain the definition.  This should be the first tool used.
- If the definition isn't directly found in the fetched documentation, `search_A2A_documentation` could be used to search for relevant sections within the documentation.
</Thoughts>

<Selected tools>
```json
[
  {
    "name": "fetch_A2A_documentation",
    "arguments": {}
  },
  {
    "name": "search_A2A_documentation",
    "arguments": {
      "query": "What is A2A protocol?"
    }
  }
]
```
</Selected tools>
Previous tools have been called. 
- Tool: 
- Arguments: {}
- Result: # A2A (Agent2Agent) Protocol High-Level Summary

This project defines and demonstrates the **Agent2Agent (A2A) protocol**, an open standard initiated by Google designed to enable communication and interoperability between disparate AI agent systems. The core goal is to allow agents built on different frameworks (e.g., LangGraph, CrewAI, Google ADK, Genkit) or by different vendors to discover each other's capabilities, negotiate interaction modes (text, forms, files, potentially audio/video later), and collaborate on tasks.

The repository provides:

1.  **Formal Specification:** A detailed JSON Schema (`specification/json/a2a.json`) defining the structure of A2A messages, including requests, responses, task states, artifacts, and agent metadata (Agent Cards).
2.  **Core Concepts Documentation (Implied):** Links in the main README suggest documentation covering agent discovery, task lifecycle, artifact exchange, streaming updates, push notifications, and enterprise readiness.
3.  **Sample Implementations:**
    *   **Common Libraries:** Reusable Python (`samples/python/common`) and JavaScript/TypeScript (`samples/js/src`) code for building A2A clients and servers, handling JSON-RPC communication, task management, and potentially authentication.
    *   **Example Agents:** Demonstrations of integrating A2A into various agent frameworks:
        *   **Python:** LangGraph (currency conversion, streaming), CrewAI (image generation, file artifacts), Google ADK (expense reports, form handling).
        *   **JavaScript/TypeScript:** Genkit (movie info via API, code generation with file artifacts).
    *   **Example Hosts:** Applications that *consume* A2A services:
        *   CLIs in both Python and JS for direct interaction.
        *   A Python-based multi-agent orchestrator (using Google ADK) that delegates tasks to other A2A agents.
4.  **Demo Web Application:** A web UI (`demo/ui`, likely using Mesop) demonstrating multi-agent interactions facilitated by the A2A protocol, including visualization of conversations, tasks, events, and agent discovery. It features a backend service coordinating with the host agent.

Key features of the A2A protocol highlighted by the specification and samples include: agent discovery via Agent Cards, standardized task management (send, get, cancel), support for different content types (text, files, structured data) via `Parts` and `Artifacts`, streaming updates for long-running tasks, and mechanisms for push notifications. The project is open source and encourages community contribution.

# A2A (Agent2Agent) Protocol 

## 1. Overview

*   **Project Name:** Agent2Agent (A2A) Protocol
*   **Purpose:** An open protocol by Google enabling communication and interoperability between AI agents built on different frameworks or by different vendors.
*   **Core Goal:** Allow agents to discover capabilities, negotiate interaction, and collaborate securely on tasks.
*   **Communication:** Uses JSON-RPC 2.0 over HTTP(S). Supports standard request/response and Server-Sent Events (SSE) for streaming.
*   **Key Components:** Specification (JSON Schema), Common Libraries (Python, JS/TS), Sample Agents (LangGraph, CrewAI, ADK, Genkit), Sample Hosts (CLI, Orchestrator), Demo Web App (Mesop).

## 2. Protocol Specification (`specification/json/a2a.json`)

### 2.1. Core JSON-RPC Structures

*   **`JSONRPCMessage`:** Base for requests/responses. Contains `jsonrpc: "2.0"` and optional `id`.
*   **`JSONRPCRequest`:** Represents a request.
    *   `method`: String identifying the operation (e.g., "message/send").
    *   `params`: Object or Array containing parameters for the method.
    *   `id`: Unique identifier (string/number) for request/response correlation. Omitted/null for notifications.
*   **`JSONRPCResponse`:** Represents a response.
    *   `result`: Contains the successful result data (can be `null`). Mutually exclusive with `error`.
    *   `error`: Contains an error object if the request failed. Mutually exclusive with `result`.
    *   `id`: Must match the corresponding request `id`.
*   **`JSONRPCError`:** Represents an error.
    *   `code`: Integer error code.
    *   `message`: String description of the error.
    *   `data`: Optional additional error details.

### 2.2. Key A2A Data Objects

*   **`AgentCard`:** Metadata describing an agent. Found typically at `/.well-known/agent.json`.
    *   `name`: (string) Human-readable name.
    *   `description`: (string | null) Agent description.
    *   `url`: (string) Base URL endpoint for the agent's A2A service.
    *   `provider`: (`AgentProvider` | null) Organization details.
    *   `version`: (string) Agent/API version.
    *   `documentationUrl`: (string | null) Link to documentation.
    *   `capabilities`: (`AgentCapabilities`) Features supported (streaming, push).
    *   `authentication`: (`AgentAuthentication` | null) Auth schemes/credentials needed.
    *   `defaultInputModes`: (string[]) Default supported input types (e.g., "text", "file").
    *   `defaultOutputModes`: (string[]) Default supported output types.
    *   `skills`: (`AgentSkill[]`) List of specific capabilities.
*   **`AgentCapabilities`:**
    *   `streaming`: (boolean) Supports `message/stream`.
    *   `pushNotifications`: (boolean) Supports `tasks/pushNotification/set|get`.
    *   `stateTransitionHistory`: (boolean) Supports providing detailed history.
*   **`AgentSkill`:**
    *   `id`: (string) Unique skill ID.
    *   `name`: (string) Skill name.
    *   `description`: (string | null) Skill description.
    *   `tags`: (string[] | null) Keywords.
    *   `examples`: (string[] | null) Usage examples.
    *   `inputModes`: (string[] | null) Overrides default inputs for this skill.
    *   `outputModes`: (string[] | null) Overrides default outputs for this skill.
*   **`Task`:** Represents a unit of work processed by an agent.
    *   `id`: (string) Unique task identifier.
    *   `sessionId`: (string | null) Groups related tasks.
    *   `status`: (`TaskStatus`) Current state and associated message.
    *   `artifacts`: (`Artifact[]` | null) Outputs generated by the task.
    *   `history`: (`Message[]` | null) (Optional) History of messages exchanged for this task (if requested via `historyLength`).
    *   `metadata`: (object | null) Additional task metadata.
*   **`TaskStatus`:**
    *   `state`: (`TaskState`) Current lifecycle state (enum).
    *   `message`: (`Message` | null) Message associated with this status (e.g., progress update, final response text, input prompt).
    *   `timestamp`: (string) ISO 8601 timestamp of the status update.
*   **`TaskState` (Enum):**
    *   `submitted`: Task received, not yet started.
    *   `working`: Task is actively being processed.
    *   `input-required`: Agent requires further input from the user/client.
    *   `completed`: Task finished successfully.
    *   `canceled`: Task was canceled.
    *   `failed`: Task failed due to an error.
    *   `unknown`: State cannot be determined.
*   **`Message`:** Communication unit between user and agent.
    *   `role`: ("user" | "agent") Sender role.
    *   `parts`: (`Part[]`) Content parts (text, file, data).
    *   `metadata`: (object | null) Message-specific metadata.
*   **`Part` (Union Type):** Represents a piece of content within a Message or Artifact.
    *   **`TextPart`:**
        *   `type`: "text"
        *   `text`: (string) The textual content.
    *   **`FilePart`:**
        *   `type`: "file"
        *   `file`: (`FileContent`) File details (bytes or URI).
    *   **`DataPart`:**
        *   `type`: "data"
        *   `data`: (object) Structured JSON data (e.g., for forms).
    *   `metadata`: (object | null) Optional metadata for the specific part.
*   **`FileContent`:** Represents file data.
    *   `name`: (string | null) Filename.
    *   `mimeType`: (string | null) MIME type.
    *   `bytes`: (string | null) Base64 encoded file content. *Mutually exclusive with `uri`.*
    *   `uri`: (string | null) URI pointing to the file content. *Mutually exclusive with `bytes`.*
*   **`Artifact`:** Output generated by a task.
    *   `name`: (string | null) Artifact name.
    *   `description`: (string | null) Artifact description.
    *   `parts`: (`Part[]`) Content parts.
    *   `index`: (integer, default 0) Order index, useful for streaming/updates.
    *   `append`: (boolean | null) For streaming, indicates if content should append to artifact at the same index.
    *   `lastChunk`: (boolean | null) For streaming, indicates the final chunk for this artifact.
    *   `metadata`: (object | null) Artifact metadata.
*   **`PushNotificationConfig`:** Configuration for push notifications.
    *   `url`: (string) Endpoint URL for the agent to POST notifications to.
    *   `token`: (string | null) Optional token for the agent to include.
    *   `authentication`: (`AuthenticationInfo` | null) Auth details the agent needs to call the URL.
*   **`TaskPushNotificationConfig`:** Associates a `PushNotificationConfig` with a task ID.

### 2.3. A2A RPC Methods

*   **`message/send`:** (Request/Response)
    *   Sends a message to initiate or continue a task.
    *   `params`: `TaskSendParams` (includes `id`, `sessionId`, `message`, optionally `pushNotification`, `historyLength`, `metadata`).
    *   `result`: `Task` (final state after synchronous processing).
*   **`message/stream`:** (Request/Stream)
    *   Sends a message and subscribes to real-time updates via SSE.
    *   `params`: `TaskSendParams`.
    *   `result` (stream events): `TaskStatusUpdateEvent` or `TaskArtifactUpdateEvent`. Final event has `final: true`.
*   **`tasks/get`:** (Request/Response)
    *   Retrieves the current state of a task.
    *   `params`: `TaskQueryParams` (includes `id`, optionally `historyLength`).
    *   `result`: `Task`.
*   **`tasks/cancel`:** (Request/Response)
    *   Requests cancellation of a running task.
    *   `params`: `TaskIdParams` (includes `id`).
    *   `result`: `Task` (updated state, likely 'canceled') or error if not cancelable.
*   **`tasks/pushNotification/set`:** (Request/Response)
    *   Sets or updates the push notification configuration for a task.
    *   `params`: `TaskPushNotificationConfig`.
    *   `result`: `TaskPushNotificationConfig` (confirmed config).
*   **`tasks/pushNotification/get`:** (Request/Response)
    *   Retrieves the current push notification configuration for a task.
    *   `params`: `TaskIdParams`.
    *   `result`: `TaskPushNotificationConfig`.
*   **`tasks/resubscribe`:** (Request/Stream)
    *   Resubscribes to task updates after a connection interruption (SSE).
    *   `params`: `TaskQueryParams`.
    *   `result` (stream events): `TaskStatusUpdateEvent` or `TaskArtifactUpdateEvent`.

### 2.4. Streaming Update Events (Result of `message/stream` or `tasks/resubscribe`)

*   **`TaskStatusUpdateEvent`:** Signals a change in task status.
    *   `id`: (string) Task ID.
    *   `status`: (`TaskStatus`) The new status object.
    *   `final`: (boolean) True if this is the terminal update for the task.
    *   `metadata`: (object | null) Event metadata.
*   **`TaskArtifactUpdateEvent`:** Signals a new or updated artifact.
    *   `id`: (string) Task ID.
    *   `artifact`: (`Artifact`) The artifact data.
    *   `final`: (boolean, usually false for artifacts) Can signal end concurrently with status.
    *   `metadata`: (object | null) Event metadata.

### 2.5. Standard Error Codes

*   `-32700`: `JSONParseError` - Invalid JSON payload.
*   `-32600`: `InvalidRequestError` - Invalid JSON-RPC request object.
*   `-32601`: `MethodNotFoundError` - Method does not exist.
*   `-32602`: `InvalidParamsError` - Invalid method parameters.
*   `-32603`: `InternalError` - Internal server error.

### 2.6. A2A Specific Error Codes

*   `-32001`: `TaskNotFoundError` - Specified task ID not found.
*   `-32002`: `TaskNotCancelableError` - Task is in a final state and cannot be canceled.
*   `-32003`: `PushNotificationNotSupportedError` - Agent does not support push notifications.
*   `-32004`: `UnsupportedOperationError` - The requested operation is not supported.
*   `-32005`: `ContentTypeNotSupportedError` - Mismatch in supported content types.

## 3. Core Concepts

*   **Agent Discovery:** Clients find agents and their capabilities by fetching the `AgentCard` JSON, typically from `/.well-known/agent.json`.
*   **Task Lifecycle:** Tasks progress through states defined in `TaskState` (submitted -> working -> [input-required] -> completed/canceled/failed).
*   **Communication:** Uses `Message` objects containing `Part`s (text, file, data). Task outputs are represented as `Artifact`s, also containing `Part`s.
*   **Streaming:** Long-running tasks can provide real-time updates using SSE via `message/stream`. Updates are sent as `TaskStatusUpdateEvent` and `TaskArtifactUpdateEvent`.
*   **Push Notifications:** Agents can proactively notify clients about task updates using webhook URLs provided via `tasks/pushNotification/set`. Authentication mechanisms (e.g., Bearer tokens via JWT signed with keys from agent's JWKS endpoint) are supported.
*   **Authentication:** Defined in `AgentCard` and `PushNotificationConfig`. Can involve various schemes (e.g., API keys, OAuth, JWT). Samples use JWT for push notifications.
*   **Forms:** Structured data can be requested and submitted using `DataPart` within Messages/Artifacts (demonstrated in ADK sample).

## 4. Implementations & Samples

### 4.1. Common Libraries

*   **Python (`samples/python/common`)**:
    *   `client/`: `A2AClient` for making requests, `A2ACardResolver` for discovery.
    *   `server/`: `A2AServer` (Starlette-based), `TaskManager` base class, `InMemoryTaskManager`.
    *   `types.py`: Pydantic models mirroring the JSON schema.
    *   `utils/`: Helpers for push notification auth (JWT signing/verification, JWKS endpoint).
*   **JavaScript/TypeScript (`samples/js/src`)**:
    *   `client/`: `A2AClient` implementation using `fetch`.
    *   `server/`: `A2AServer` (Express-based), `TaskStore` interface, `InMemoryTaskStore`, `FileStore`.
    *   `schema.ts`: TypeScript interfaces matching the JSON schema.
    *   `handler.ts`, `error.ts`, `utils.ts`: Support code for the server.

### 4.2. Python Samples

*   **Location:** `samples/python/agents/` & `samples/python/hosts/`
*   **Setup:** Uses `uv` and `pyproject.toml`. Requires Python >= 3.12/3.13. API keys via `.env`.
*   **Agents:**
    *   **LangGraph (`agents/langgraph`)**: Currency conversion agent. Demonstrates tool use, multi-turn (`input-required`), and **streaming** (`message/stream`).
    *   **CrewAI (`agents/crewai`)**: Image generation agent. Demonstrates multi-turn and handling **file artifacts** (images).
    *   **Google ADK (`agents/google_adk`)**: Expense reimbursement agent. Demonstrates multi-turn and handling **forms** using `DataPart`.
*   **Hosts:**
    *   **CLI (`hosts/cli`)**: Simple command-line client to interact with any A2A agent. Supports streaming and optional push notification listening.
    *   **Multi-Agent Orchestrator (`hosts/multiagent`)**: An ADK-based "Host Agent" that manages connections (`RemoteAgentConnections`) to other A2A agents and delegates tasks based on instructions.

### 4.3. JavaScript/TypeScript Samples

*   **Location:** `samples/js/`
*   **Setup:** Uses `npm`/`pnpm`, `tsx`, `tsconfig.json`. Requires Node.js >= 18. API keys via environment variables. Framework: **Genkit**.
*   **Agents (`src/agents/`)**:
    *   **Movie Agent (`movie-agent`)**: Uses TMDB API via Genkit tools to answer movie questions. Demonstrates tool use and multi-turn (`AWAITING_USER_INPUT` mapped to `input-required`).
    *   **Coder Agent (`coder`)**: Generates code files. Demonstrates producing multiple **file artifacts** via streaming updates. Uses custom Genkit format (`code-format.ts`).
*   **Hosts:**
    *   **CLI (`src/cli.ts`)**: Command-line client for interacting with JS agents.

### 4.4. Demo Application (`demo/`)

*   **UI (`demo/ui`)**: Web application built with **Mesop**.
    *   Visualizes conversations with multiple agents via the host orchestrator.
    *   Renders text, images, forms.
    *   Allows dynamic agent registration via URL.
    *   Provides views for task list and event logs.
*   **Service (`demo/ui/service`)**: Backend service for the Mesop UI.
    *   `server/`: Manages conversations, routes messages, interfaces with the host agent (`ADKHostManager` or `InMemoryFakeAgentManager`).
    *   `client/`: Client used by the UI to talk to its *own* backend service.

## 5. Development & Setup

*   **Prerequisites:** Python (>=3.12 or 3.13), Node.js (>=18), `uv` (for Python), `npm`/`pnpm` (for JS).
*   **API Keys:** Required for LLM access (e.g., `GOOGLE_API_KEY`, `TMDB_API_KEY`), typically set via `.env` files or environment variables.
*   **Running Samples:** Generally involves running an agent server (`uv run ...` or `npm run ...`) and then a host client/app (e.g., `uv run hosts/cli --agent <agent_url>`).

## 6. Contribution

*   See `CONTRIBUTING.md`.
*   GitHub discussion available as the primary means of communication.
*   GitHub issues for bugs and feature requests.
*   Google Form for private feedback.


- Tool: 
- Arguments: {'query': 'What is A2A protocol?'}
- Result: ## Query

What is A2A protocol?.

## Response

### Sources:
Important: you can fetch the full content of any source using the fetch_url_content tool

#### (google/A2A/docs/index.md)[https://raw.githubusercontent.com/google/A2A/main/docs/index.md] (Score: 0.63)
- 

The **Agent2Agent (A2A) Protocol** is an open standard designed to enable seamless communication and collaboration between AI agents. In a world where agents are built using diverse frameworks and by different vendors, A2A provides a common language, breaking down silos and fostering interoperability.

- [Blog Post: Announcing the Agent2Agent Protocol (A2A)](https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/)
- [Watch the A2A Demo Video](https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/A2A_demo_v4.mp4)

![A2A Main Graphic](assets/a2a-main.png){width="50%"}
{style="text-align: center; margin-bottom:1em; margin-top:2em;"}

---

#

#### (google/A2A/docs/specification.md)[https://raw.githubusercontent.com/google/A2A/main/docs/specification.md] (Score: 0.63)
- 

The Agent2Agent (A2A) Protocol is an open standard designed to facilitate communication and interoperability between independent, potentially opaque AI agent systems. In an ecosystem where agents might be built using different frameworks, languages, or by different vendors, A2A provides a common language and interaction model.

This document provides the detailed technical specification for the A2A protocol. Its primary goal is to enable agents to:

- Discover each other's capabilities.
- Negotiate interaction modalities (text, files, structured data).
- Manage collaborative tasks.
- Securely exchange information to achieve user goals **without needing access to each other's internal state, memory, or tools.**

#

#### (google/A2A/llms.txt)[https://raw.githubusercontent.com/google/A2A/main/llms.txt] (Score: 0.62)
- operability
9. AI agents
10. Agent communication standard


- [index.md](https://raw.githubusercontent.com/google/A2A/main/docs/index.md): Agent2Agent (A2A) Protocol Documentation
The Agent2Agent (A2A) Protocol is an open standard that enables seamless communication and collaboration between AI agents built on diverse frameworks and by different vendors. It provides a common language, breaking down silos and fostering interoperability. A2A facilitates dynamic, multimodal communication between agents as peers, allowing them to delegate sub-tasks, exchange information, and coordinate actions to solve complex problems. The protocol ensures security and preserves intellectual property by enabling agents to interact without sharing internal memory, tools, or proprietary logic. To get started, users can read the introduction, dive into the specification, follow tutorials, or explore code samples. 

A code example is not explicitly provided in the document; however, the document references a Python quickstart tutorial: 
```markdow

#### (google/A2A/docs/specification/details.md)[https://raw.githubusercontent.com/google/A2A/main/docs/specification/details.md] (Score: 0.62)
- 

The Agent2Agent (A2A) Protocol is an open standard designed to facilitate communication and interoperability between independent, potentially opaque AI agent systems. In an ecosystem where agents might be built using different frameworks, languages, or vendors, A2A provides a common language and interaction model. This enables agents to discover each other's capabilities, negotiate interaction modalities (text, files, structured data), manage collaborative tasks, and securely exchange information to achieve user goals **without needing access to each other's internal state, memory, or tools.**

**Key Goals:**

- **Interoperability:** Bridge the gap between disparate agentic systems.
- **Collaboration:** Enable agents to delegate tasks, exchange context, and work together on complex user requests.
- **Discovery:** Allow agents to dynamically find and understand the capabilities of other agents.
- **Flexibility:** Support various interaction modes (text, files, forms, streaming).
- **Security:** Facilitate secure communication patterns suitable for enterprise environments.
- **Asynchronicity:** Support long-running tasks and interactions that may involve human-in-the-loop scenarios.

This document provides the detailed technical specification for the A2A protocol, defining its data structures, communication methods, and core concepts.

#

#### (google/A2A/docs/community.md)[https://raw.githubusercontent.com/google/A2A/main/docs/community.md] (Score: 0.62)
- 

The **Agent2Agent (A2A) protocol** is generating significant buzz across the
tech world, and for good reason! This open interoperability protocol is designed
to enable **seamless collaboration between AI agents across diverse frameworks
and vendors**. By standardizing communication, A2A aims to unlock complex
workflows, enhance productivity, and foster a new era of **"Agent
Interoperability"**. Don't just take our word for it  see what the community is
saying!


#### (google/A2A/docs/topics/a2a-and-mcp.md)[https://raw.githubusercontent.com/google/A2A/main/docs/topics/a2a-and-mcp.md] (Score: 0.60)
- A2A)

- **Focus:** A2A standardizes how independent, often opaque, **AI agents communicate and collaborate with each other as peers.**
- **Mechanism:** It provides an application-level protocol for agents to:
    - Discover each other's high-level skills and capabilities (via Agent Cards).
    - Negotiate interaction modalities (text, files, structured data).
    - Manage shared, stateful, and potentially long-running tasks.
    - Exchange conversational context, instructions, and complex, multi-part results.
- **Use Cases:**
    - A customer service agent delegating a complex billing inquiry to a specialized billing agent, maintaining context of the customer interaction.
    - A travel planning agent coordinating with separate flight, hotel, and activity booking agents, managing a multi-stage booking process.
    - Agents exchanging information and status updates for a collaborative project that evolves over time.
- **Key Difference from Tool Interaction:** A2A allows for more dynamic, stateful, and potentially multi-modal interactions than typically seen with simple tool calls. Agents using A2A communicate *as agents* (or on behalf of users) rather than just invoking a discrete function

#### (google/A2A/docs/topics/enterprise-ready.md)[https://raw.githubusercontent.com/google/A2A/main/docs/topics/enterprise-ready.md] (Score: 0.60)
- 

The Agent2Agent (A2A) protocol is designed with enterprise requirements at its core. Instead of inventing new, proprietary standards for security and operations, A2A aims to integrate seamlessly with existing enterprise infrastructure and widely adopted best practices. A2A treats remote agents as standard, HTTP-based enterprise applications. This approach allows organizations to leverage their existing investments and expertise in security, monitoring, governance, and identity management.

A key principle of A2A is that agents are typically "opaque"  they do not share internal memory, tools, or direct resource access with each other. This opacity naturally aligns with standard client/server security paradigms.


#### (google/A2A/docs/topics/what-is-a2a.md)[https://raw.githubusercontent.com/google/A2A/main/docs/topics/what-is-a2a.md] (Score: 0.60)
- 

Adopting A2A can lead to significant advantages:

- **Increased Interoperability:** Break down silos between different AI agent ecosystems, allowing agents from various vendors and frameworks to work together.
- **Enhanced Agent Capabilities:** Allow developers to create more sophisticated applications by composing the strengths of multiple specialized agents.
- **Reduced Integration Complexity:** Standardize the "how" of agent communication, allowing teams to focus on the "what"  the value their agents provide.
- **Fostering Innovation:** Encourage the development of a richer ecosystem of specialized agents that can readily plug into larger collaborative workflows.
- **Future-Proofing:** Provide a flexible framework that can adapt as agent technologies continue to evolve.

By establishing common ground for agent-to-agent communication, A2A aims to accelerate the adoption and utility of AI agents across diverse industries and applications, paving the way for more powerful and collaborative AI systems.

[Watch the A2A Demo Video](https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/A2A_demo_v4.mp4)

Next, learn about the [Key Concepts](./key-concepts.md) that form the foundation of the A2A protoc
- 

A2A provides a standardized way for these independent, often "opaque" (black-box) agentic systems to interact. It defines:

- **A common transport and format:** JSON-RPC 2.0 over HTTP(S) for how messages are structured and transmitted.
- **Discovery mechanisms (Agent Cards):** How agents can advertise their capabilities and be found by other agents.
- **Task management workflows:** How collaborative tasks are initiated, progressed, and completed. This includes support for tasks that may be long-running or require multiple turns of interaction.
- **Support for various data modalities:** How agents exchange not just text, but also files, structured data (like forms), and potentially other rich media.
- **Core principles for security and asynchronicity:** Guidelines for secure communication and handling tasks that might take significant time or involve human-in-the-loop processes.

#

#### (google/A2A/docs/llms.txt)[https://raw.githubusercontent.com/google/A2A/main/docs/llms.txt] (Score: 0.60)
- 

*   **Project Name:** Agent2Agent (A2A) Protocol
*   **Purpose:** An open protocol by Google enabling communication and interoperability between AI agents built on different frameworks or by different vendors.
*   **Core Goal:** Allow agents to discover capabilities, negotiate interaction, and collaborate securely on tasks.
*   **Communication:** Uses JSON-RPC 2.0 over HTTP(S). Supports standard request/response and Server-Sent Events (SSE) for streaming.
*   **Key Components:** Specification (JSON Schema), Common Libraries (Python, JS/TS), Sample Agents (LangGraph, CrewAI, ADK, Genkit), Sample Hosts (CLI, Orchestrator), Demo Web App (Mesop).

#

Step 1<Thoughts>
Thoughts:
- The previous response provides a comprehensive explanation of the A2A protocol.  It's highly detailed and seems to cover most aspects of the protocol.
- There's no need to call any additional tools as the information already obtained is sufficient to answer the question thoroughly.
</Thoughts>

<Selected tools>
[]
</Selected tools>

<Answer>
The A2A (Agent2Agent) protocol is an open standard initiated by Google to enable communication and interoperability between different AI agent systems.  Its core goal is to allow agents built on various frameworks or by different vendors to discover each other's capabilities, negotiate interaction methods (text, files, structured data, potentially audio/video), and collaborate on tasks.  

Key features include:

* **Agent Discovery:** Agents discover each other's capabilities via Agent Cards, typically located at `/.well-known/agent.json`.
* **Standardized Task Management:**  A standardized workflow for managing tasks (send, get, cancel).
* **Support for Various Content Types:** Supports various content types through `Parts` and `Artifacts` (text, files, structured data).
* **Streaming Updates:**  Supports streaming updates for long-running tasks using Server-Sent Events (SSE).
* **Push Notifications:**  Provides mechanisms for push notifications to keep clients updated.
* **JSON-RPC 2.0 over HTTP(S):** Uses JSON-RPC 2.0 over HTTP(S) for communication, supporting both standard request/response and SSE for streaming.

The protocol is defined by a JSON Schema, and supporting libraries and sample implementations are available in Python and JavaScript/TypeScript.  These samples demonstrate integration with various agent frameworks (LangGraph, CrewAI, Google ADK, Genkit) and include example agents, hosts, and a demo web application.  The protocol also incorporates considerations for security and enterprise readiness.
</Answer>
</Agent>
<Thoughts>
Thoughts:
- The question "What is A2A protocol?" has already been answered comprehensively by the A2A Protocol Agent.  
- No further agents are needed. The provided answer is sufficient.
</Thoughts>

<Selected agents>
```json
[]
```
</Selected agents>

<Answer>
The A2A Protocol Agent's response provides a complete and accurate definition of the A2A protocol, including its key features and functionalities.  No further information is required.
</Answer>

================
File: samples/python/agents/crewai/__main__.py
================
"""This file serves as the main entry point for the application.

It initializes the A2A server, defines the agent's capabilities,
and starts the server to handle incoming requests.
"""

import logging
import os

import click

from a2a.server.apps import A2AStarletteApplication
from a2a.server.request_handlers import DefaultRequestHandler
from a2a.server.tasks import InMemoryTaskStore
from a2a.types import (
    AgentCapabilities,
    AgentCard,
    AgentSkill,
)
from agent import ImageGenerationAgent
from agent_executor import ImageGenerationAgentExecutor
from dotenv import load_dotenv


load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MissingAPIKeyError(Exception):
    """Exception for missing API key."""


@click.command()
@click.option('--host', 'host', default='localhost')
@click.option('--port', 'port', default=10001)
def main(host, port):
    """Entry point for the A2A + CrewAI Image generation sample."""
    try:
        if not os.getenv('GOOGLE_API_KEY') and not os.getenv(
            'GOOGLE_GENAI_USE_VERTEXAI'
        ):
            raise MissingAPIKeyError(
                'GOOGLE_API_KEY or Vertex AI environment variables not set.'
            )

        capabilities = AgentCapabilities(streaming=False)
        skill = AgentSkill(
            id='image_generator',
            name='Image Generator',
            description=(
                'Generate stunning, high-quality images on demand and leverage'
                ' powerful editing capabilities to modify, enhance, or completely'
                ' transform visuals.'
            ),
            tags=['generate image', 'edit image'],
            examples=['Generate a photorealistic image of raspberry lemonade'],
        )

        agent_host_url = (
            os.getenv('HOST_OVERRIDE')
            if os.getenv('HOST_OVERRIDE')
            else f'http://{host}:{port}/'
        )
        agent_card = AgentCard(
            name='Image Generator Agent',
            description=(
                'Generate stunning, high-quality images on demand and leverage'
                ' powerful editing capabilities to modify, enhance, or completely'
                ' transform visuals.'
            ),
            url=agent_host_url,
            version='1.0.0',
            default_input_modes=ImageGenerationAgent.SUPPORTED_CONTENT_TYPES,
            default_output_modes=ImageGenerationAgent.SUPPORTED_CONTENT_TYPES,
            capabilities=capabilities,
            skills=[skill],
        )

        request_handler = DefaultRequestHandler(
            agent_executor=ImageGenerationAgentExecutor(),
            task_store=InMemoryTaskStore(),
        )
        server = A2AStarletteApplication(
            agent_card=agent_card, http_handler=request_handler
        )
        import uvicorn

        uvicorn.run(server.build(), host=host, port=port)

    except MissingAPIKeyError as e:
        logger.error(f'Error: {e}')
        exit(1)
    except Exception as e:
        logger.error(f'An error occurred during server startup: {e}')
        exit(1)


if __name__ == '__main__':
    main()

================
File: samples/python/agents/crewai/agent_executor.py
================
from a2a.server.agent_execution import AgentExecutor, RequestContext
from a2a.server.events import EventQueue
from a2a.types import (
    FilePart,
    FileWithBytes,
    InvalidParamsError,
    Part,
    Task,
    TextPart,
    UnsupportedOperationError,
)
from a2a.utils import (
    completed_task,
    new_artifact,
)
from a2a.utils.errors import ServerError
from agent import ImageGenerationAgent


class ImageGenerationAgentExecutor(AgentExecutor):
    """Reimbursement AgentExecutor Example."""

    def __init__(self) -> None:
        self.agent = ImageGenerationAgent()

    async def execute(
        self,
        context: RequestContext,
        event_queue: EventQueue,
    ) -> None:
        error = self._validate_request(context)
        if error:
            raise ServerError(error=InvalidParamsError())

        query = context.get_user_input()
        try:
            result = self.agent.invoke(query, context.context_id)
            print(f'Final Result ===> {result}')
        except Exception as e:
            print('Error invoking agent: %s', e)
            raise ServerError(
                error=ValueError(f'Error invoking agent: {e}')
            ) from e

        data = self.agent.get_image_data(
            session_id=context.context_id, image_key=result.raw
        )
        if data and not data.error:
            parts = [
                FilePart(
                    file=FileWithBytes(
                        bytes=data.bytes,
                        mime_type=data.mime_type,
                        name=data.id,
                    )
                )
            ]
        else:
            parts = [
                Part(
                    root=TextPart(
                        text=data.error if data else 'failed to generate image'
                    ),
                )
            ]
        await event_queue.enqueue_event(
            completed_task(
                context.task_id,
                context.context_id,
                [new_artifact(parts, f'image_{context.task_id}')],
                [context.message],
            )
        )

    async def cancel(
        self, request: RequestContext, event_queue: EventQueue
    ) -> Task | None:
        raise ServerError(error=UnsupportedOperationError())

    def _validate_request(self, context: RequestContext) -> bool:
        return False

================
File: samples/python/agents/crewai/agent.py
================
"""Crew AI based sample for A2A protocol.

Handles the agents and also presents the tools required.
"""

import base64
import logging
import os
import re

from collections.abc import AsyncIterable
from io import BytesIO
from typing import Any
from uuid import uuid4

from PIL import Image
from crewai import LLM, Agent, Crew, Task
from crewai.process import Process
from crewai.tools import tool
from dotenv import load_dotenv
from google import genai
from google.genai import types
from in_memory_cache import InMemoryCache
from pydantic import BaseModel


load_dotenv()

logger = logging.getLogger(__name__)


class Imagedata(BaseModel):
    """Represents image data.

    Attributes:
      id: Unique identifier for the image.
      name: Name of the image.
      mime_type: MIME type of the image.
      bytes: Base64 encoded image data.
      error: Error message if there was an issue with the image.
    """

    id: str | None = None
    name: str | None = None
    mime_type: str | None = None
    bytes: str | None = None
    error: str | None = None


@tool('ImageGenerationTool')
def generate_image_tool(
    prompt: str, session_id: str, artifact_file_id: str = None
) -> str:
    """Image generation tool that generates images or modifies a given image based on a prompt."""
    if not prompt:
        raise ValueError('Prompt cannot be empty')

    client = genai.Client()
    cache = InMemoryCache()

    text_input = (
        prompt,
        'Ignore any input images if they do not match the request.',
    )

    ref_image = None
    logger.info(f'Session id {session_id}')
    print(f'Session id {session_id}')

    # TODO (rvelicheti) - Change convoluted memory handling logic to a better
    # version.
    # Get the image from the cache and send it back to the model.
    # Assuming the last version of the generated image is applicable.
    # Convert to PIL Image so the context sent to the LLM is not overloaded
    try:
        ref_image_data = None
        # image_id = session_cache[session_id][-1]
        session_image_data = cache.get(session_id)
        if artifact_file_id:
            try:
                ref_image_data = session_image_data[artifact_file_id]
                logger.info('Found reference image in prompt input')
            except Exception:
                ref_image_data = None
        if not ref_image_data:
            # Insertion order is maintained from python 3.7
            latest_image_key = list(session_image_data.keys())[-1]
            ref_image_data = session_image_data[latest_image_key]

        ref_bytes = base64.b64decode(ref_image_data.bytes)
        ref_image = Image.open(BytesIO(ref_bytes))
    except Exception:
        ref_image = None

    if ref_image:
        contents = [text_input, ref_image]
    else:
        contents = text_input

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp',
            contents=contents,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            ),
        )
    except Exception as e:
        logger.error(f'Error generating image {e}')
        print(f'Exception {e}')
        return -999999999

    for part in response.candidates[0].content.parts:
        if part.inline_data is not None:
            try:
                print('Creating image data')
                data = Imagedata(
                    bytes=base64.b64encode(part.inline_data.data).decode(
                        'utf-8'
                    ),
                    mime_type=part.inline_data.mime_type,
                    name='generated_image.png',
                    id=uuid4().hex,
                )
                session_data = cache.get(session_id)
                if session_data is None:
                    # Session doesn't exist, create it with the new item
                    cache.set(session_id, {data.id: data})
                else:
                    # Session exists, update the existing dictionary directly
                    session_data[data.id] = data

                return data.id
            except Exception as e:
                logger.error(f'Error unpacking image {e}')
                print(f'Exception {e}')
    return -999999999


class ImageGenerationAgent:
    """Agent that generates images based on user prompts."""

    SUPPORTED_CONTENT_TYPES = ['text', 'text/plain', 'image/png']

    def __init__(self):
        if os.getenv('GOOGLE_GENAI_USE_VERTEXAI'):
            self.model = LLM(model='vertex_ai/gemini-2.0-flash')
        elif os.getenv('GOOGLE_API_KEY'):
            self.model = LLM(
                model='gemini/gemini-2.0-flash',
                api_key=os.getenv('GOOGLE_API_KEY'),
            )

        self.image_creator_agent = Agent(
            role='Image Creation Expert',
            goal=(
                "Generate an image based on the user's text prompt.If the prompt is"
                ' vague, ask clarifying questions (though the tool currently'
                " doesn't support back-and-forth within one run). Focus on"
                " interpreting the user's request and using the Image Generator"
                ' tool effectively.'
            ),
            backstory=(
                'You are a digital artist powered by AI. You specialize in taking'
                ' textual descriptions and transforming them into visual'
                ' representations using a powerful image generation tool. You aim'
                ' for accuracy and creativity based on the prompt provided.'
            ),
            verbose=False,
            allow_delegation=False,
            tools=[generate_image_tool],
            llm=self.model,
        )

        self.image_creation_task = Task(
            description=(
                "Receive a user prompt: '{user_prompt}'.\nAnalyze the prompt and"
                ' identify if you need to create a new image or edit an existing'
                ' one. Look for pronouns like this, that etc in the prompt, they'
                ' might provide context, rewrite the prompt to include the'
                ' context.If creating a new image, ignore any images provided as'
                " input context.Use the 'Image Generator' tool to for your image"
                ' creation or modification. The tool will expect a prompt which is'
                ' the {user_prompt} and the session_id which is {session_id}.'
                ' Optionally the tool will also expect an artifact_file_id which is '
                ' sent to you as {artifact_file_id}'
            ),
            expected_output='The id of the generated image',
            agent=self.image_creator_agent,
        )

        self.image_crew = Crew(
            agents=[self.image_creator_agent],
            tasks=[self.image_creation_task],
            process=Process.sequential,
            verbose=False,
        )

    def extract_artifact_file_id(self, query):
        try:
            pattern = r'(?:id|artifact-file-id)\s+([0-9a-f]{32})'
            match = re.search(pattern, query)

            if match:
                return match.group(1)
            return None
        except Exception:
            return None

    def invoke(self, query, session_id) -> str:
        """Kickoff CrewAI and return the response."""
        artifact_file_id = self.extract_artifact_file_id(query)

        inputs = {
            'user_prompt': query,
            'session_id': session_id,
            'artifact_file_id': artifact_file_id,
        }
        logger.info(f'Inputs {inputs}')
        print(f'Inputs {inputs}')
        response = self.image_crew.kickoff(inputs)
        return response

    async def stream(self, query: str) -> AsyncIterable[dict[str, Any]]:
        """Streaming is not supported by CrewAI."""
        raise NotImplementedError('Streaming is not supported by CrewAI.')

    def get_image_data(self, session_id: str, image_key: str) -> Imagedata:
        """Return Imagedata given a key. This is a helper method from the agent."""
        cache = InMemoryCache()
        session_data = cache.get(session_id)
        try:
            cache.get(session_id)
            return session_data[image_key]
        except KeyError:
            logger.error('Error generating image')
            return Imagedata(error='Error generating image, please try again.')

================
File: samples/python/agents/crewai/in_memory_cache.py
================
"""In Memory Cache utility."""

import threading
import time

from typing import Any, Optional


class InMemoryCache:
    """A thread-safe Singleton class to manage cache data.

    Ensures only one instance of the cache exists across the application.
    """

    _instance: Optional['InMemoryCache'] = None
    _lock: threading.Lock = threading.Lock()
    _initialized: bool = False

    def __new__(cls):
        """Override __new__ to control instance creation (Singleton pattern).

        Uses a lock to ensure thread safety during the first instantiation.

        Returns:
            The singleton instance of InMemoryCache.
        """
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self):
        """Initialize the cache storage.

        Uses a flag (_initialized) to ensure this logic runs only on the very first
        creation of the singleton instance.
        """
        if not self._initialized:
            with self._lock:
                if not self._initialized:
                    # print("Initializing SessionCache storage")
                    self._cache_data: dict[str, dict[str, Any]] = {}
                    self._ttl: dict[str, float] = {}
                    self._data_lock: threading.Lock = threading.Lock()
                    self._initialized = True

    def set(self, key: str, value: Any, ttl: int | None = None) -> None:
        """Set a key-value pair.

        Args:
            key: The key for the data.
            value: The data to store.
            ttl: Time to live in seconds. If None, data will not expire.
        """
        with self._data_lock:
            self._cache_data[key] = value

            if ttl is not None:
                self._ttl[key] = time.time() + ttl
            elif key in self._ttl:
                del self._ttl[key]

    def get(self, key: str, default: Any = None) -> Any:
        """Get the value associated with a key.

        Args:
            key: The key for the data within the session.
            default: The value to return if the session or key is not found.

        Returns:
            The cached value, or the default value if not found.
        """
        with self._data_lock:
            if key in self._ttl and time.time() > self._ttl[key]:
                del self._cache_data[key]
                del self._ttl[key]
                return default
            return self._cache_data.get(key, default)

    def delete(self, key: str) -> None:
        """Delete a specific key-value pair from a cache.

        Args:
            key: The key to delete.

        Returns:
            True if the key was found and deleted, False otherwise.
        """
        with self._data_lock:
            if key in self._cache_data:
                del self._cache_data[key]
                if key in self._ttl:
                    del self._ttl[key]
                return True
            return False

    def clear(self) -> bool:
        """Remove all data.

        Returns:
            True if the data was cleared, False otherwise.
        """
        with self._data_lock:
            self._cache_data.clear()
            self._ttl.clear()
            return True
        return False

================
File: samples/python/agents/crewai/pyproject.toml
================
[project]
name = "a2a-samples-image-gen"
version = "0.1.0"
description = "Generate or modify images using A2A and CrewAI"
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    "crewai[tools]>=0.95.0",
    "google-genai>=1.9.0",
    "a2a-sdk>=0.3.0",
]

================
File: samples/python/agents/crewai/README.md
================
# CrewAI Agent with A2A Protocol

This sample demonstrates a simple image generation agent built with [CrewAI](https://www.crewai.com/open-source) and exposed through the A2A protocol.

## How It Works

This agent utilizes CrewAI and the Google Gemini API to generate images based on text prompts. The A2A protocol enables standardized interaction with the agent, allowing clients to send requests and receive images as artifacts.

```mermaid
sequenceDiagram
    participant Client as A2A Client
    participant Server as A2A Server
    participant Agent as CrewAI Agent
    participant API as Gemini API

    Client->>Server: Send task with text prompt
    Server->>Agent: Forward prompt to image agent
    Note over Server,Agent: Optional: Simulated streaming updates
    Agent->>API: Generate image using Gemini
    API->>Agent: Return generated image
    Agent->>Server: Store image and return ID
    Server->>Client: Respond with image artifact
```

## Key Components

- **CrewAI Agent**: Image generation agent with specialized tools
- **A2A Server**: Provides standardized protocol for interacting with the agent
- **Image Generation**: Uses Gemini API to create images from text descriptions
- **Cache System**: Stores generated images for retrieval (in-memory or file-based)

## Prerequisites

- Python 3.12 or higher
- [UV](https://docs.astral.sh/uv/) package manager (recommended)
- Google API Key (for Gemini access)

## Setup & Running

1. Navigate to the samples directory:

   ```bash
   cd samples/python/agents/crewai
   ```

2. Create an environment file with your API key (or Vertex AI credentials):

   ```bash
   echo "GOOGLE_API_KEY=your_api_key_here" > .env
   ```

3. Set up the Python environment:

   ```bash
   uv python pin 3.13
   uv venv
   source .venv/bin/activate
   ```

4. Run the agent with desired options:

   ```bash
   # Basic run
   uv run .

   # On custom host/port
   uv run . --host 0.0.0.0 --port 8080
   ```

5. Run the A2A client:

   In a separate terminal:

   ```bash
   # Connect to the agent (specify the agent URL with correct port and host)
   cd samples/python/hosts/cli
   uv run . --agent http://localhost:10001
   
   # If you changed the port when starting the agent, use that port instead
   # uv run . --agent http://localhost:YOUR_PORT
   ```

## Build Container Image

Agent can also be built using a container file.

1. Navigate to the `samples/python` directory:

   ```bash
   cd samples/python
   ```

2. Build the container file

    ```bash
    podman build -f agents/crewai/Containerfile . -t crewai-a2a-server
    ```

   > [!Tip]  
   > Podman is a drop-in replacement for `docker` which can also be used in these commands.

3. Run your container

    ```bash
    podman run -p 10001:10001 -e GOOGLE_API_KEY=your_api_key_here crewai-a2a-server
    ```

4. Run A2A client (follow step 5 from the section above)

> [!Important]
>
> - **Access URL:** You must access the A2A client through the URL `0.0.0.0:10001`. Using `localhost` will not work.
> - **Hostname Override:** If you're deploying to an environment where the hostname is defined differently outside the container, use the `HOST_OVERRIDE` environment variable to set the expected hostname on the Agent Card. This ensures proper communication with your client application.

## Features & Improvements

**Features:**

- Text-to-image generation using Google Gemini
- Support for modifying existing images using references
- Robust error handling with automatic retries
- Optional file-based cache persistence
- Improved artifact ID extraction from queries

**Limitations:**

- No true streaming (CrewAI doesn't natively support it)
- Limited agent interactions (no multi-turn conversations)

## Learn More

- [A2A Protocol Documentation](https://a2a-protocol.org)
- [CrewAI Documentation](https://docs.crewai.com/introduction)
- [Google Gemini API](https://ai.google.dev/gemini-api)

================
File: samples/python/agents/headless_agent_auth/__main__.py
================
import asyncio
import json
import logging
import os
import sys

import click
import uvicorn

from dotenv import load_dotenv


load_dotenv()

from a2a.server.apps import A2AStarletteApplication
from a2a.server.request_handlers import DefaultRequestHandler
from a2a.server.tasks import InMemoryTaskStore
from a2a.types import (
    AgentAuthentication,
    AgentCapabilities,
    AgentCard,
    AgentSkill,
    # ClientCredentialsOAuthFlow,
    # OAuth2SecurityScheme,
    # OAuthFlows,
)
from agent import HRAgent
from agent_executor import HRAgentExecutor
from api import hr_api
from oauth2_middleware import OAuth2Middleware


logging.basicConfig(level=logging.INFO)
logger = logging.getLogger()


@click.command()
@click.option('--host', default='0.0.0.0')
@click.option('--port_agent', default=10050)
@click.option('--port_api', default=10051)
def main(host: str, port_agent: int, port_api: int):
    async def run_all():
        await asyncio.gather(
            start_agent(host, port_agent),
            start_api(host, port_api),
        )

    asyncio.run(run_all())


async def start_agent(host: str, port):
    agent_card = AgentCard(
        name='Staff0 HR Agent',
        description='This agent handles external verification requests about Staff0 employees made by third parties.',
        url=f'http://{host}:{port}/',
        version='0.1.0',
        default_input_modes=HRAgent.SUPPORTED_CONTENT_TYPES,
        default_output_modes=HRAgent.SUPPORTED_CONTENT_TYPES,
        capabilities=AgentCapabilities(streaming=True),
        skills=[
            AgentSkill(
                id='is_active_employee',
                name='Check Employment Status Tool',
                description='Confirm whether a person is an active employee of the company.',
                tags=['employment status'],
                examples=[
                    'Does John Doe with email jdoe@staff0.com work at Staff0?'
                ],
            )
        ],
        authentication=AgentAuthentication(
            schemes=['oauth2'],
            credentials=json.dumps(
                {
                    'tokenUrl': f'https://{os.getenv("HR_AUTH0_DOMAIN")}/oauth/token',
                    'scopes': {
                        'read:employee_status': 'Allows confirming whether a person is an active employee of the company.'
                    },
                }
            ),
        ),
        # security_schemes={
        #     'oauth2_m2m_client': OAuth2SecurityScheme(
        #         description='',
        #         flows=OAuthFlows(
        #             authorization_code=ClientCredentialsOAuthFlow(
        #                 token_url=f'https://{os.getenv("HR_AUTH0_DOMAIN")}/oauth/token',
        #                 scopes={
        #                     'read:employee_status': 'Allows confirming whether a person is an active employee of the company.',
        #                 },
        #             ),
        #         ),
        #     ),
        # },
        # security=[{
        #     'oauth2_m2m_client': [
        #         'read:employee_status',
        #     ],
        # }],
    )

    request_handler = DefaultRequestHandler(
        agent_executor=HRAgentExecutor(),
        task_store=InMemoryTaskStore(),
    )

    server = A2AStarletteApplication(
        agent_card=agent_card, http_handler=request_handler
    )

    app = server.build()
    app.add_middleware(
        OAuth2Middleware,
        agent_card=agent_card,
        public_paths=['/.well-known/agent.json'],
    )

    logger.info(f'Starting HR Agent server on {host}:{port}')
    await uvicorn.Server(uvicorn.Config(app=app, host=host, port=port)).serve()


async def start_api(host: str, port):
    logger.info(f'Starting HR API server on {host}:{port}')
    await uvicorn.Server(
        uvicorn.Config(app=hr_api, host=host, port=port)
    ).serve()


# this ensures that `main()` runs when using `uv run .`
if not hasattr(sys, '_called_from_uvicorn'):
    main()

================
File: samples/python/agents/headless_agent_auth/.env.example
================
# Gemini
GOOGLE_API_KEY=

# Auth0
HR_AUTH0_DOMAIN=

# A2A Client -> HR AGENT (auth method: Client Credentials)
A2A_CLIENT_AUTH0_CLIENT_ID=
A2A_CLIENT_AUTH0_CLIENT_SECRET=

# HR AGENT -> HR API (auth method: CIBA)
HR_AGENT_AUTH0_CLIENT_ID=
HR_AGENT_AUTH0_CLIENT_SECRET=

HR_AGENT_AUTH0_AUDIENCE=https://staff0/agent
HR_API_AUTH0_AUDIENCE=https://staff0/api

================
File: samples/python/agents/headless_agent_auth/agent_executor.py
================
from a2a.server.agent_execution import AgentExecutor, RequestContext
from a2a.server.events.event_queue import EventQueue
from a2a.types import (
    Task,
    TaskArtifactUpdateEvent,
    TaskState,
    TaskStatus,
    TaskStatusUpdateEvent,
)
from a2a.utils import new_agent_text_message, new_task, new_text_artifact
from agent import HRAgent


class HRAgentExecutor(AgentExecutor):
    """HR AgentExecutor Example."""

    def __init__(self):
        self.agent = HRAgent()

    async def execute(
        self,
        context: RequestContext,
        event_queue: EventQueue,
    ) -> None:
        query = context.get_user_input()
        task = context.current_task

        if not task:
            task = new_task(context.message)
            await event_queue.enqueue_event(task)
        # invoke the underlying agent, using streaming results
        async for event in self.agent.stream(query, task.context_id):
            task_state = TaskState(event['task_state'])
            if event['is_task_complete']:
                await event_queue.enqueue_event(
                    TaskArtifactUpdateEvent(
                        append=False,
                        context_id=task.context_id,
                        task_id=task.id,
                        last_chunk=True,
                        artifact=new_text_artifact(
                            name='current_result',
                            description='Result of request to agent.',
                            text=event['content'],
                        ),
                    )
                )
                await event_queue.enqueue_event(
                    TaskStatusUpdateEvent(
                        status=TaskStatus(state=task_state),
                        final=True,
                        context_id=task.context_id,
                        task_id=task.id,
                    )
                )
            else:
                await event_queue.enqueue_event(
                    TaskStatusUpdateEvent(
                        status=TaskStatus(
                            state=task_state,
                            message=new_agent_text_message(
                                event['content'],
                                task.context_id,
                                task.id,
                            ),
                        ),
                        final=task_state
                        in {
                            TaskState.input_required,
                            TaskState.failed,
                            TaskState.unknown,
                        },
                        context_id=task.context_id,
                        task_id=task.id,
                    )
                )

    async def cancel(
        self, request: RequestContext, event_queue: EventQueue
    ) -> Task | None:
        raise Exception('cancel not supported')

================
File: samples/python/agents/headless_agent_auth/agent.py
================
import os

from collections.abc import AsyncIterable
from typing import Any, Literal

import httpx

from auth0.authentication.get_token import GetToken
from auth0.management import Auth0
from auth0_ai_langchain.auth0_ai import Auth0AI
from auth0_ai_langchain.ciba import get_ciba_credentials
from langchain_core.messages import AIMessage, ToolMessage
from langchain_core.runnables.config import RunnableConfig
from langchain_core.tools import tool
from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent
from pydantic import BaseModel


auth0_ai = Auth0AI(
    auth0={
        'domain': os.getenv('HR_AUTH0_DOMAIN'),
        'client_id': os.getenv('HR_AGENT_AUTH0_CLIENT_ID'),
        'client_secret': os.getenv('HR_AGENT_AUTH0_CLIENT_SECRET'),
    }
)


with_async_user_confirmation = auth0_ai.with_async_user_confirmation(
    binding_message='Approve sharing your employment details with an external party.',
    scopes=['read:employee'],
    user_id=lambda employee_id, **__: employee_id,
    audience=os.getenv('HR_API_AUTH0_AUDIENCE'),
    on_authorization_request='block',  # TODO: this is just for demo purposes
)


get_token = GetToken(
    domain=os.getenv('HR_AUTH0_DOMAIN'),
    client_id=os.getenv('HR_AGENT_AUTH0_CLIENT_ID'),
    client_secret=os.getenv('HR_AGENT_AUTH0_CLIENT_SECRET'),
)


@tool
async def is_active_employee(employee_id: str) -> dict[str, Any]:
    """Confirm whether a person is an active employee of the company.

    Args:
        employee_id (str): The employee's identification.

    Returns:
        dict: A dictionary containing the employment status, or an error message if the request fails.
    """
    try:
        credentials = get_ciba_credentials()
        response = await httpx.AsyncClient().get(
            f'{os.getenv("HR_API_BASE_URL", "http://localhost:10051")}/employees/{employee_id}',
            headers={
                'Authorization': f'{credentials["token_type"]} {credentials["access_token"]}',
                'Content-Type': 'application/json',
            },
        )

        if response.status_code == 404:
            return {'active': False}
        if response.status_code == 200:
            return {'active': True}
        response.raise_for_status()
    except httpx.HTTPError as e:
        return {'error': f'HR API request failed: {e}'}
    except Exception:
        return {'error': 'Unexpected response from HR API.'}


@tool
def get_employee_id_by_email(work_email: str) -> dict[str, Any] | None:
    """Return the employee ID by email.

    Args:
        work_email (str): The employee's work email.

    Returns:
        dict: A dictionary containing the employee ID if it exists, otherwise None.
    """
    try:
        user = Auth0(
            domain=get_token.domain,
            token=get_token.client_credentials(
                f'https://{os.getenv("HR_AUTH0_DOMAIN")}/api/v2/'
            )['access_token'],
        ).users_by_email.search_users_by_email(
            email=work_email, fields=['user_id']
        )[0]

        return {'employee_id': user['user_id']} if user else None
    except Exception:
        return {'error': 'Unexpected response from Auth0 Management API.'}


class ResponseFormat(BaseModel):
    """Respond to the user in this format."""

    status: Literal['completed', 'input-required', 'rejected', 'failed'] = (
        'failed'
    )
    message: str


class HRAgent:
    SUPPORTED_CONTENT_TYPES = ['text', 'text/plain']

    SYSTEM_INSTRUCTION: str = (
        'You are an agent who handles external verification requests about Staff0 employees made by third parties.'
        'Do not attempt to answer unrelated questions or use tools for other purposes.'
        "If you are asked about a person's employee status using their employee ID, use the `is_active_employee` tool."
        'If they provide a work email instead, first call the `get_employee_id_by_email` tool to get the employee ID, and then use `is_active_employee`.'
    )

    RESPONSE_FORMAT_INSTRUCTION: str = (
        'Set the status to "completed" if the request has been fully processed.'
        'Set the status to "input-required" if the tool response indicates that user input is needed to proceed.'
        'If the tool response contains an AccessDeniedInterrupt error, set the message to "The user denied the authorization request", and set the status to "rejected".'
        'For any other tool error, set the status to "failed".'
    )

    def __init__(self):
        self.model = ChatGoogleGenerativeAI(model='gemini-2.0-flash')
        self.tools = [
            get_employee_id_by_email,
            with_async_user_confirmation(is_active_employee),
        ]

        self.graph = create_react_agent(
            self.model,
            tools=self.tools,
            checkpointer=MemorySaver(),
            prompt=self.SYSTEM_INSTRUCTION,
            response_format=(self.RESPONSE_FORMAT_INSTRUCTION, ResponseFormat),
        )

    async def invoke(self, query: str, context_id: str) -> dict[str, Any]:
        config: RunnableConfig = {'configurable': {'thread_id': context_id}}
        await self.graph.ainvoke({'messages': [('user', query)]}, config)
        return self.get_agent_response(config)

    async def stream(
        self, query: str, context_id: str
    ) -> AsyncIterable[dict[str, Any]]:
        inputs: dict[str, Any] = {'messages': [('user', query)]}
        config: RunnableConfig = {'configurable': {'thread_id': context_id}}

        async for item in self.graph.astream(
            inputs, config, stream_mode='values'
        ):
            message = item['messages'][-1] if 'messages' in item else None
            if message:
                if (
                    isinstance(message, AIMessage)
                    and message.tool_calls
                    and len(message.tool_calls) > 0
                ):
                    yield {
                        'is_task_complete': False,
                        'task_state': 'working',
                        'content': 'Looking up the employment status...',
                    }
                elif isinstance(message, ToolMessage):
                    yield {
                        'is_task_complete': False,
                        'task_state': 'working',
                        'content': 'Processing the employment status...',
                    }

        yield self.get_agent_response(config)

    def get_agent_response(self, config: RunnableConfig) -> dict[str, Any]:
        current_state = self.graph.get_state(config)
        structured_response = current_state.values.get('structured_response')

        if structured_response and isinstance(
            structured_response, ResponseFormat
        ):
            return {
                'is_task_complete': structured_response.status == 'completed',
                'task_state': structured_response.status,
                'content': structured_response.message,
            }

        return {
            'is_task_complete': False,
            'task_state': 'unknown',
            'content': 'We are unable to process your request at the moment. Please try again.',
        }

================
File: samples/python/agents/headless_agent_auth/api.py
================
import os

from fastapi import Depends, FastAPI
from fastapi_plugin import Auth0FastAPI


auth0 = Auth0FastAPI(
    domain=os.getenv('HR_AUTH0_DOMAIN'),
    audience=os.getenv('HR_API_AUTH0_AUDIENCE'),
)

app = FastAPI()


@app.get('/employees/{id}')
def get_employee(
    id: str, _claims: dict = Depends(auth0.require_auth(scopes='read:employee'))
):
    # TODO: if needed, return more employee details
    return {'employee_id': id}


hr_api = app

================
File: samples/python/agents/headless_agent_auth/oauth2_middleware.py
================
import json
import os

from a2a.types import AgentCard
from auth0_api_python import ApiClient, ApiClientOptions
from starlette.applications import Starlette
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.requests import Request
from starlette.responses import JSONResponse, PlainTextResponse


api_client = ApiClient(
    ApiClientOptions(
        domain=os.getenv('HR_AUTH0_DOMAIN'),
        audience=os.getenv('HR_AGENT_AUTH0_AUDIENCE'),
    )
)


class OAuth2Middleware(BaseHTTPMiddleware):
    """Starlette middleware that authenticates A2A access using an OAuth2 bearer token."""

    def __init__(
        self,
        app: Starlette,
        agent_card: AgentCard = None,
        public_paths: list[str] = None,
    ):
        super().__init__(app)
        self.agent_card = agent_card
        self.public_paths = set(public_paths or [])

        # Process the AgentCard to identify what (if any) Security Requirements are defined at the root of the
        # AgentCard, indicating agent-level authentication/authorization.

        # Use app state for this demonstration (simplicity)
        self.a2a_auth = {}

        # Process the Authentication Requirements Object
        if agent_card.authentication:
            credentials = json.loads(
                agent_card.authentication.credentials or '{}'
            )
            if 'scopes' in credentials:
                self.a2a_auth = {
                    'required_scopes': credentials['scopes'].keys()
                }

        # # Process the Security Requirements Object
        # for sec_req in agent_card.security or []:
        #     # Since we pre-validated (non-exhaustive) the used parts of the Security Schemes and Security
        #     # Requirements, the code below WILL NOT do any validation.

        #     # An empty Security Requirement Object means you allow anonymous, no need to process any other Security
        #     # Requirements Objects
        #     if not sec_req:
        #         break

        #     # Demonstrate how one could process the Security Requirements to configure the machinery used to
        #     # authenticate and/or authorize agent interactions.
        #     #
        #     # Note: This is written purely to support the sample and is for demonstration purposes only.
        #     for name, scopes in sec_req.items():
        #         # sec_scheme = self.agent_card.security_schemes[name]

        #         # if not isinstance(sec_scheme, OAuth2SecurityScheme) or sec_scheme.flows.authorization_code is None:
        #         #     raise NotImplementedError('Only OAuth2SecurityScheme -> ClientCredentialsOAuthFlow is supported.')

        #         self.a2a_auth = { 'required_scopes': scopes }

    async def dispatch(self, request: Request, call_next):
        path = request.url.path

        # Allow public paths and anonymous access
        if path in self.public_paths or not self.a2a_auth:
            return await call_next(request)

        # Authenticate the request
        auth_header = request.headers.get('Authorization')
        if not auth_header or not auth_header.startswith('Bearer '):
            return self._unauthorized(
                'Missing or malformed Authorization header.', request
            )

        access_token = auth_header.split('Bearer ')[1]

        try:
            if self.a2a_auth:
                payload = await api_client.verify_access_token(
                    access_token=access_token
                )
                scopes = payload.get('scope', '').split()
                missing_scopes = [
                    s
                    for s in self.a2a_auth['required_scopes']
                    if s not in scopes
                ]
                if missing_scopes:
                    return self._forbidden(
                        f'Missing required scopes: {missing_scopes}', request
                    )

        except Exception as e:
            return self._forbidden(f'Authentication failed: {e}', request)

        return await call_next(request)

    def _forbidden(self, reason: str, request: Request):
        accept_header = request.headers.get('accept', '')
        if 'text/event-stream' in accept_header:
            return PlainTextResponse(
                f'error forbidden: {reason}',
                status_code=403,
                media_type='text/event-stream',
            )
        return JSONResponse(
            {'error': 'forbidden', 'reason': reason}, status_code=403
        )

    def _unauthorized(self, reason: str, request: Request):
        accept_header = request.headers.get('accept', '')
        if 'text/event-stream' in accept_header:
            return PlainTextResponse(
                f'error unauthorized: {reason}',
                status_code=401,
                media_type='text/event-stream',
            )
        return JSONResponse(
            {'error': 'unauthorized', 'reason': reason}, status_code=401
        )

================
File: samples/python/agents/headless_agent_auth/pyproject.toml
================
[project]
name = "a2a-sample-headless-agent-auth"
version = "0.1.0"
description = "Example of an agent using Auth0 Client-Initiated Backchannel Authentication (CIBA) from a tool, and agent-level authorization via the OAuth 2.0 Client Credentials flow."
readme = "README.md"
requires-python = ">=3.13"
dependencies = [
    "a2a-sdk>=0.3.0",
    "asyncclick>=8.1.8",
    "auth0-ai-langchain==1.0.0b2",
    "auth0-api-python==1.0.0b3",
    "auth0-fastapi-api>=1.0.0b3",
    "auth0-python>=4.9.0",
    "click>=8.2.0",
    "fastapi>=0.115.12",
    "httpx>=0.28.1",
    "langchain-google-genai>=2.1.4",
    "langgraph>=0.4.3",
    "pydantic>=2.11.4",
    "python-dotenv>=1.1.0",
]

================
File: samples/python/agents/headless_agent_auth/README.md
================
# Auth for Headless Agents

This sample demonstrates how headless agent's tools can leverage [Auth0's Client-Initiated Backchannel Authentication (CIBA) flow](https://auth0.com/docs/get-started/authentication-and-authorization-flow/client-initiated-backchannel-authentication-flow) to request user authorization via push notification and obtain tokens for accessing separate APIs.

Additionally, it shows agent-level authorization via the [OAuth 2.0 Client Credentials flow](https://auth0.com/docs/get-started/authentication-and-authorization-flow/client-credentials-flow).

## How It Works

Allows an A2A client to securely interact with an external HR agent owned by the fictional company Staff0 to verify whether the provided user data corresponds to an active employee.

With the authorization of the employee involved (via push notification), the Staff0 HR agent can access the company's internal HR API to retrieve employment details.

```mermaid
sequenceDiagram
   participant Employee as John Doe
   participant A2AClient as A2A Client
   participant Auth0 as Auth0 (staff0.auth0.com)
   participant HR_Agent as Staff0 HR Agent
   participant HR_API as Staff0 HR API

   A2AClient->>HR_Agent: Get A2A Agent Card
   HR_Agent-->>A2AClient: Agent Card
   A2AClient->>Auth0: Request access token (Client Credentials)
   Auth0-->>A2AClient: Access Token
   A2AClient->>HR_Agent: Does John Doe with email jdoe@staff0.com work at Staff0? (Access Token)
   HR_Agent->>Auth0: Request access token (CIBA)
   Auth0->>Employee: Push notification to approve access
   Employee-->>Auth0: Approves access
   Auth0-->>HR_Agent: Access Token
   HR_Agent->>HR_API: Retrieve employment details (Access Token)
   HR_API-->>HR_Agent: Employment details
   HR_Agent-->>A2AClient: Yes, John Doe is an active employee.
```

## Prerequisites

- Python 3.12 or higher
- [UV](https://docs.astral.sh/uv/)
- [Gemini API key](https://ai.google.dev/gemini-api/docs/api-key)
- An [Auth0](https://auth0.com/) tenant with the following configuration:
  - **APIs**
    - HR API
      - Audience: `https://staff0/api`
      - Permissions: `read:employee`
    - HR Agent
      - Audience: `https://staff0/agent`
      - Permissions: `read:employee_status`
  - **Applications**
    - A2A Client
      - Grant Types: `Client Credentials`
      - APIs: `HR Agent` (enabled permissions: `read:employee_status`)
    - HR Agent
      - Grant Types: `Client Initiated Backchannel Authentication (CIBA)`
      - APIs: `Auth0 Management API` (enabled permissions: `read:users`)
  - Push Notifications using [Auth0 Guardian](https://auth0.com/docs/secure/multi-factor-authentication/auth0-guardian) must be `enabled`.
  - A test user enrolled in Guardian MFA.

## Running the Sample

1. Create a `.env` file by copying [.env.example](.env.example), and provide the required environment variable values.

2. Start HR Agent and HR API:

   ```bash
   uv run --prerelease=allow .
   ```

3. Run the test client:
   ```bash
   uv run --prerelease=allow test_client.py
   ```

## Disclaimer
Important: The sample code provided is for demonstration purposes and illustrates the mechanics of the Agent-to-Agent (A2A) protocol. When building production applications, it is critical to treat any agent operating outside of your direct control as a potentially untrusted entity.

All data received from an external agentincluding but not limited to its AgentCard, messages, artifacts, and task statusesshould be handled as untrusted input. For example, a malicious agent could provide an AgentCard containing crafted data in its fields (e.g., description, name, skills.description). If this data is used without sanitization to construct prompts for a Large Language Model (LLM), it could expose your application to prompt injection attacks.  Failure to properly validate and sanitize this data before use can introduce security vulnerabilities into your application.

Developers are responsible for implementing appropriate security measures, such as input validation and secure handling of credentials to protect their systems and users.

================
File: samples/python/agents/headless_agent_auth/test_client.py
================
import asyncio
import json
import os

from typing import Any
from urllib.parse import urlparse
from uuid import uuid4

import asyncclick as click
import httpx

from a2a.client import A2ACardResolver, A2AClient
from a2a.types import (
    AgentCard,
    GetTaskRequest,
    Message,
    MessageSendParams,
    SendMessageRequest,
    SendStreamingMessageRequest,
    Task,
    TaskArtifactUpdateEvent,
    TaskQueryParams,
    TaskState,
    TaskStatusUpdateEvent,
    TextPart,
)
from auth0.authentication.get_token import GetToken
from dotenv import load_dotenv


load_dotenv()
access_token = None


class AgentAuth(httpx.Auth):
    """Custom httpx's authentication class to inject access token required by agent."""

    def __init__(self, agent_card: AgentCard):
        self.agent_card = agent_card

    def auth_flow(self, request):
        global access_token
        auth = self.agent_card.authentication

        # skip if not using oauth2 or credentials details are missing
        if not (
            any(scheme.lower() == 'oauth2' for scheme in auth.schemes)
            and auth.credentials
        ):
            yield request
            return

        if not access_token:
            token_url = json.loads(auth.credentials)['tokenUrl']
            print(f'\nFetching agent access token from {token_url}...')
            get_token = GetToken(
                domain=urlparse(token_url).hostname,
                client_id=os.getenv('A2A_CLIENT_AUTH0_CLIENT_ID'),
                client_secret=os.getenv('A2A_CLIENT_AUTH0_CLIENT_SECRET'),
            )
            access_token = get_token.client_credentials(
                os.getenv('HR_AGENT_AUTH0_AUDIENCE')
            )['access_token']
            print('Done.\n')

        request.headers['Authorization'] = f'Bearer {access_token}'
        yield request


@click.command()
@click.option('--agent', default='http://localhost:10050')
@click.option('--context_id')
@click.option('--history', default=False, is_flag=True)
@click.option('--debug', default=False, is_flag=True)
async def cli(agent: str, context_id: str | None, history: bool, debug: bool):
    async with httpx.AsyncClient() as httpx_client:
        agent_card = await (
            A2ACardResolver(
                httpx_client=httpx_client,
                base_url=agent,
            )
        ).get_agent_card()

        print('======= Agent Card ========')
        print(agent_card.model_dump_json(exclude_none=True, indent=2))

        httpx_client.auth = AgentAuth(agent_card)

        client = A2AClient(
            httpx_client=httpx_client,
            agent_card=agent_card,
        )

        if not context_id:
            context_id = uuid4().hex

        continue_loop = True
        streaming = agent_card.capabilities.streaming

        while continue_loop:
            task_id = uuid4().hex
            print('=========  Starting a New Task ======== ')
            continue_loop = await complete_task(
                client,
                streaming,
                task_id,
                context_id,
                debug,
            )

            if history and continue_loop:
                print('========= History ======== ')
                get_task_response = await client.get_task(
                    GetTaskRequest(
                        id=str(uuid4()),
                        params=TaskQueryParams(id=task_id, history_length=10),
                    )
                )
                print(
                    get_task_response.root.model_dump_json(
                        include={'result': {'history': True}}
                    )
                )


def create_send_params(
    text: str, task_id: str | None = None, context_id: str | None = None
) -> MessageSendParams:
    """Helper function to create the payload for sending a task."""
    send_params: dict[str, Any] = {
        'message': {
            'role': 'user',
            'parts': [{'type': 'text', 'text': text}],
            'messageId': uuid4().hex,
        },
        'configuration': {
            'acceptedOutputModes': ['text'],
        },
    }

    if task_id:
        send_params['message']['taskId'] = task_id

    if context_id:
        send_params['message']['contextId'] = context_id

    return MessageSendParams(**send_params)


async def complete_task(
    client: A2AClient,
    streaming: bool,
    task_id: str,
    context_id: str,
    debug: bool = False,
) -> bool:
    prompt = click.prompt(
        '\nWhat do you want to send to the agent? (:q or quit to exit)'
    )

    if prompt == ':q' or prompt == 'quit':
        return False

    send_params = create_send_params(
        text=prompt,
        task_id=task_id,
        context_id=context_id,
    )

    task = None
    if streaming:
        stream_response = client.send_message_streaming(
            SendStreamingMessageRequest(id=str(uuid4()), params=send_params)
        )
        async for chunk in stream_response:
            result = chunk.root.result
            print(
                f'stream event => {chunk.root.model_dump_json(exclude_none=True)}'
                if debug
                else (
                    next(
                        (
                            f'stream message => role: {result.role.value}, type: {part.root.type}, text: {part.root.text}'
                            for part in result.parts
                            if isinstance(part.root, TextPart)
                        ),
                        '',
                    )
                    if isinstance(result, Message)
                    else next(
                        (
                            f'stream message => role: {msg.role.value}, type: {part.root.type}, text: {part.root.text}'
                            for msg in result.history or []
                            for part in msg.parts
                            if isinstance(part.root, TextPart)
                        ),
                        '',
                    )
                    if isinstance(result, Task)
                    else next(
                        (
                            f'stream message => role: {result.status.message.role.value}, type: {part.root.type}, text: {part.root.text}'
                            for part in (
                                result.status.message.parts
                                if result.status.message
                                else []
                            )
                            if isinstance(part.root, TextPart)
                        ),
                        '',
                    )
                    if isinstance(result, TaskStatusUpdateEvent)
                    else next(
                        (
                            f'stream artifact => type: {part.root.type}, text: {part.root.text}'
                            for part in result.artifact.parts
                            if isinstance(part.root, TextPart)
                        ),
                        '',
                    )
                    if isinstance(result, TaskArtifactUpdateEvent)
                    else ''
                )
            )

        get_task_response = await client.get_task(
            GetTaskRequest(id=str(uuid4()), params=TaskQueryParams(id=task_id))
        )
        task = get_task_response.root.result
    else:
        send_message_response = await client.send_message(
            SendMessageRequest(id=str(uuid4()), params=send_params)
        )
        task = send_message_response.root.result
        print(f'\n{task.model_dump_json(exclude_none=True)}')

    # if the result is that more input is required, loop again.
    if task.status.state == TaskState.input_required:
        return await complete_task(
            client,
            streaming,
            task_id,
            context_id,
            debug,
        )

    # task is complete
    return True


if __name__ == '__main__':
    asyncio.run(cli())

================
File: samples/python/agents/langgraph/.vscode/launch.json
================
{
  "version": "0.2.0",
  "configurations": [
    {
      "name": "Debug Currency Agent",
      "type": "debugpy",
      "request": "launch",
      "program": "${workspaceFolder}/app/__main__.py",
      "console": "integratedTerminal",
      "justMyCode": false,
      "env": {
        "PYTHONPATH": "${workspaceFolder}"
      },
      "cwd": "${workspaceFolder}",
      "args": ["--host", "localhost", "--port", "10000"]
    }
  ]
}

================
File: samples/python/agents/langgraph/app/__main__.py
================
import logging
import os
import sys

import click
import httpx
import uvicorn

from a2a.server.apps import A2AStarletteApplication
from a2a.server.request_handlers import DefaultRequestHandler
from a2a.server.tasks import (
    BasePushNotificationSender,
    InMemoryPushNotificationConfigStore,
    InMemoryTaskStore,
)
from a2a.types import (
    AgentCapabilities,
    AgentCard,
    AgentSkill,
)
from dotenv import load_dotenv

from app.agent import CurrencyAgent
from app.agent_executor import CurrencyAgentExecutor


load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MissingAPIKeyError(Exception):
    """Exception for missing API key."""


@click.command()
@click.option('--host', 'host', default='localhost')
@click.option('--port', 'port', default=10000)
def main(host, port):
    """Starts the Currency Agent server."""
    try:
        if os.getenv('model_source', 'google') == 'google':
            if not os.getenv('GOOGLE_API_KEY'):
                raise MissingAPIKeyError(
                    'GOOGLE_API_KEY environment variable not set.'
                )
        else:
            if not os.getenv('TOOL_LLM_URL'):
                raise MissingAPIKeyError(
                    'TOOL_LLM_URL environment variable not set.'
                )
            if not os.getenv('TOOL_LLM_NAME'):
                raise MissingAPIKeyError(
                    'TOOL_LLM_NAME environment not variable not set.'
                )

        capabilities = AgentCapabilities(streaming=True, push_notifications=True)
        skill = AgentSkill(
            id='convert_currency',
            name='Currency Exchange Rates Tool',
            description='Helps with exchange values between various currencies',
            tags=['currency conversion', 'currency exchange'],
            examples=['What is exchange rate between USD and GBP?'],
        )
        agent_card = AgentCard(
            name='Currency Agent',
            description='Helps with exchange rates for currencies',
            url=f'http://{host}:{port}/',
            version='1.0.0',
            default_input_modes=CurrencyAgent.SUPPORTED_CONTENT_TYPES,
            default_output_modes=CurrencyAgent.SUPPORTED_CONTENT_TYPES,
            capabilities=capabilities,
            skills=[skill],
        )


        # --8<-- [start:DefaultRequestHandler]
        httpx_client = httpx.AsyncClient()
        push_config_store = InMemoryPushNotificationConfigStore()
        push_sender = BasePushNotificationSender(httpx_client=httpx_client,
                        config_store=push_config_store)
        request_handler = DefaultRequestHandler(
            agent_executor=CurrencyAgentExecutor(),
            task_store=InMemoryTaskStore(),
            push_config_store=push_config_store,
            push_sender= push_sender
        )
        server = A2AStarletteApplication(
            agent_card=agent_card, http_handler=request_handler
        )

        uvicorn.run(server.build(), host=host, port=port)
        # --8<-- [end:DefaultRequestHandler]

    except MissingAPIKeyError as e:
        logger.error(f'Error: {e}')
        sys.exit(1)
    except Exception as e:
        logger.error(f'An error occurred during server startup: {e}')
        sys.exit(1)


if __name__ == '__main__':
    main()

================
File: samples/python/agents/langgraph/app/agent_executor.py
================
import logging

from a2a.server.agent_execution import AgentExecutor, RequestContext
from a2a.server.events import EventQueue
from a2a.server.tasks import TaskUpdater
from a2a.types import (
    InternalError,
    InvalidParamsError,
    Part,
    TaskState,
    TextPart,
    UnsupportedOperationError,
)
from a2a.utils import (
    new_agent_text_message,
    new_task,
)
from a2a.utils.errors import ServerError

from app.agent import CurrencyAgent


logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class CurrencyAgentExecutor(AgentExecutor):
    """Currency Conversion AgentExecutor Example."""

    def __init__(self):
        self.agent = CurrencyAgent()

    async def execute(
        self,
        context: RequestContext,
        event_queue: EventQueue,
    ) -> None:
        error = self._validate_request(context)
        if error:
            raise ServerError(error=InvalidParamsError())

        query = context.get_user_input()
        task = context.current_task
        if not task:
            task = new_task(context.message)  # type: ignore
            await event_queue.enqueue_event(task)
        updater = TaskUpdater(event_queue, task.id, task.context_id)
        try:
            async for item in self.agent.stream(query, task.context_id):
                is_task_complete = item['is_task_complete']
                require_user_input = item['require_user_input']

                if not is_task_complete and not require_user_input:
                    await updater.update_status(
                        TaskState.working,
                        new_agent_text_message(
                            item['content'],
                            task.context_id,
                            task.id,
                        ),
                    )
                elif require_user_input:
                    await updater.update_status(
                        TaskState.input_required,
                        new_agent_text_message(
                            item['content'],
                            task.context_id,
                            task.id,
                        ),
                        final=True,
                    )
                    break
                else:
                    await updater.add_artifact(
                        [Part(root=TextPart(text=item['content']))],
                        name='conversion_result',
                    )
                    await updater.complete()
                    break

        except Exception as e:
            logger.error(f'An error occurred while streaming the response: {e}')
            raise ServerError(error=InternalError()) from e

    def _validate_request(self, context: RequestContext) -> bool:
        return False

    async def cancel(
        self, context: RequestContext, event_queue: EventQueue
    ) -> None:
        raise ServerError(error=UnsupportedOperationError())

================
File: samples/python/agents/langgraph/app/agent.py
================
import os

from collections.abc import AsyncIterable
from typing import Any, Literal

import httpx

from langchain_core.messages import AIMessage, ToolMessage
from langchain_core.tools import tool
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent
from pydantic import BaseModel


memory = MemorySaver()


@tool
def get_exchange_rate(
    currency_from: str = 'USD',
    currency_to: str = 'EUR',
    currency_date: str = 'latest',
):
    """Use this to get current exchange rate.

    Args:
        currency_from: The currency to convert from (e.g., "USD").
        currency_to: The currency to convert to (e.g., "EUR").
        currency_date: The date for the exchange rate or "latest". Defaults to
            "latest".

    Returns:
        A dictionary containing the exchange rate data, or an error message if
        the request fails.
    """
    try:
        response = httpx.get(
            f'https://api.frankfurter.app/{currency_date}',
            params={'from': currency_from, 'to': currency_to},
        )
        response.raise_for_status()

        data = response.json()
        if 'rates' not in data:
            return {'error': 'Invalid API response format.'}
        return data
    except httpx.HTTPError as e:
        return {'error': f'API request failed: {e}'}
    except ValueError:
        return {'error': 'Invalid JSON response from API.'}


class ResponseFormat(BaseModel):
    """Respond to the user in this format."""

    status: Literal['input_required', 'completed', 'error'] = 'input_required'
    message: str


class CurrencyAgent:
    """CurrencyAgent - a specialized assistant for currency convesions."""

    SYSTEM_INSTRUCTION = (
        'You are a specialized assistant for currency conversions. '
        "Your sole purpose is to use the 'get_exchange_rate' tool to answer questions about currency exchange rates. "
        'If the user asks about anything other than currency conversion or exchange rates, '
        'politely state that you cannot help with that topic and can only assist with currency-related queries. '
        'Do not attempt to answer unrelated questions or use tools for other purposes.'
    )

    FORMAT_INSTRUCTION = (
        'Set response status to input_required if the user needs to provide more information to complete the request.'
        'Set response status to error if there is an error while processing the request.'
        'Set response status to completed if the request is complete.'
    )

    def __init__(self):
        model_source = os.getenv('model_source', 'google')
        if model_source == 'google':
            self.model = ChatGoogleGenerativeAI(model='gemini-2.0-flash')
        else:
            self.model = ChatOpenAI(
                model=os.getenv('TOOL_LLM_NAME'),
                openai_api_key=os.getenv('API_KEY', 'EMPTY'),
                openai_api_base=os.getenv('TOOL_LLM_URL'),
                temperature=0,
            )
        self.tools = [get_exchange_rate]

        self.graph = create_react_agent(
            self.model,
            tools=self.tools,
            checkpointer=memory,
            prompt=self.SYSTEM_INSTRUCTION,
            response_format=(self.FORMAT_INSTRUCTION, ResponseFormat),
        )

    async def stream(self, query, context_id) -> AsyncIterable[dict[str, Any]]:
        inputs = {'messages': [('user', query)]}
        config = {'configurable': {'thread_id': context_id}}

        for item in self.graph.stream(inputs, config, stream_mode='values'):
            message = item['messages'][-1]
            if (
                isinstance(message, AIMessage)
                and message.tool_calls
                and len(message.tool_calls) > 0
            ):
                yield {
                    'is_task_complete': False,
                    'require_user_input': False,
                    'content': 'Looking up the exchange rates...',
                }
            elif isinstance(message, ToolMessage):
                yield {
                    'is_task_complete': False,
                    'require_user_input': False,
                    'content': 'Processing the exchange rates..',
                }

        yield self.get_agent_response(config)

    def get_agent_response(self, config):
        current_state = self.graph.get_state(config)
        structured_response = current_state.values.get('structured_response')
        if structured_response and isinstance(
            structured_response, ResponseFormat
        ):
            if structured_response.status == 'input_required':
                return {
                    'is_task_complete': False,
                    'require_user_input': True,
                    'content': structured_response.message,
                }
            if structured_response.status == 'error':
                return {
                    'is_task_complete': False,
                    'require_user_input': True,
                    'content': structured_response.message,
                }
            if structured_response.status == 'completed':
                return {
                    'is_task_complete': True,
                    'require_user_input': False,
                    'content': structured_response.message,
                }

        return {
            'is_task_complete': False,
            'require_user_input': True,
            'content': (
                'We are unable to process your request at the moment. '
                'Please try again.'
            ),
        }

    SUPPORTED_CONTENT_TYPES = ['text', 'text/plain']

================
File: samples/python/agents/langgraph/app/test_client.py
================
import logging

from typing import Any
from uuid import uuid4

import httpx

from a2a.client import A2ACardResolver, A2AClient
from a2a.types import (
    AgentCard,
    MessageSendParams,
    SendMessageRequest,
    SendStreamingMessageRequest,
)
from a2a.utils.constants import (
    AGENT_CARD_WELL_KNOWN_PATH,
    EXTENDED_AGENT_CARD_PATH,
)


async def main() -> None:
    # Configure logging to show INFO level messages
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)  # Get a logger instance

    # --8<-- [start:A2ACardResolver]

    base_url = 'http://localhost:10000'

    async with httpx.AsyncClient() as httpx_client:
        # Initialize A2ACardResolver
        resolver = A2ACardResolver(
            httpx_client=httpx_client,
            base_url=base_url,
            # agent_card_path uses default, extended_agent_card_path also uses default
        )
        # --8<-- [end:A2ACardResolver]

        # Fetch Public Agent Card and Initialize Client
        final_agent_card_to_use: AgentCard | None = None

        try:
            logger.info(
                f'Attempting to fetch public agent card from: {base_url}{AGENT_CARD_WELL_KNOWN_PATH}'
            )
            _public_card = (
                await resolver.get_agent_card()
            )  # Fetches from default public path
            logger.info('Successfully fetched public agent card:')
            logger.info(
                _public_card.model_dump_json(indent=2, exclude_none=True)
            )
            final_agent_card_to_use = _public_card
            logger.info(
                '\nUsing PUBLIC agent card for client initialization (default).'
            )

            if _public_card.supports_authenticated_extended_card:
                try:
                    logger.info(
                        '\nPublic card supports authenticated extended card. '
                        'Attempting to fetch from: '
                        f'{base_url}{EXTENDED_AGENT_CARD_PATH}'
                    )
                    auth_headers_dict = {
                        'Authorization': 'Bearer dummy-token-for-extended-card'
                    }
                    _extended_card = await resolver.get_agent_card(
                        relative_card_path=EXTENDED_AGENT_CARD_PATH,
                        http_kwargs={'headers': auth_headers_dict},
                    )
                    logger.info(
                        'Successfully fetched authenticated extended agent card:'
                    )
                    logger.info(
                        _extended_card.model_dump_json(
                            indent=2, exclude_none=True
                        )
                    )
                    final_agent_card_to_use = (
                        _extended_card  # Update to use the extended card
                    )
                    logger.info(
                        '\nUsing AUTHENTICATED EXTENDED agent card for client '
                        'initialization.'
                    )
                except Exception as e_extended:
                    logger.warning(
                        f'Failed to fetch extended agent card: {e_extended}. '
                        'Will proceed with public card.',
                        exc_info=True,
                    )
            elif (
                _public_card
            ):  # supports_authenticated_extended_card is False or None
                logger.info(
                    '\nPublic card does not indicate support for an extended card. Using public card.'
                )

        except Exception as e:
            logger.error(
                f'Critical error fetching public agent card: {e}', exc_info=True
            )
            raise RuntimeError(
                'Failed to fetch the public agent card. Cannot continue.'
            ) from e

        # --8<-- [start:send_message]
        client = A2AClient(
            httpx_client=httpx_client, agent_card=final_agent_card_to_use
        )
        logger.info('A2AClient initialized.')

        send_message_payload: dict[str, Any] = {
            'message': {
                'role': 'user',
                'parts': [
                    {'kind': 'text', 'text': 'how much is 10 USD in INR?'}
                ],
                'message_id': uuid4().hex,
            },
        }
        request = SendMessageRequest(
            id=str(uuid4()), params=MessageSendParams(**send_message_payload)
        )

        response = await client.send_message(request)
        print(response.model_dump(mode='json', exclude_none=True))
        # --8<-- [end:send_message]

        # --8<-- [start:Multiturn]
        send_message_payload_multiturn: dict[str, Any] = {
            'message': {
                'role': 'user',
                'parts': [
                    {
                        'kind': 'text',
                        'text': 'How much is the exchange rate for 1 USD?',
                    }
                ],
                'message_id': uuid4().hex,
            },
        }
        request = SendMessageRequest(
            id=str(uuid4()),
            params=MessageSendParams(**send_message_payload_multiturn),
        )

        response = await client.send_message(request)
        print(response.model_dump(mode='json', exclude_none=True))

        task_id = response.root.result.id
        context_id = response.root.result.context_id

        second_send_message_payload_multiturn: dict[str, Any] = {
            'message': {
                'role': 'user',
                'parts': [{'kind': 'text', 'text': 'CAD'}],
                'message_id': uuid4().hex,
                'task_id': task_id,
                'context_id': context_id,
            },
        }

        second_request = SendMessageRequest(
            id=str(uuid4()),
            params=MessageSendParams(**second_send_message_payload_multiturn),
        )

        second_response = await client.send_message(second_request)
        print(second_response.model_dump(mode='json', exclude_none=True))
        # --8<-- [end:Multiturn]

        # --8<-- [start:send_message_streaming]

        streaming_request = SendStreamingMessageRequest(
            id=str(uuid4()), params=MessageSendParams(**send_message_payload)
        )

        stream_response = client.send_message_streaming(streaming_request)

        async for chunk in stream_response:
            print(chunk.model_dump(mode='json', exclude_none=True))
        # --8<-- [end:send_message_streaming]


if __name__ == '__main__':
    import asyncio

    asyncio.run(main())

================
File: samples/python/agents/langgraph/pyproject.toml
================
[project]
name = "a2a-sample-agent-langgraph"
version = "0.1.0"
description = "Sample LangGraph currency agent with A2A Protocol"
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    "a2a-sdk==0.3.0",
    "click>=8.1.8",
    "httpx>=0.28.1",
    "langchain-google-genai>=2.0.10",
    "langgraph>=0.3.18",
    "langchain-openai>=0.1.0",
    "pydantic>=2.10.6",
    "python-dotenv>=1.1.0",
    "uvicorn>=0.34.2",
    "sse-starlette>=2.3.6",
    "starlette>=0.46.2",
]

[tool.hatch.build.targets.wheel]
packages = ["app"]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

================
File: samples/python/agents/langgraph/README.md
================
# LangGraph Currency Agent with A2A Protocol

This sample demonstrates a currency conversion agent built with [LangGraph](https://langchain-ai.github.io/langgraph/) and exposed through the A2A protocol. It showcases conversational interactions with support for multi-turn dialogue and streaming responses.

## How It Works

This agent uses LangGraph with LLM (for example Google Gemini..) to provide currency exchange information through a ReAct agent pattern. The A2A protocol enables standardized interaction with the agent, allowing clients to send requests and receive real-time updates.

```mermaid
sequenceDiagram
    participant Client as A2A Client
    participant Server as A2A Server
    participant Agent as LangGraph Agent
    participant API as Frankfurter API

    Client->>Server: Send task with currency query
    Server->>Agent: Forward query to currency agent

    alt Complete Information
        Agent->>API: Call get_exchange_rate tool
        API->>Agent: Return exchange rate data
        Agent->>Server: Process data & return result
        Server->>Client: Respond with currency information
    else Incomplete Information
        Agent->>Server: Request additional input
        Server->>Client: Set state to "input-required"
        Client->>Server: Send additional information
        Server->>Agent: Forward additional info
        Agent->>API: Call get_exchange_rate tool
        API->>Agent: Return exchange rate data
        Agent->>Server: Process data & return result
        Server->>Client: Respond with currency information
    end

    alt With Streaming
        Note over Client,Server: Real-time status updates
        Server->>Client: "Looking up exchange rates..."
        Server->>Client: "Processing exchange rates..."
        Server->>Client: Final result
    end
```

## Key Features

- **Multi-turn Conversations**: Agent can request additional information when needed
- **Real-time Streaming**: Provides status updates during processing
- **Push Notifications**: Support for webhook-based notifications
- **Conversational Memory**: Maintains context across interactions
- **Currency Exchange Tool**: Integrates with Frankfurter API for real-time rates

## Prerequisites

- Python 3.12 or higher
- [UV](https://docs.astral.sh/uv/)
- Access to an LLM and API Key

## Setup & Running

1. Navigate to the samples directory:

   ```bash
   cd samples/python/agents/langgraph
   ```

2. Create an environment file with your API key:

   ```bash
   If you're using a Google Gemini model (gemini-pro, etc.):
   echo "GOOGLE_API_KEY=your_api_key_here" > .env
  
   
   If you're using OpenAI or any compatible API (e.g., local LLM via Ollama, LM Studio, etc.):

   echo "API_KEY=your_api_key_here" > .env  (not neccessary if have no api key)
   echo "TOOL_LLM_URL=your_llm_url" > .env
   echo "TOOL_LLM_NAME=your_llm_name" > .env

   ```

3. Run the agent:

   ```bash
   # Basic run on default port 10000
   uv run app

   # On custom host/port
   uv run app --host 0.0.0.0 --port 8080
   ```

4. In a separate terminal, run the test client:

   ```bash
   uv run app/test_client.py
   ```

## Build Container Image

Agent can also be built using a container file.

1. Navigate to the `samples/python/agents/langgraph` directory:

  ```bash
  cd samples/python/agents/langgraph
  ```

2. Build the container file

    ```bash
    podman build . -t langgraph-a2a-server
    ```

> [!Tip]  
> Podman is a drop-in replacement for `docker` which can also be used in these commands.

3. Run your container

    ```bash
    podman run -p 10000:10000 -e GOOGLE_API_KEY=your_api_key_here langgraph-a2a-server
    ```

4. Run A2A client (follow step 5 from the section above)

> [!Important]
> * **Access URL:** You must access the A2A client through the URL `0.0.0.0:10000`. Using `localhost` will not work.
> * **Hostname Override:** If you're deploying to an environment where the hostname is defined differently outside the container, use the `HOST_OVERRIDE` environment variable to set the expected hostname on the Agent Card. This ensures proper communication with your client application.

## Technical Implementation

- **LangGraph ReAct Agent**: Uses the ReAct pattern for reasoning and tool usage
- **Streaming Support**: Provides incremental updates during processing
- **Checkpoint Memory**: Maintains conversation state between turns
- **Push Notification System**: Webhook-based updates with JWK authentication
- **A2A Protocol Integration**: Full compliance with A2A specifications

## Limitations

- Only supports text-based input/output (no multi-modal support)
- Uses Frankfurter API which has limited currency options
- Memory is session-based and not persisted between server restarts

## Examples

**Synchronous request**

Request:

```
POST http://localhost:10000
Content-Type: application/json

{
    "id": "12113c25-b752-473f-977e-c9ad33cf4f56",
    "jsonrpc": "2.0",
    "method": "message/send",
    "params": {
        "message": {
            "kind": "message",
            "messageId": "120ec73f93024993becf954d03a672bc",
            "parts": [
                {
                    "kind": "text",
                    "text": "how much is 10 USD in INR?"
                }
            ],
            "role": "user"
        }
    }
}
```

Response:

```
{
    "id": "12113c25-b752-473f-977e-c9ad33cf4f56",
    "jsonrpc": "2.0",
    "result": {
        "artifacts": [
            {
                "artifactId": "08373241-a745-4abe-a78b-9ca60882bcc6",
                "name": "conversion_result",
                "parts": [
                    {
                        "kind": "text",
                        "text": "10 USD is 856.2 INR."
                    }
                ]
            }
        ],
        "contextId": "e329f200-eaf4-4ae9-a8ef-a33cf9485367",
        "history": [
            {
                "contextId": "e329f200-eaf4-4ae9-a8ef-a33cf9485367",
                "kind": "message",
                "messageId": "120ec73f93024993becf954d03a672bc",
                "parts": [
                    {
                        "kind": "text",
                        "text": "how much is 10 USD in INR?"
                    }
                ],
                "role": "user",
                "taskId": "58124b63-dd3b-46b8-bf1d-1cc1aefd1c8f"
            },
            {
                "contextId": "e329f200-eaf4-4ae9-a8ef-a33cf9485367",
                "kind": "message",
                "messageId": "d8b4d7de-709f-40f7-ae0c-fd6ee398a2bf",
                "parts": [
                    {
                        "kind": "text",
                        "text": "Looking up the exchange rates..."
                    }
                ],
                "role": "agent",
                "taskId": "58124b63-dd3b-46b8-bf1d-1cc1aefd1c8f"
            },
            {
                "contextId": "e329f200-eaf4-4ae9-a8ef-a33cf9485367",
                "kind": "message",
                "messageId": "ee0cb3b6-c3d6-4316-8d58-315c437a2a77",
                "parts": [
                    {
                        "kind": "text",
                        "text": "Processing the exchange rates.."
                    }
                ],
                "role": "agent",
                "taskId": "58124b63-dd3b-46b8-bf1d-1cc1aefd1c8f"
            }
        ],
        "id": "58124b63-dd3b-46b8-bf1d-1cc1aefd1c8f",
        "kind": "task",
        "status": {
            "state": "completed"
        }
    }
}
```

**Multi-turn example**

Request - Seq 1:

```
POST http://localhost:10000
Content-Type: application/json

{
    "id": "27be771b-708f-43b8-8366-968966d07ec0",
    "jsonrpc": "2.0",
    "method": "message/send",
    "params": {
        "message": {
            "kind": "message",
            "messageId": "296eafc9233142bd98279e4055165f12",
            "parts": [
                {
                    "kind": "text",
                    "text": "How much is the exchange rate for 1 USD?"
                }
            ],
            "role": "user"
        }
    }
}
```

Response - Seq 2:

```
{
    "id": "27be771b-708f-43b8-8366-968966d07ec0",
    "jsonrpc": "2.0",
    "result": {
        "contextId": "a7cc0bef-17b5-41fc-9379-40b99f46a101",
        "history": [
            {
                "contextId": "a7cc0bef-17b5-41fc-9379-40b99f46a101",
                "kind": "message",
                "messageId": "296eafc9233142bd98279e4055165f12",
                "parts": [
                    {
                        "kind": "text",
                        "text": "How much is the exchange rate for 1 USD?"
                    }
                ],
                "role": "user",
                "taskId": "9d94c2d4-06e4-40e1-876b-22f5a2666e61"
            }
        ],
        "id": "9d94c2d4-06e4-40e1-876b-22f5a2666e61",
        "kind": "task",
        "status": {
            "message": {
                "contextId": "a7cc0bef-17b5-41fc-9379-40b99f46a101",
                "kind": "message",
                "messageId": "f0f5f3ff-335c-4e77-9b4a-01ff3908e7be",
                "parts": [
                    {
                        "kind": "text",
                        "text": "Please specify which currency you would like to convert to."
                    }
                ],
                "role": "agent",
                "taskId": "9d94c2d4-06e4-40e1-876b-22f5a2666e61"
            },
            "state": "input-required"
        }
    }
}
```

Request - Seq 3:

```
POST http://localhost:10000
Content-Type: application/json

{
    "id": "b88d818d-1192-42be-b4eb-3ee6b96a7e35",
    "jsonrpc": "2.0",
    "method": "message/send",
    "params": {
        "message": {
            "contextId": "a7cc0bef-17b5-41fc-9379-40b99f46a101",
            "kind": "message",
            "messageId": "70371e1f231f4597b65ccdf534930ca9",
            "parts": [
                {
                    "kind": "text",
                    "text": "CAD"
                }
            ],
            "role": "user",
            "taskId": "9d94c2d4-06e4-40e1-876b-22f5a2666e61"
        }
    }
}
```

Response - Seq 4:

```
{
    "id": "b88d818d-1192-42be-b4eb-3ee6b96a7e35",
    "jsonrpc": "2.0",
    "result": {
        "artifacts": [
            {
                "artifactId": "08373241-a745-4abe-a78b-9ca60882bcc6",
                "name": "conversion_result",
                "parts": [
                    {
                        "kind": "text",
                        "text": "The exchange rate for 1 USD to CAD is 1.3739."
                    }
                ]
            }
        ],
        "contextId": "a7cc0bef-17b5-41fc-9379-40b99f46a101",
        "history": [
            {
                "contextId": "a7cc0bef-17b5-41fc-9379-40b99f46a101",
                "kind": "message",
                "messageId": "296eafc9233142bd98279e4055165f12",
                "parts": [
                    {
                        "kind": "text",
                        "text": "How much is the exchange rate for 1 USD?"
                    }
                ],
                "role": "user",
                "taskId": "9d94c2d4-06e4-40e1-876b-22f5a2666e61"
            },
            {
                "contextId": "a7cc0bef-17b5-41fc-9379-40b99f46a101",
                "kind": "message",
                "messageId": "f0f5f3ff-335c-4e77-9b4a-01ff3908e7be",
                "parts": [
                    {
                        "kind": "text",
                        "text": "Please specify which currency you would like to convert to."
                    }
                ],
                "role": "agent",
                "taskId": "9d94c2d4-06e4-40e1-876b-22f5a2666e61"
            },
            {
                "contextId": "a7cc0bef-17b5-41fc-9379-40b99f46a101",
                "kind": "message",
                "messageId": "70371e1f231f4597b65ccdf534930ca9",
                "parts": [
                    {
                        "kind": "text",
                        "text": "CAD"
                    }
                ],
                "role": "user",
                "taskId": "9d94c2d4-06e4-40e1-876b-22f5a2666e61"
            },
            {
                "contextId": "a7cc0bef-17b5-41fc-9379-40b99f46a101",
                "kind": "message",
                "messageId": "0eb4f200-a8cd-4d34-94f8-4d223eb1b2c0",
                "parts": [
                    {
                        "kind": "text",
                        "text": "Looking up the exchange rates..."
                    }
                ],
                "role": "agent",
                "taskId": "9d94c2d4-06e4-40e1-876b-22f5a2666e61"
            },
            {
                "contextId": "a7cc0bef-17b5-41fc-9379-40b99f46a101",
                "kind": "message",
                "messageId": "41c7c03a-a772-4dc8-a868-e8c7b7defc91",
                "parts": [
                    {
                        "kind": "text",
                        "text": "Processing the exchange rates.."
                    }
                ],
                "role": "agent",
                "taskId": "9d94c2d4-06e4-40e1-876b-22f5a2666e61"
            }
        ],
        "id": "9d94c2d4-06e4-40e1-876b-22f5a2666e61",
        "kind": "task",
        "status": {
            "state": "completed"
        }
    }
}
```

**Streaming example**

Request:

```
{
    "id": "6d12d159-ec67-46e6-8d43-18480ce7f6ca",
    "jsonrpc": "2.0",
    "method": "message/stream",
    "params": {
        "message": {
            "kind": "message",
            "messageId": "2f9538ef0984471aa0d5179ce3c67a28",
            "parts": [
                {
                    "kind": "text",
                    "text": "how much is 10 USD in INR?"
                }
            ],
            "role": "user"
        }
    }
}
```

Response:

```
data: {"id":"6d12d159-ec67-46e6-8d43-18480ce7f6ca","jsonrpc":"2.0","result":{"contextId":"cd09e369-340a-4563-bca4-e5f2e0b9ff81","history":[{"contextId":"cd09e369-340a-4563-bca4-e5f2e0b9ff81","kind":"message","messageId":"2f9538ef0984471aa0d5179ce3c67a28","parts":[{"kind":"text","text":"how much is 10 USD in INR?"}],"role":"user","taskId":"423a2569-f272-4d75-a4d1-cdc6682188e5"}],"id":"423a2569-f272-4d75-a4d1-cdc6682188e5","kind":"task","status":{"state":"submitted"}}}

data: {"id":"6d12d159-ec67-46e6-8d43-18480ce7f6ca","jsonrpc":"2.0","result":{"contextId":"cd09e369-340a-4563-bca4-e5f2e0b9ff81","final":false,"kind":"status-update","status":{"message":{"contextId":"cd09e369-340a-4563-bca4-e5f2e0b9ff81","kind":"message","messageId":"1854a825-c64f-4f30-96f2-c8aa558b83f9","parts":[{"kind":"text","text":"Looking up the exchange rates..."}],"role":"agent","taskId":"423a2569-f272-4d75-a4d1-cdc6682188e5"},"state":"working"},"taskId":"423a2569-f272-4d75-a4d1-cdc6682188e5"}}

data: {"id":"6d12d159-ec67-46e6-8d43-18480ce7f6ca","jsonrpc":"2.0","result":{"contextId":"cd09e369-340a-4563-bca4-e5f2e0b9ff81","final":false,"kind":"status-update","status":{"message":{"contextId":"cd09e369-340a-4563-bca4-e5f2e0b9ff81","kind":"message","messageId":"e72127a6-4830-4320-bf23-235ac79b9a13","parts":[{"kind":"text","text":"Processing the exchange rates.."}],"role":"agent","taskId":"423a2569-f272-4d75-a4d1-cdc6682188e5"},"state":"working"},"taskId":"423a2569-f272-4d75-a4d1-cdc6682188e5"}}

data: {"id":"6d12d159-ec67-46e6-8d43-18480ce7f6ca","jsonrpc":"2.0","result":{"artifact":{"artifactId":"08373241-a745-4abe-a78b-9ca60882bcc6","name":"conversion_result","parts":[{"kind":"text","text":"10 USD is 856.2 INR."}]},"contextId":"cd09e369-340a-4563-bca4-e5f2e0b9ff81","kind":"artifact-update","taskId":"423a2569-f272-4d75-a4d1-cdc6682188e5"}}

data: {"id":"6d12d159-ec67-46e6-8d43-18480ce7f6ca","jsonrpc":"2.0","result":{"contextId":"cd09e369-340a-4563-bca4-e5f2e0b9ff81","final":true,"kind":"status-update","status":{"state":"completed"},"taskId":"423a2569-f272-4d75-a4d1-cdc6682188e5"}}
```

## Learn More

- [A2A Protocol Documentation](https://a2a-protocol.org/)
- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)
- [Frankfurter API](https://www.frankfurter.app/docs/)
- [Google Gemini API](https://ai.google.dev/gemini-api)


## Disclaimer
Important: The sample code provided is for demonstration purposes and illustrates the mechanics of the Agent-to-Agent (A2A) protocol. When building production applications, it is critical to treat any agent operating outside of your direct control as a potentially untrusted entity.

All data received from an external agentincluding but not limited to its AgentCard, messages, artifacts, and task statusesshould be handled as untrusted input. For example, a malicious agent could provide an AgentCard containing crafted data in its fields (e.g., description, name, skills.description). If this data is used without sanitization to construct prompts for a Large Language Model (LLM), it could expose your application to prompt injection attacks.  Failure to properly validate and sanitize this data before use can introduce security vulnerabilities into your application.

Developers are responsible for implementing appropriate security measures, such as input validation and secure handling of credentials to protect their systems and users.




================================================================
End of Codebase
================================================================
