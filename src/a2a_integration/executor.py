"""
Deprecated LangGraph A2A Agent Executor.

This module provides a clean integration between LangGraph and A2A protocol,
removing unnecessary abstractions and over-engineering.
"""
import asyncio
import json
from typing import Any, Callable, cast

import structlog
from a2a.server.agent_execution import AgentExecutor, RequestContext
from a2a.server.events import EventQueue
from a2a.server.tasks import TaskUpdater
from a2a.types import (
    DataPart,
    Part,
    TaskState,
    TextPart,
)
from a2a.utils import (
    get_data_parts,
    new_agent_parts_message,
    new_agent_text_message,
)
from langchain_core.messages import (
    message_to_dict,
    messages_to_dict,
)
from langgraph.graph.state import CompiledStateGraph

from src.a2a_integration.models import LangGraphExecutorConfig

logger = structlog.get_logger(__name__)


class LangGraphAgentExecutor(AgentExecutor):
    """
    A2A Agent Executor for LangGraph.

    This executor provides a clean bridge between LangGraph state machines
    and the A2A protocol, focusing on simplicity and SDK compliance.
    """

    def __init__(
        self,
        graph: CompiledStateGraph | None = None,
        result_extractor: Callable[[dict[str, Any]], str] | None = None,
        config: LangGraphExecutorConfig | None = None,
    ):
        """
        Initialize the LangGraph A2A Executor.

        Simplified to work directly with create_react_agent graphs.

        Args:
            graph: The compiled LangGraph from create_react_agent
            result_extractor: Optional function to extract and structure results for A2A
            config: Configuration for the executor
        """
        self.graph = graph
        self.config = config or LangGraphExecutorConfig()
        self.result_extractor = self._get_result_extractor(result_extractor)
        self._active_tasks: dict[str, asyncio.Task] = {}

        if graph:
            logger.info("âœ… LangGraphAgentExecutor: Graph ê¸°ë°˜ ì´ˆê¸°í™”")
        else:
            logger.warning("âš ï¸ LangGraphAgentExecutor: Graphê°€ ì œê³µë˜ì§€ ì•ŠìŒ")

    def _get_result_extractor(self, custom_extractor: Callable[[dict[str, Any]], str] | None) -> Callable[[dict[str, Any]], str]:
        """Get the appropriate result extractor based on agent type."""
        if custom_extractor:
            return custom_extractor

        return self._default_extract_text

    async def _send_result(
        self,
        updater: TaskUpdater,
        result: Any,
        event_queue: EventQueue,
        complete_task: bool = True,
    ) -> None:
        """Send result as TextPart, DataPart, or both based on content type."""
        logger.info(f"ğŸ”§ _send_result called with result type: {type(result)}")
        logger.info(f"ğŸ”§ _send_result - result is dict: {isinstance(result, dict)}")
        logger.info(f"ğŸ”§ _send_result - result keys: {list(result.keys()) if isinstance(result, dict) else 'N/A'}")

        if isinstance(result, dict) and result:
            # For structured data, send both text description and data
            parts = []

            # resultê°€ ì´ë¯¸ result_extractorì—ì„œ ì¶”ì¶œëœ ê²½ìš°ë¥¼ íŒë‹¨
            # extracted_resultì¸ ê²½ìš° 'success', 'collected_data' ë“±ì˜ í‚¤ë¥¼ í¬í•¨
            is_already_extracted = any(key in result for key in ['success', 'collected_data', 'analysis_result', 'trading_result'])

            if not is_already_extracted and self.result_extractor:
                try:
                    logger.info(f"ğŸ”§ Calling result_extractor with result: {type(result)}")
                    extracted = self.result_extractor(result)
                    logger.info(f"ğŸ”§ result_extractor returned: {type(extracted)}")

                    # result_extractorê°€ dictë¥¼ ë°˜í™˜í•˜ë©´ DataPartìš©ì´ê³ , strì´ë©´ TextPartìš©
                    if isinstance(extracted, str) and extracted:
                        logger.info(f"ğŸ”§ result_extractor returned text: {extracted[:100]}...")
                        parts.append(Part(root=TextPart(text=extracted)))
                        logger.info("âœ… Added TextPart to response")
                    elif isinstance(extracted, dict):
                        logger.info(f"ğŸ”§ result_extractor returned dict: {list(extracted.keys())}")
                        # dictë¥¼ ë°˜í™˜í•œ ê²½ìš°, ì´ê²ƒì„ resultë¡œ ì‚¬ìš©
                        result = extracted
                        logger.info("ğŸ”§ Using extracted dict as result")
                    else:
                        logger.info(f"ğŸ”§ result_extractor returned unexpected type: {type(extracted)}")
                except Exception as e:
                    logger.error(f"âŒ result_extractor failed: {e}")
                    pass
            else:
                logger.info("ğŸ”§ Result is already extracted or no result_extractor, using as-is")

            # Add structured data
            # Clean the result to ensure it's JSON serializable
            logger.info("ğŸ”§ Cleaning result for JSON serialization...")
            clean_result = self._clean_for_json(result)
            logger.info(f"ğŸ”§ clean_result: {clean_result}")
            if clean_result:
                parts.append(Part(root=DataPart(data=clean_result)))
                logger.info("âœ… Added DataPart to response")
            else:
                logger.warning("âš ï¸ clean_result is empty, no DataPart added")

            # Create and enqueue message
            logger.info(f"ğŸ”§ Total parts created: {len(parts)}")
            if parts:
                message = new_agent_parts_message(parts)
                logger.info(f"ğŸ”§ Enqueuing message with {len(parts)} parts")
                await event_queue.enqueue_event(message)
                logger.info("âœ… Message enqueued successfully")
            else:
                logger.warning("âš ï¸ No parts created, no message sent")
        elif result:
            # For simple text results
            text = (
                self.result_extractor(result)
                if callable(self.result_extractor)
                else str(result)
            )
            if text:
                message = new_agent_text_message(text)
                await event_queue.enqueue_event(message)
                logger.info("âœ… Text message enqueued successfully")

        # Handle task completion based on complete_task parameter
        if complete_task:
            await updater.complete()
            logger.info("âœ… Task completed via _send_result")

    def _clean_for_json(self, obj: Any) -> Any:
        """Clean object to be JSON serializable."""
        # LangChain ë©”ì‹œì§€ ê°ì²´ ì²˜ë¦¬
        if hasattr(obj, '__class__') and 'langchain_core.messages' in str(obj.__class__):
            try:
                return message_to_dict(obj)
            except Exception:
                return str(obj)
        elif isinstance(obj, dict):
            return {k: self._clean_for_json(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            # LangChain ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬
            if obj and hasattr(obj[0], '__class__') and 'langchain_core.messages' in str(obj[0].__class__):
                try:
                    return messages_to_dict(obj)
                except Exception:
                    return [self._clean_for_json(item) for item in obj]
            else:
                return [self._clean_for_json(item) for item in obj]
        elif isinstance(obj, (str, int, float, bool, type(None))):
            return obj
        else:
            return str(obj)

    def _default_extract_text(self, result: dict[str, Any]) -> str:
        """Default method to extract text from LangGraph output."""
        if isinstance(result, str):
            return result

        # Try common patterns
        for key in ["response", "output", "answer", "result", "messages", "message"]:
            if key in result:
                value = result[key]
                if isinstance(value, str):
                    return value
                elif isinstance(value, list) and value:
                    # Handle message list
                    last_msg = value[-1]
                    if isinstance(last_msg, dict):
                        return str(last_msg.get("content", last_msg))
                    return str(last_msg)
                elif isinstance(value, dict):
                    return str(value.get("content", value))

        # Fallback to JSON
        return json.dumps(result, ensure_ascii=False, indent=2)

    async def _extract_final_message(self, result: dict[str, Any], collected_messages: list[str] | None = None) -> str:
        """
        ìµœì¢… ë©”ì‹œì§€ ì¶”ì¶œì„ result_extractorì—ê²Œ ìœ„ì„í•©ë‹ˆë‹¤.

        Args:
            result: LangGraph ì‹¤í–‰ ê²°ê³¼
            collected_messages: ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ìˆ˜ì§‘ëœ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ (ì˜µì…˜)

        Returns:
            ì¶”ì¶œëœ ìµœì¢… ë©”ì‹œì§€ ë¬¸ìì—´
        """
        try:
            logger.debug("=== ìµœì¢… ë©”ì‹œì§€ ì¶”ì¶œ ì‹œì‘ ===")

            # 1. result_extractorê°€ ìˆìœ¼ë©´ ì‚¬ìš© (ì—ì´ì „íŠ¸ë³„ íŠ¹í™” ë¡œì§)
            if self.result_extractor:
                extracted = self.result_extractor(result)
                if extracted and extracted.strip():
                    logger.info(f"âœ… Result extractor ì‚¬ìš©: {extracted[:100]}...")
                    return extracted

            # 2. ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ìˆ˜ì§‘ëœ ë©”ì‹œì§€ ì‚¬ìš©
            if collected_messages:
                collected_text = "".join(collected_messages)
                if collected_text.strip():
                    logger.info(f"âœ… ìŠ¤íŠ¸ë¦¬ë° ë©”ì‹œì§€ ì‚¬ìš©: {collected_text[:100]}...")
                    return collected_text

            # 3. ê¸°ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (í´ë°±)
            text_result = self._default_extract_text(result)
            if text_result and text_result.strip():
                logger.info(f"âœ… ê¸°ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ: {text_result[:100]}...")
                return text_result

            # 4. ìµœì¢… í´ë°±
            logger.warning("âš ï¸ ë©”ì‹œì§€ ì¶”ì¶œ ì‹¤íŒ¨, í´ë°± ë©”ì‹œì§€ ì‚¬ìš©")
            return "ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."

        except Exception as e:
            logger.error(f"âŒ ë©”ì‹œì§€ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            if collected_messages:
                return "".join(collected_messages)
            return "ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."

    async def execute(self, context: RequestContext, event_queue: EventQueue) -> None:
        """
        A2A ìš”ì²­ì„ ì²˜ë¦¬í•˜ê³  LangGraph ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

        Args:
            context: A2A ìš”ì²­ ì»¨í…ìŠ¤íŠ¸
            event_queue: ì´ë²¤íŠ¸ í
        """
        try:
            logger.info("Starting A2A agent execution")
            logger.info(f"High-level interface: {self.use_high_level_interface}")

            # EventQueue ìƒíƒœ í™•ì¸
            logger.info(f"EventQueue received: {event_queue}")
            logger.info(f"EventQueue type: {type(event_queue)}")
            if hasattr(event_queue, "_closed"):
                logger.info(f"EventQueue._closed: {event_queue._closed}")
            if hasattr(event_queue, "closed"):
                logger.info(f"EventQueue.closed: {event_queue.closed}")

            # ì…ë ¥ ì²˜ë¦¬
            processed_input = await self._process_input(context)
            logger.info(f"Processed input: {processed_input}")

            task_id = cast(str, context.task_id)
            context_id = getattr(context, "context_id", task_id)

            logger.info(f"Using context task_id: {task_id}")
            logger.info(f"Creating TaskUpdater for task_id: {task_id}")
            updater = TaskUpdater(
                event_queue=event_queue,
                task_id=task_id,
                context_id=str(context_id),
            )
            logger.info("TaskUpdater created successfully")

            # Graph ê¸°ë°˜ ì‹¤í–‰ (create_react_agentì—ì„œ ìƒì„±ëœ graph ì‚¬ìš©)
            logger.info("ğŸ”„ Using graph-based execution")

            # RequestContext.configuration.blocking í™•ì¸
            is_blocking = False  # ê¸°ë³¸ì ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ ì‚¬ìš©
            if hasattr(context, "request") and context.request:
                if (
                    hasattr(context.request, "configuration")
                    and context.request.configuration
                ):
                    is_blocking = getattr(
                        context.request.configuration, "blocking", False  # ê¸°ë³¸ê°’ì€ False (ìŠ¤íŠ¸ë¦¬ë°)
                    )

            logger.info(
                f"Execution mode - blocking: {is_blocking}, config.enable_streaming: {self.config.enable_streaming}"
            )

            # ì‘ì—… ì‹œì‘ ìƒíƒœ ì—…ë°ì´íŠ¸ (ìƒíƒœë§Œ ë³€ê²½, ë©”ì‹œì§€ ì—†ì´)
            try:
                await updater.update_status(TaskState.working)
                logger.info("Initial working status update sent")
            except Exception as e:
                logger.warning(f"Failed to send initial status update: {e}")

            # EventQueue ìƒíƒœ ì¬í™•ì¸
            if hasattr(event_queue, "_closed"):
                logger.info(
                    f"After TaskUpdater creation - EventQueue._closed: {event_queue._closed}"
                )

            # ì™„ë£Œ ìƒíƒœë¥¼ ì¶”ì í•˜ëŠ” í”Œë˜ê·¸
            is_completed = False
            collected_messages = []

            # Blocking ëª¨ë“œì´ê±°ë‚˜ ìŠ¤íŠ¸ë¦¬ë°ì´ ë¹„í™œì„±í™”ëœ ê²½ìš° ë™ê¸° ì‹¤í–‰
            if is_blocking or not self.config.enable_streaming:
                logger.info("Using synchronous execution (ainvoke)")

                try:
                    logger.info(f"ğŸš€ Starting ainvoke with processed_input: {type(processed_input)}")
                    logger.info(f"ğŸš€ Graph type: {type(self.graph)}")
                    logger.info(f"ğŸš€ Config: configurable thread_id: {context_id}")

                    # TODO: ì´ ë¶€ë¶„ì„ ê° í˜¸ì¶œ í•¨ìˆ˜ë“¤ë¡œ ë°”ê¿”ì•¼í•¨
                    result = await self.graph.ainvoke(
                        processed_input,
                        config={"configurable": {"thread_id": context_id}},
                        stream_mode="messages",
                    )

                    logger.info(f"âœ… ainvoke completed successfully, result type: {type(result)}")
                    logger.info(f"âœ… Result is None: {result is None}")
                    if result:
                        logger.info(f"âœ… Result keys: {list(result.keys()) if hasattr(result, 'keys') else 'Not a dict'}")

                    # ainvoke ê²°ê³¼ê°€ Noneì¸ ê²½ìš° ìµœì¢… ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
                    if result is None:
                        logger.info("ainvoke returned None, getting final state...")
                        state_snapshot = await self.graph.aget_state(
                            config={"configurable": {"thread_id": context_id}}
                        )
                        result = state_snapshot.values if state_snapshot else None
                        logger.info(f"Got final state: {type(result)}")

                    # result_extractorë¡œ DataPart ì¶”ì¶œ ì‹œë„
                    try:
                        logger.info("Attempting to extract result using result_extractor...")
                        logger.info(f"result type before extractor: {type(result)}")
                        logger.info(f"result keys: {result.keys() if hasattr(result, 'keys') else 'Not a dict'}")
                        logger.info(f"result content (first 200 chars): {str(result)[:200] if result else 'None'}")
                        extracted_result = self.result_extractor(result)
                        logger.info(f"result_extractor returned type: {type(extracted_result)}")

                        if isinstance(extracted_result, dict) and extracted_result:
                            logger.info("result_extractor returned dict, sending DataPart")
                            await self._send_result(updater, extracted_result, event_queue)

                            # DataPart ì „ì†¡ í›„ í´ë¼ì´ì–¸íŠ¸ê°€ ìˆ˜ì‹ í•  ì‹œê°„ì„ í™•ë³´
                            logger.info("â³ DataPart ì „ì†¡ í›„ í´ë¼ì´ì–¸íŠ¸ ìˆ˜ì‹  ëŒ€ê¸° ì¤‘...")
                            await asyncio.sleep(0.5)  # 500ms ëŒ€ê¸°
                            logger.info("âœ… í´ë¼ì´ì–¸íŠ¸ ìˆ˜ì‹  ëŒ€ê¸° ì™„ë£Œ")

                            await updater.complete()
                            logger.info("âœ… [Sync Mode] Task completed with DataPart response")
                            return
                        elif isinstance(extracted_result, str) and extracted_result:
                            logger.info("ğŸ“ [Sync Mode] result_extractor returned text")
                            # í…ìŠ¤íŠ¸ê°€ ë°˜í™˜ëœ ê²½ìš° TextPartë¡œ ì „ì†¡
                            await updater.update_status(
                                TaskState.completed,
                                new_agent_text_message(extracted_result, context_id, task_id),
                                final=True,
                            )
                            logger.info("âœ… [Sync Mode] Task completed with text response")
                            return
                        else:
                            logger.info(f"â„¹ï¸ [Sync Mode] result_extractor returned {type(extracted_result)}, falling back to text extraction")
                    except Exception as e:
                        logger.debug(f"[Sync Mode] result_extractor failed or not applicable: {e}")

                    # í´ë°±: ê¸°ì¡´ í…ìŠ¤íŠ¸ ë©”ì‹œì§€ ì²˜ë¦¬
                    final_message = await self._extract_final_message(result, collected_messages=None)

                    # ì™„ë£Œ ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
                    await updater.update_status(
                        TaskState.completed,
                        new_agent_text_message(final_message, context_id, task_id),
                        final=True,
                    )

                    logger.debug("Task completed after synchronous execution (text fallback)")
                    return

                except Exception as e:
                    # ì—ëŸ¬ ë°œìƒ ì‹œ failed ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
                    await updater.update_status(
                        TaskState.failed,
                        new_agent_text_message(
                            f"ì‘ì—… ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                            context_id,
                            task_id,
                        ),
                        final=True,
                    )
                    logger.error(f"Synchronous execution failed: {e}")
                    raise

            # ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ (astream_events ì‚¬ìš©)
            logger.info("ğŸš€ Starting streaming execution with astream_events")
            logger.info(f"Thread ID: {context_id}, Task ID: {task_id}")

            # EventQueue ìƒíƒœ í™•ì¸
            if hasattr(event_queue, "_closed"):
                logger.debug(f"EventQueue._closed before streaming: {event_queue._closed}")

            # ìŠ¤íŠ¸ë¦¬ë° ìƒíƒœ ì¶”ì  ë³€ìˆ˜
            event_count = 0
            node_count = 0

            # ë©”ì‹œì§€ ë²„í¼ë§ ê´€ë¦¬
            message_buffer = []  # ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ìš© ë²„í¼
            buffer_size = 0
            MAX_BUFFER_SIZE = 100  # 100ìë§ˆë‹¤ ì „ì†¡í•˜ì—¬ ì‹¤ì‹œê°„ì„± í–¥ìƒ

            # ìŠ¤íŠ¸ë¦¬ë° ì§„í–‰ ìƒíƒœ ì¶”ì 
            streaming_start_time = asyncio.get_event_loop().time()
            last_heartbeat_time = streaming_start_time
            HEARTBEAT_INTERVAL = 10.0  # 10ì´ˆë§ˆë‹¤ heartbeat ì „ì†¡

            # ë…¸ë“œ ê²°ê³¼ ìˆ˜ì§‘ (ìŠ¤íŠ¸ë¦¬ë° ì¤‘ì— ì§ì ‘ ìˆ˜ì§‘)
            process_collection_result = None  # DataCollector ê²°ê³¼ ì§ì ‘ ì €ì¥

            try:
                async for event in self.graph.astream_events(
                    processed_input,
                    config={"configurable": {"thread_id": context_id}},
                ):
                    event_count += 1
                    # ì´ë²¤íŠ¸ íƒ€ì… í™•ì¸
                    event_type = event.get("event", "")

                    # Heartbeat ë©”ì‹œì§€ ì „ì†¡ (ì—°ê²° ìœ ì§€ìš©)
                    current_time = asyncio.get_event_loop().time()
                    if current_time - last_heartbeat_time > HEARTBEAT_INTERVAL:
                        try:
                            # ì‘ì—… ì§„í–‰ ì¤‘ì„ì„ ì•Œë¦¬ëŠ” ìƒíƒœ ì—…ë°ì´íŠ¸
                            await updater.update_status(TaskState.working)
                            logger.debug(f"ğŸ’“ Heartbeat sent after {HEARTBEAT_INTERVAL}s")
                            last_heartbeat_time = current_time
                        except Exception as e:
                            logger.warning(f"Failed to send heartbeat: {e}")

                    # ì²« ëª‡ ê°œì˜ ì´ë²¤íŠ¸ì—ì„œë§Œ EventQueue ìƒíƒœ í™•ì¸
                    if event_count <= 3 and hasattr(event_queue, "_closed"):
                        logger.debug(
                            f"Event {event_count} - EventQueue._closed: {event_queue._closed}"
                        )

                    # ë…¸ë“œ ì‹œì‘/ì¢…ë£Œ ì¶”ì 
                    if event_type == "on_chain_start":
                        node_name = event.get("name", "unknown")
                        node_count += 1
                        logger.debug(f"ğŸ“ [{node_count}] Starting node: {node_name}")

                    elif event_type == "on_chain_end":
                        node_name = event.get("name", "unknown")
                        logger.debug(f"âœ“ Node completed: {node_name}")

                        # DataCollector ë…¸ë“œ ê²°ê³¼ ì§ì ‘ ìºì¹˜
                        if node_name == "process_collection":
                            output = event.get("data", {}).get("output", {})
                            if output and isinstance(output, dict):
                                process_collection_result = output
                                logger.info(f"ğŸ¯ [Direct Capture] DataCollector result: {list(output.keys())}")
                                logger.info(f"ğŸ¯ [Direct Capture] DataCollector success: {output.get('success', False)}")

                        # ì™„ë£Œ ë…¸ë“œ ê°ì§€: __end__, process_collection
                        # process_collection: DataCollectorAgentA2A ë©”ì¸ ë…¸ë“œ
                        completion_nodes = ["__end__", "process_collection"]
                        if node_name in completion_nodes and not is_completed:
                            is_completed = True
                            logger.info(f"ğŸ¯ Graph completion detected at node: {node_name}")

                            # í˜„ì¬ ìƒíƒœì—ì„œ ìµœì¢… ë©”ì‹œì§€ ì¶”ì¶œ (ìˆ˜ì§‘ëœ ë©”ì‹œì§€ í¬í•¨)
                            try:
                                logger.info(f"ğŸ” Getting state for thread_id: {context_id}")
                                current_state = await self.graph.aget_state(
                                    config={"configurable": {"thread_id": context_id}}
                                )

                                logger.info(f"ğŸ” State retrieval result: {current_state is not None}")
                                if current_state:
                                    logger.info(f"ğŸ” State.values: {current_state.values is not None}")
                                    if current_state.values:
                                        logger.info(f"ğŸ” State.values keys: {list(current_state.values.keys()) if isinstance(current_state.values, dict) else type(current_state.values)}")
                                        logger.info(f"ğŸ” State.values content: {current_state.values}")

                                if current_state and current_state.values:
                                    # ìˆ˜ì§‘ëœ ë©”ì‹œì§€ë¥¼ _extract_final_messageì— ì „ë‹¬
                                    final_text = await self._extract_final_message(
                                        current_state.values,
                                        collected_messages=collected_messages
                                    )
                                    logger.info(f"âœ… Extracted final message from state: {final_text[:100]}...")
                                else:
                                    # ìƒíƒœê°€ ì—†ëŠ” ê²½ìš° ìˆ˜ì§‘ëœ ë©”ì‹œì§€ë§Œìœ¼ë¡œ ì²˜ë¦¬
                                    logger.warning("No state values found, using collected messages")
                                    final_text = await self._extract_final_message(
                                        {},
                                        collected_messages=collected_messages
                                    )

                            except Exception as e:
                                logger.warning(f"âš ï¸ Failed to extract state result: {e}")
                                # ì—ëŸ¬ ì‹œì—ë„ ìˆ˜ì§‘ëœ ë©”ì‹œì§€ë¥¼ í™œìš©
                                final_text = await self._extract_final_message(
                                    {},
                                    collected_messages=collected_messages
                                )

                            # result_extractorë¥¼ ì‚¬ìš©í•´ì„œ ì‹¤ì œ ê²°ê³¼ ì¶”ì¶œ ë° A2A ë©”ì‹œì§€ ì „ì†¡
                            result_to_use = None

                            # 1ìˆœìœ„: ì§ì ‘ ìºì¹˜í•œ DataCollector ê²°ê³¼ ì‚¬ìš©
                            if process_collection_result and isinstance(process_collection_result, dict):
                                logger.info("ğŸ¯ Using directly captured DataCollector result")
                                result_to_use = process_collection_result
                            # 2ìˆœìœ„: LangGraph ìƒíƒœ ì‚¬ìš©
                            elif current_state and current_state.values:
                                logger.info("ğŸ¯ Using LangGraph state result")
                                result_to_use = current_state.values

                            if result_to_use:
                                logger.info("ğŸ”§ Extracting result using result_extractor...")
                                try:
                                    # result_extractorê°€ Dictë¥¼ ë°˜í™˜í•œë‹¤ë©´ _send_result ì‚¬ìš©
                                    extracted_result = self.result_extractor(result_to_use)
                                    if isinstance(extracted_result, dict):
                                        logger.info("âœ… result_extractor returned dict, using _send_result")
                                        await self._send_result(updater, extracted_result, event_queue, complete_task=True)
                                        # ì™„ë£Œ í”Œë˜ê·¸ ì„¤ì • (ìŠ¤íŠ¸ë¦¬ë° ë£¨í”„ ì •ìƒ ì¢…ë£Œ)
                                        is_completed = True
                                        logger.info("âœ… DataPart response sent, task completed")
                                        break  # ìŠ¤íŠ¸ë¦¬ë° ë£¨í”„ ì •ìƒ ì¢…ë£Œ
                                    else:
                                        logger.info("â„¹ï¸ result_extractor returned text, using text message")
                                        await updater.update_status(
                                            TaskState.completed,
                                            new_agent_text_message(final_text, context_id, task_id),
                                            final=True,
                                        )
                                        # ì™„ë£Œ í”Œë˜ê·¸ ì„¤ì •
                                        is_completed = True
                                        logger.info("âœ… Text response sent, task completed")
                                        break
                                except Exception as e:
                                    logger.error(f"âŒ Failed to extract result: {e}")
                                    # í´ë°±ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë©”ì‹œì§€ ì‚¬ìš©
                                    await updater.update_status(
                                        TaskState.completed,
                                        new_agent_text_message(final_text, context_id, task_id),
                                        final=True,
                                    )
                                    is_completed = True
                                    logger.info("âœ… Error handled with text fallback, task completed")
                                    break
                            else:
                                # ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° í…ìŠ¤íŠ¸ ë©”ì‹œì§€ë§Œ ì‚¬ìš©
                                await updater.update_status(
                                    TaskState.completed,
                                    new_agent_text_message(final_text, context_id, task_id),
                                    final=True,
                                )
                                is_completed = True
                                logger.info("âœ… Fallback text response sent, task completed")
                                break

                            # ì´ë¯¸ return í–ˆìœ¼ë¯€ë¡œ ì—¬ê¸°ëŠ” ë„ë‹¬í•˜ì§€ ì•ŠìŒ
                            logger.info("âœ… Task completed after streaming")
                            break

                    # ìƒíƒœ ì—…ë°ì´íŠ¸ ì´ë²¤íŠ¸ ì²˜ë¦¬
                    elif event_type == "on_chain_stream":
                        if "data" in event and event["data"]:
                            chunk = event["data"].get("chunk")
                            if chunk:
                                # ì²­í¬ì—ì„œ ë‚´ìš© ì¶”ì¶œ
                                if hasattr(chunk, "content"):
                                    content = chunk.content
                                else:
                                    content = (
                                        chunk.get("content", "")
                                        if isinstance(chunk, dict)
                                        else ""
                                    )

                                if content:
                                    collected_messages.append(content)

                    # LLM ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ (í† í° ë‹¨ìœ„) - LLM ì‘ë‹µë§Œ ìŠ¤íŠ¸ë¦¬ë°
                    elif event_type == "on_llm_stream":
                        if "data" in event and event["data"]:
                            chunk = event["data"].get("chunk")
                            if chunk:
                                # AIMessageChunk ì²˜ë¦¬
                                if hasattr(chunk, "content"):
                                    content = chunk.content
                                else:
                                    content = (
                                        chunk.get("content", "")
                                        if isinstance(chunk, dict)
                                        else ""
                                    )

                                if content:
                                    # í† í°ì„ ë²„í¼ì— ì¶”ê°€
                                    message_buffer.append(content)
                                    buffer_size += len(content)
                                    collected_messages.append(content)

                                    # ë²„í¼ê°€ ê°€ë“ ì°¨ë©´ LLM ì‘ë‹µ ì „ì†¡ (ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°)
                                    if buffer_size >= MAX_BUFFER_SIZE:
                                        buffer_content = "".join(message_buffer)
                                        try:
                                            await updater.update_status(
                                                TaskState.working,
                                                new_agent_text_message(
                                                    buffer_content,
                                                    context_id,
                                                    task_id,
                                                ),
                                            )
                                            logger.debug(f"ğŸ“¤ Sent buffered message ({buffer_size} chars)")
                                            message_buffer.clear()
                                            buffer_size = 0
                                        except Exception as e:
                                            logger.warning(f"âš ï¸ Failed to send buffered message: {e}")

                    # LLM ì™„ë£Œ ì´ë²¤íŠ¸
                    elif event_type == "on_llm_end":
                        logger.debug(
                            f"LLM response completed for node: {event.get('name', 'unknown')}"
                        )

                        # ë‚¨ì€ ë²„í¼ ë‚´ìš©ì´ ìˆìœ¼ë©´ ì „ì†¡ (LLM ì‘ë‹µ ë§ˆì§€ë§‰ ë¶€ë¶„)
                        if message_buffer:
                            buffer_content = "".join(message_buffer)
                            try:
                                await updater.update_status(
                                    TaskState.working,
                                    new_agent_text_message(
                                        buffer_content, context_id, task_id
                                    ),
                                )
                                message_buffer.clear()
                                buffer_size = 0
                            except Exception as e:
                                logger.warning(f"Failed to send remaining buffer: {e}")

                # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í†µê³„
                streaming_duration = asyncio.get_event_loop().time() - streaming_start_time
                logger.info("ğŸ“Š Streaming Statistics:")
                logger.info(f"  - Total events: {event_count}")
                logger.info(f"  - Nodes executed: {node_count}")
                logger.info(f"  - Messages collected: {len(collected_messages)}")
                logger.info(f"  - Duration: {streaming_duration:.2f}s")
                logger.info(f"  - Completed: {is_completed}")

                # ìŠ¤íŠ¸ë¦¬ë°ì´ ì™„ë£Œë˜ì—ˆëŠ”ë° __end__ ë…¸ë“œë¥¼ ëª» ë§Œë‚œ ê²½ìš°
                if not is_completed:
                    logger.info("âš ï¸ Streaming ended without explicit completion node")

                    # LangGraphì˜ í˜„ì¬ ìƒíƒœë¥¼ ê°€ì ¸ì™€ì„œ ê²°ê³¼ ì¶”ì¶œ
                    try:
                        logger.info(f"ğŸ” [Fallback] Getting state for thread_id: {context_id}")
                        current_state = await self.graph.aget_state(
                            config={"configurable": {"thread_id": context_id}}
                        )

                        logger.info(f"ğŸ” [Fallback] State retrieval result: {current_state is not None}")
                        if current_state:
                            logger.info(f"ğŸ” [Fallback] State.values: {current_state.values is not None}")
                            if current_state.values:
                                logger.info(f"ğŸ” [Fallback] State.values keys: {list(current_state.values.keys()) if isinstance(current_state.values, dict) else type(current_state.values)}")
                                logger.info(f"ğŸ” [Fallback] State.values content: {current_state.values}")

                        # ìƒíƒœì—ì„œ ìµœì¢… ë©”ì‹œì§€ ì¶”ì¶œ (ìˆ˜ì§‘ëœ ë©”ì‹œì§€ í¬í•¨)
                        if current_state and current_state.values:
                            final_text = await self._extract_final_message(
                                current_state.values,
                                collected_messages=collected_messages
                            )
                            logger.info(f"âœ… Extracted final message from graph state: {final_text[:100]}...")
                        else:
                            # ìƒíƒœê°€ ì—†ì–´ë„ ìˆ˜ì§‘ëœ ë©”ì‹œì§€ë¡œ ì‹œë„
                            logger.warning("No state values found, using only collected messages")
                            final_text = await self._extract_final_message(
                                {},
                                collected_messages=collected_messages
                            )

                    except Exception as e:
                        logger.warning(f"âš ï¸ Failed to extract state result: {e}")
                        # ì—ëŸ¬ ì‹œì—ë„ ìˆ˜ì§‘ëœ ë©”ì‹œì§€ë¥¼ í™œìš©
                        final_text = await self._extract_final_message(
                            {},
                            collected_messages=collected_messages
                        )

                    # í´ë°± ì™„ë£Œì—ì„œë„ _send_result ì‚¬ìš© ì‹œë„
                    current_state = None
                    try:
                        current_state = await self.graph.aget_state(
                            config={"configurable": {"thread_id": context_id}}
                        )
                    except Exception as e:
                        logger.debug(f"Failed to get fallback state: {e}")

                    if current_state and current_state.values:
                        logger.info("ğŸ”§ [Fallback] Extracting result using result_extractor...")
                        try:
                            extracted_result = self.result_extractor(current_state.values)
                            if isinstance(extracted_result, dict):
                                logger.info("âœ… [Fallback] result_extractor returned dict, using _send_result")
                                await self._send_result(updater, extracted_result, event_queue, complete_task=True)
                                # ì™„ë£Œ í”Œë˜ê·¸ ì„¤ì •
                                is_completed = True
                                logger.info("âœ… [Fallback] DataPart response sent, task completed")
                            else:
                                logger.info("â„¹ï¸ [Fallback] result_extractor returned text, using text message")
                                await updater.update_status(
                                    TaskState.completed,
                                    new_agent_text_message(final_text, context_id, task_id),
                                    final=True,
                                )
                                is_completed = True
                                logger.info("âœ… [Fallback] Text response sent, task completed")
                        except Exception as e:
                            logger.error(f"âŒ [Fallback] Failed to extract result: {e}")
                            await updater.update_status(
                                TaskState.completed,
                                new_agent_text_message(final_text, context_id, task_id),
                                final=True,
                            )
                            is_completed = True
                            logger.info("âœ… [Fallback] Error handled with text response, task completed")
                    else:
                        # í´ë°±: í…ìŠ¤íŠ¸ ë©”ì‹œì§€ë§Œ ì‚¬ìš©
                        await updater.update_status(
                            TaskState.completed,
                            new_agent_text_message(final_text, context_id, task_id),
                            final=True,
                        )
                        is_completed = True
                        logger.info("âœ… [Fallback] Text-only response sent, task completed")

                    logger.info("âœ… Task completed after streaming (fallback completion)")

            except Exception as e:
                logger.error(f"Error during streaming: {e}")
                # ì—ëŸ¬ ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
                try:
                    await updater.update_status(
                        TaskState.failed,
                        new_agent_text_message(
                            f"ì‘ì—… ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                            context_id,
                            task_id,
                        ),
                        final=True,
                    )
                except Exception as update_error:
                    logger.error(f"Failed to update error status: {update_error}")
                raise

        except Exception as e:
            logger.error(f"Critical error in executor: {e}")
            raise

    async def _process_input(self, context: RequestContext) -> dict[str, Any]:
        """Extract and process input from request context."""
        query = context.get_user_input()

        # Try to get structured data from DataPart
        payload = {}
        if context.message and getattr(context.message, "parts", None):
            try:
                data_parts = get_data_parts(context.message.parts)
                if data_parts:
                    last_part = data_parts[-1]
                    if isinstance(last_part, dict):
                        payload = last_part
            except Exception as e:
                logger.debug(f"No DataPart found: {e}")

        # Build input for LangGraph
        if payload:
            # Always use payload directly for structured data - Dict í˜•íƒœë¡œ ë°˜í™˜
            # A2A DataPartì—ì„œ ì˜¨ êµ¬ì¡°í™”ëœ ë°ì´í„°ëŠ” í•­ìƒ Dictë¡œ ì²˜ë¦¬
            logger.debug(f"Using structured payload: {payload}")
            return payload
        elif query:
            # í…ìŠ¤íŠ¸ ì¿¼ë¦¬ë§Œ ìˆëŠ” ê²½ìš°ì—ë§Œ messagesë¡œ ë˜í•‘
            return {"messages": [{"role": "user", "content": query}]}
        else:
            return {"messages": []}

    async def _is_resume_operation(self, context: RequestContext) -> bool:
        """Check if this is a resume operation (Human-in-the-Loop)."""
        if not context.current_task:
            return False

        # Check if task is in input_required state
        if context.current_task.status == TaskState.input_required:
            return True

        # Check for explicit resume signal
        query = context.get_user_input().lower()
        return any(word in query for word in ["resume", "continue", "proceed"])

    def _extract_resume_value(self, context: RequestContext) -> Any:
        """Extract the value to resume with."""
        return context.get_user_input()

    async def _execute_with_streaming_events(
        self,
        invoke_input: Any,
        config: dict[str, Any],
        updater: TaskUpdater,
        task: Any,
        event_queue: EventQueue,
    ) -> dict[str, Any] | None:
        """
        Execute graph with streaming using astream_events for detailed event tracking.

        This method provides fine-grained event streaming, allowing us to track:
        - Chain starts and ends
        - Individual node executions
        - LLM token streaming
        - Tool calls and responses
        """
        accumulated_text = ""
        last_result = None
        current_node = None
        queue_closed = False

        try:
            # Use astream_events for detailed event streaming
            async for event in self.graph.astream_events(
                invoke_input, config, version="v2"
            ):
                # Skip if queue is already closed
                if queue_closed:
                    continue
                event_type = event.get("event", "")

                # Track which node is currently executing
                if event_type == "on_chain_start":
                    # Chain or node starting
                    metadata = event.get("metadata", {})
                    if "langgraph_node" in metadata:
                        current_node = metadata["langgraph_node"]
                        logger.debug(f"Starting node: {current_node}")

                        # Check if we reached the end node
                        if current_node == "__end__":
                            logger.debug("Reached __end__ node, completing task")
                            try:
                                await updater.complete()
                                queue_closed = True
                            except Exception as e:
                                logger.debug(f"Error completing task: {e}")
                            break

                elif event_type == "on_chain_stream":
                    # Streaming output from a chain/node
                    chunk = event.get("data", {}).get("chunk", {})

                    if isinstance(chunk, dict):
                        # Handle interrupt
                        if "__interrupt__" in chunk:
                            logger.info(
                                "Interrupt detected, switching to input_required"
                            )
                            interrupt_info = chunk["__interrupt__"]

                            # Create message requesting input
                            msg_text = (
                                "Human input required. Please provide your response."
                            )
                            if isinstance(interrupt_info, dict):
                                msg_text = interrupt_info.get("message", msg_text)

                            message = new_agent_text_message(msg_text)
                            try:
                                await event_queue.enqueue_event(message)
                                await updater.update_status(TaskState.input_required)
                            except Exception as e:
                                logger.debug(
                                    f"Could not send message (queue may be closed): {e}"
                                )
                                queue_closed = True
                            return last_result

                        # Store result
                        last_result = chunk

                        # Send structured data if present
                        if "data" in chunk:
                            try:
                                data_part = Part(
                                    root=DataPart(
                                        data=self._clean_for_json(chunk["data"])
                                    )
                                )
                                message = new_agent_parts_message([data_part])
                                try:
                                    await event_queue.enqueue_event(message)
                                except Exception as e:
                                    logger.debug(
                                        f"Could not send data part (queue may be closed): {e}"
                                    )
                                    queue_closed = True
                            except Exception as e:
                                logger.debug(f"Could not send DataPart: {e}")

                elif event_type == "on_chat_model_stream":
                    # Direct LLM token streaming
                    # event["data"]["chunk"] is an AIMessageChunk object, not a dict
                    try:
                        data = event.get("data", {})
                        chunk = data.get("chunk") if isinstance(data, dict) else None

                        # AIMessageChunk has a .content attribute
                        if chunk:
                            if hasattr(chunk, "content"):
                                content = chunk.content
                            else:
                                # Fallback for dict-like objects
                                content = (
                                    chunk.get("content", "")
                                    if isinstance(chunk, dict)
                                    else ""
                                )

                            if content:
                                # Stream text tokens as they arrive
                                message = new_agent_text_message(content)
                                try:
                                    await event_queue.enqueue_event(message)
                                except Exception as e:
                                    logger.debug(
                                        f"Could not stream text (queue may be closed): {e}"
                                    )
                                    queue_closed = True
                                accumulated_text += content
                    except Exception as e:
                        logger.debug(f"Error processing chat model stream: {e}")

                elif event_type == "on_tool_start":
                    # Tool execution starting
                    tool_name = event.get("name", "unknown")
                    logger.debug(f"Tool {tool_name} starting")

                    # Optionally notify about tool execution
                    if self.config.enable_interrupt_handling:
                        message = new_agent_text_message(
                            f"\nğŸ”§ Executing tool: {tool_name}\n"
                        )
                        try:
                            await event_queue.enqueue_event(message)
                        except Exception as e:
                            logger.debug(
                                f"Could not send tool notification (queue may be closed): {e}"
                            )
                            queue_closed = True

                elif event_type == "on_tool_end":
                    # Tool execution completed
                    output = event.get("data", {}).get("output", {})
                    if output and isinstance(output, dict):
                        # Send tool output as DataPart
                        try:
                            data_part = Part(
                                root=DataPart(data=self._clean_for_json(output))
                            )
                            message = new_agent_parts_message([data_part])
                            try:
                                await event_queue.enqueue_event(message)
                            except Exception as e:
                                logger.debug(f"Could not send tool output: {e}")
                        except Exception as e:
                            logger.debug(f"Could not send tool output: {e}")

                elif event_type == "on_chain_end":
                    # Chain or node completed
                    output = event.get("data", {}).get("output", {})
                    if output and current_node:
                        logger.debug(f"Node {current_node} completed")
                        # Process final output from node
                        if isinstance(output, dict):
                            last_result = output

                            # Try to extract and send text
                            try:
                                text = self.result_extractor(output)
                                if text and text != accumulated_text:
                                    delta = text[len(accumulated_text) :]
                                    if delta:
                                        message = new_agent_text_message(delta)
                                        try:
                                            await event_queue.enqueue_event(message)
                                        except Exception as e:
                                            logger.debug(
                                                f"Could not send text delta (queue may be closed): {e}"
                                            )
                                            queue_closed = True
                                        accumulated_text = text
                            except Exception as e:
                                logger.debug(f"Could not extract text: {e}")

                elif event_type == "on_chat_model_end":
                    # LLM call completed
                    response = event.get("data", {}).get("output", {})
                    if response:
                        logger.debug(f"LLM response completed for node: {current_node}")

        except Exception as e:
            logger.error(f"Streaming events execution error: {e}")
            raise
        finally:
            # Complete the task if not already completed
            if not queue_closed:
                try:
                    await updater.complete()
                    logger.debug("Task completed after streaming")
                except Exception as e:
                    logger.debug(f"Error during completion: {e}")

        return last_result

    async def _execute_with_streaming(
        self,
        invoke_input: Any,
        config: dict[str, Any],
        updater: TaskUpdater,
        task: Any,
        event_queue: EventQueue,
    ) -> dict[str, Any] | None:
        """Execute graph with basic streaming using astream."""
        accumulated_text = ""
        last_result = None
        queue_closed = False

        try:
            async for chunk in self.graph.astream(invoke_input, config):
                if isinstance(chunk, dict):
                    # Handle interrupt
                    if "__interrupt__" in chunk:
                        logger.info("Interrupt detected, switching to input_required")
                        interrupt_info = chunk["__interrupt__"]

                        # Create message requesting input
                        msg_text = "Human input required. Please provide your response."
                        if isinstance(interrupt_info, dict):
                            msg_text = interrupt_info.get("message", msg_text)

                        message = new_agent_text_message(msg_text)
                        try:
                            await event_queue.enqueue_event(message)
                            await updater.update_status(TaskState.input_required)
                        except Exception as e:
                            logger.debug(
                                f"Could not send message (queue may be closed): {e}"
                            )
                        return last_result

                    # Process normal chunk
                    last_result = chunk

                    # Extract and stream text/data
                    try:
                        # For streaming, prefer text-only for incremental updates
                        text = self.result_extractor(chunk)
                        if text and text != accumulated_text:
                            # Send incremental text update
                            delta = text[len(accumulated_text) :]
                            if delta:
                                message = new_agent_text_message(delta)
                                await event_queue.enqueue_event(message)
                                accumulated_text = text
                    except Exception as e:
                        logger.debug(f"Could not extract text from chunk: {e}")

                    # If chunk contains structured data, also send as DataPart
                    if isinstance(chunk, dict) and "data" in chunk:
                        try:
                            data_part = Part(
                                root=DataPart(data=self._clean_for_json(chunk["data"]))
                            )
                            message = new_agent_parts_message([data_part])
                            try:
                                await event_queue.enqueue_event(message)
                            except Exception as e:
                                logger.debug(f"Could not send tool output: {e}")
                        except Exception as e:
                            logger.debug(f"Could not send DataPart: {e}")

        except Exception as e:
            logger.error(f"Streaming execution error: {e}")
            raise
        finally:
            # Complete the task if not already completed
            if not queue_closed:
                try:
                    await updater.complete()
                    logger.debug("Task completed after streaming")
                except Exception as e:
                    logger.debug(f"Error during completion: {e}")

        return last_result

    async def _execute_without_streaming(
        self,
        invoke_input: Any,
        config: dict[str, Any],
    ) -> dict[str, Any]:
        """Execute graph without streaming using asyncio.to_thread()."""
        # asyncio.to_thread()ë¥¼ ì‚¬ìš©í•˜ì—¬ ainvokeë¥¼ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
        # ìŠ¤ë ˆë“œ ë‚´ì—ì„œ ìƒˆë¡œìš´ ì´ë²¤íŠ¸ ë£¨í”„ë¥¼ ìƒì„±í•˜ì—¬ MCP ë„êµ¬ í˜¸í™˜ì„± ìœ ì§€
        def run_ainvoke():
            return asyncio.run(self.graph.ainvoke(invoke_input, config))

        return await asyncio.to_thread(run_ainvoke)

    async def cancel(
        self,
        context: RequestContext,
        event_queue: EventQueue,
    ) -> None:
        """
        Cancel an ongoing task.

        This is called when the user requests to cancel a running task.
        """
        logger.info(f"Cancelling task: {context.task_id}")

        # Cancel any active asyncio tasks
        if context.task_id in self._active_tasks:
            task = self._active_tasks.pop(context.task_id)
            if not task.done():
                task.cancel()

        # Update task state
        if context.current_task:
            updater = TaskUpdater(
                event_queue=event_queue,
                task_id=context.current_task.id,
                context_id=str(context.context_id),
            )
            await updater.cancel()
            logger.info(f"Task {context.task_id} cancelled")
