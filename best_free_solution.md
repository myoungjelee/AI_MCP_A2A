# ğŸ¯ ë¬´ë£Œ ìµœì„  ë°©ë²•

## 1ï¸âƒ£ ì‚¬ì „ ì„¤ì • (1íšŒë§Œ)

### Vercel ë°°í¬

```bash
cd frontend
npx vercel --prod
```

- ê²°ê³¼: `https://ai-mcp-a2a.vercel.app` (ê³ ì • URL)

### í”„ë¡ íŠ¸ì—”ë“œ ì½”ë“œ ìˆ˜ì •

```javascript
// frontend/app/page.tsx
// í•˜ë“œì½”ë”©ëœ localhost:8000ì„ í™˜ê²½ë³€ìˆ˜ë¡œ ë³€ê²½
const backendUrl = process.env.NEXT_PUBLIC_BACKEND_URL || 'http://localhost:8000'
const response = await fetch(`${backendUrl}/analyze/stream`, {
```

---

## 2ï¸âƒ£ ë©´ì ‘ ë‹¹ì¼

### ë‹¨ê³„ 1: ë°±ì—”ë“œ ì‹¤í–‰

```bash
docker-compose up -d
ollama serve
```

### ë‹¨ê³„ 2: ë°±ì—”ë“œ í„°ë„ë§

```bash
ngrok http 8000
```

- ê²°ê³¼: `https://abc123.ngrok.io` (ë§¤ë²ˆ ë‹¤ë¦„)

### ë‹¨ê³„ 3: í™˜ê²½ë³€ìˆ˜ ì„¤ì •

```bash
# Vercelì—ì„œ í™˜ê²½ë³€ìˆ˜ ì„¤ì •
NEXT_PUBLIC_BACKEND_URL=https://abc123.ngrok.io
```

---

## 3ï¸âƒ£ ì™„ì„±!

- **í”„ë¡ íŠ¸ì—”ë“œ**: `https://ai-mcp-a2a.vercel.app` (ê³ ì •)
- **ë°±ì—”ë“œ**: `https://abc123.ngrok.io` (ë™ì )

## âœ… ì¥ì 

- ì™„ì „ ë¬´ë£Œ
- ê³ ì • í”„ë¡ íŠ¸ì—”ë“œ URL
- ì „ì„¸ê³„ ì ‘ì† ê°€ëŠ¥
- ì„¤ì • 5ë¶„
